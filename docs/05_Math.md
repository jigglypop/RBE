# 5. 수학적 원리: 정밀한 융합 순전파와 상태-전이 미분

## 5.1. 개요

본 패러다임의 핵심은 **디코딩 없는 융합 연산(Decoding-Free Fused Operations)**과 **상태-전이 미분(State-Transition Differentiation)**을 통한 정밀한 학습이다. 표준적인 신경망이 가중치를 명시적으로 저장하는 것과 달리, RBE는 `Packed128` 구조의 hi(상태 비트)와 lo(연속 파라미터)를 융합하여 가중치를 즉석에서 생성하며 동시에 연산을 수행한다.

실제 구현에서 검증된 핵심 원리들:

1.  **융합 순전파 (`fused_forward`)**: 가중치 디코딩과 행렬 곱셈을 단일 연산으로 융합
2.  **상태-전이 미분 (`apply_state_transition`)**: 그래디언트 신호에 따른 비트별 상태 전이
3.  **정밀한 역전파 (`fused_backward`)**: hi/lo 분리 그래디언트 계산과 안정적인 파라미터 업데이트
4.  **배치 그래디언트 누적**: 다중 샘플에 대한 효율적인 그래디언트 관리

## 5.2. 융합 순전파의 수학적 원리

### 5.2.1. `Packed128` 기반 가중치 생성

가중치 `W_{ij}`는 다음과 같이 즉석에서 생성된다:

```
W_{ij} = StateFunction(primary_state, ModulatedInput(i,j,r,θ)) × ModulationFactor(secondary_state)
```

여기서:
- `primary_state = (hi >> (hash(i,j) × 3)) & 0x7` (3비트, 8가지 함수 상태)
- `secondary_state = (hi >> ((hash(i,j) + 8) × 2)) & 0x3` (2비트, 4가지 변조 상태)
- `r_fp32 = f32::from_bits((lo >> 32) as u32)` (연속 스케일 파라미터)
- `θ_fp32 = f32::from_bits(lo as u32)` (연속 위상 파라미터)

### 5.2.2. 8가지 상태 함수 (실제 구현)

상태별 함수는 수치적 안정성을 보장하도록 구현되었다:

```
StateFunction(state, input, phase) = {
  0: sin(input + phase)                    // sin 상태
  1: cos(input + phase)                    // cos 상태  
  2: tanh(input × phase)                   // tanh 상태
  3: sech²(input × phase)                  // sech² 상태 (tanh의 미분)
  4: exp(input × phase × 0.1) [clamped]    // exp 상태 (폭발 방지)
  5: ln(|input × phase| + ε) [clamped]     // log 상태 (0 방지)
  6: 1/(input × phase + ε) [clamped]       // 1/x 상태 (무한대 방지)
  7: input×phase + 0.1×input² [clamped]    // 다항식 상태
}
```

모든 함수는 `[-1, 1]` 범위로 클램핑되며 NaN/무한대를 0으로 대체한다.

### 5.2.3. 융합 GEMM 연산

표준 GEMM `y = Wx`를 융합 연산으로 변환:

```
for i in 0..rows:
    for j in 0..cols:
        W_ij = fused_forward(i, j, packed_params)
        y[i] += W_ij × x[j]
```

이는 메모리 대역폭을 극적으로 감소시키며, 하드웨어에서 계산이 메모리 접근보다 빠른 경우 **faster-than-dense** 성능을 달성한다.

## 5.3. 상태-전이 미분의 구현

### 5.3.1. 그래디언트 신호에 따른 상태 전이

그래디언트 신호 `g`에 따라 상태가 전이된다:

```
if |g| > 0.2:  // 강한 그래디언트
    if g > 0: 미분 방향 전이 (sin→cos, tanh→sech²)
    if g < 0: 역방향 전이 (sin→polynomial, cos→1/x)
elif |g| > 0.05:  // 중간 그래디언트
    보조 상태 기반 미세 조정 (순환 전진/후진, 비트 토글)
else:  // 약한 그래디언트
    상태 유지
```

### 5.3.2. 다중 상태 시스템

실제 구현에서는 위치별로 독립적인 상태 관리:
- **주요 상태 (3비트)**: 8가지 함수 선택
- **보조 상태 (2비트)**: 4가지 변조 방식
- **해시 기반 분산**: `hash(i,j) = (i×31 + j) & mask`로 위치별 독립성 확보

### 5.3.3. 수치 예제: 실제 테스트 결과

2×2 그래디언트 테스트에서 관찰된 상태 전이:

```
초기 상태: hi=0x000000000005e313
그래디언트 적용:
- g=0.000 → 상태 유지: 0x000000000005e313
- g=0.150 → 강한 전이: 0x00000000e413b31d  
- g=0.250 → 더 강한 전이: 0x00000000e44bb3fe
- g=-0.100 → 역방향 전이: 0x00000000006fb3de
```

상태 비트의 변화를 통해 함수 특성이 동적으로 조정됨을 확인할 수 있다.

## 5.4. 정밀한 역전파 구현

### 5.4.1. 이중 파라미터 그래디언트 계산

역전파는 hi(이산)와 lo(연속) 파라미터를 분리하여 처리한다:

```rust
for (i, j, error) in matrix_elements:
    // 1. 상태 전이 미분 (hi 업데이트)
    seed.apply_state_transition(error, i, j)
    
    // 2. 연속 파라미터 그래디언트 (lo 업데이트)
    let dr = numerical_derivative_r(seed, i, j)
    let dθ = numerical_derivative_θ(seed, i, j)
    
    grad_r += error × dr
    grad_θ += error × dθ
```

### 5.4.2. 수치 미분을 통한 안정적인 그래디언트

연속 파라미터의 그래디언트는 수치 미분으로 계산:

```
∂W_ij/∂r = [W(r+ε) - W(r-ε)] / (2ε)
∂W_ij/∂θ = [W(θ+ε) - W(θ-ε)] / (2ε)
```

여기서 `ε = 1e-4`로 설정하여 수치적 안정성과 정확도의 균형을 맞춘다.

### 5.4.3. 실제 학습 성능

8×8 행렬 학습 테스트 결과:
- **초기 MSE**: 0.325719
- **최종 MSE**: 0.131549  
- **손실 개선**: 58.98%
- **파라미터 변화**: r: 0.5000→0.4558, θ: 0.0000→0.6220

이는 상태 전이와 연속 파라미터 업데이트가 효과적으로 융합되어 학습됨을 보여준다.

## 5.5. 배치 그래디언트 누적과 적응적 학습

### 5.5.1. 그래디언트 누적 구조

```rust
struct GradientAccumulator {
    r_grad_sum: f32,
    theta_grad_sum: f32,
    state_transition_count: u32,
    total_samples: u32,
}
```

배치 내 모든 샘플의 그래디언트를 누적한 후 평균을 내어 안정적인 학습을 보장한다.

### 5.5.2. 적응적 학습률

상태 전이 횟수에 따라 학습률을 조정:

```
adaptive_lr = base_lr × (1 / (1 + transition_count × 0.01))
```

상태 전이가 많이 발생할수록 학습률을 감소시켜 안정성을 확보한다.

## 5.6. 수학적 보장과 수렴성

### 5.6.1. 함수 연속성

모든 상태 함수는 클램핑과 안전 검사를 통해 연속성을 보장한다:
- 입력 범위 제한: `[-10, 10]`
- 출력 범위 제한: `[-1, 1]`
- NaN/무한대 처리: 자동으로 0 대체

### 5.6.2. 그래디언트 유계성

수치 미분의 epsilon과 클램핑을 통해 그래디언트가 유계임을 보장:
- `|∂W/∂r| ≤ 2/ε = 2×10⁴` (실제로는 클램핑으로 더 작음)
- `|∂W/∂θ| ≤ 2/ε = 2×10⁴`

### 5.6.3. 수렴 조건

테스트에서 확인된 수렴 조건:
1. 학습률 `lr ∈ [0.01, 0.1]`
2. 초기 파라미터 `r ∈ [0.1, 2.0]`, `θ ∈ ℝ`
3. 그래디언트 정규화: 배치 크기로 나누기

이러한 조건들이 만족될 때 안정적인 수렴을 보장한다. 