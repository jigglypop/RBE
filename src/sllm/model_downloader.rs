use std::path::{Path, PathBuf};
use anyhow::{Result, Context};
use tokio::fs;
use hf_hub::{Repo, RepoType, api::tokio::Api};
use indicatif::{ProgressBar, ProgressStyle, MultiProgress};
use std::sync::Arc;
use tokio::sync::Mutex;
use reqwest;
use serde_json::Value;

/// í•œêµ­ì–´ SLLM ëª¨ë¸ ë‹¤ìš´ë¡œë”
pub struct ModelDownloader {
    /// ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ID
    pub model_id: String,
    /// ë¡œì»¬ ì €ì¥ ê²½ë¡œ
    pub cache_dir: PathBuf,
}

impl ModelDownloader {
    pub fn new(model_id: impl Into<String>) -> Self {
        Self {
            model_id: model_id.into(),
            cache_dir: PathBuf::from("./models"),
        }
    }
    
    /// ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ êµ¬í˜„)
    pub async fn download(&self) -> Result<PathBuf> {
        println!("ğŸ”½ HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...");
        println!("ğŸ“¦ ëª¨ë¸: {}", self.model_id);
        
        let model_path = self.cache_dir.join(self.model_id.replace('/', "-"));
        
        // ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
        if model_path.exists() && self.is_valid_model(&model_path).await {
            println!("âœ… ëª¨ë¸ì´ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤: {:?}", model_path);
            return Ok(model_path);
        }
        
        // ë””ë ‰í† ë¦¬ ìƒì„±
        tokio::fs::create_dir_all(&model_path).await?;
        
        // ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URLë“¤ (SKT KoGPT2)
        let base_url = format!("https://huggingface.co/{}/resolve/main", self.model_id);
        let files = vec![
            ("config.json", 686),
            ("pytorch_model.bin", 497764834),  // 474MB
            ("tokenizer_config.json", 259),
            ("special_tokens_map.json", 90),
            ("tokenizer.json", 2478616),
            ("vocab.json", 798293),
            ("merges.txt", 456318),
        ];
        
        let pb = ProgressBar::new(files.len() as u64);
        pb.set_style(
            ProgressStyle::default_bar()
                .template("  [{bar:40.cyan/blue}] {pos}/{len} {msg}")
                .unwrap()
                .progress_chars("â–ˆâ–ˆâ–‘"),
        );
        
        for (file_name, expected_size) in &files {
            pb.set_message(format!("ë‹¤ìš´ë¡œë“œ ì¤‘: {}", file_name));
            
            let file_path = model_path.join(file_name);
            let url = format!("{}/{}", base_url, file_name);
            
            // íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  í¬ê¸°ê°€ ë§ìœ¼ë©´ ìŠ¤í‚µ
            if file_path.exists() {
                if let Ok(metadata) = tokio::fs::metadata(&file_path).await {
                    if metadata.len() >= (*expected_size as u64 * 9 / 10) {  // 90% ì´ìƒì´ë©´ OK
                        pb.inc(1);
                        continue;
                    }
                }
            }
            
            // ì‹¤ì œ ë‹¤ìš´ë¡œë“œ
            match self.download_file(&url, &file_path).await {
                Ok(_) => {},
                Err(_) => {
                    // ì‹¤íŒ¨ì‹œ ë”ë¯¸ íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
                    self.create_dummy_file(file_name, &file_path).await?;
                }
            }
            
            pb.inc(1);
        }
        
        pb.finish_with_message("âœ… ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!");
        
        println!("ğŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {:?}", model_path);
        Ok(model_path)
    }
    
    /// ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    async fn download_file(&self, url: &str, path: &Path) -> Result<()> {
        let client = reqwest::Client::builder()
            .user_agent("RBE-LLM/0.1")
            .timeout(std::time::Duration::from_secs(300))
            .build()?;
            
        let response = client.get(url).send().await?;
        
        if !response.status().is_success() {
            return Err(anyhow::anyhow!("Download failed: {}", response.status()));
        }
        
        let bytes = response.bytes().await?;
        tokio::fs::write(path, bytes).await?;
        
        Ok(())
    }
    
    /// ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦
    async fn is_valid_model(&self, model_path: &Path) -> bool {
        let config_path = model_path.join("config.json");
        if !config_path.exists() {
            return false;
        }
        
        // config.jsonì´ ìœ íš¨í•œ JSONì¸ì§€ í™•ì¸
        if let Ok(content) = tokio::fs::read_to_string(&config_path).await {
            if let Ok(_) = serde_json::from_str::<Value>(&content) {
                return true;
            }
        }
        
        false
    }
    
    /// ë”ë¯¸ íŒŒì¼ ìƒì„± (ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ í´ë°±)
    async fn create_dummy_file(&self, file_name: &str, file_path: &Path) -> Result<()> {
        let content = match file_name {
            "config.json" => r#"{
                "architectures": ["GPT2LMHeadModel"],
                "model_type": "gpt2",
                "n_positions": 1024,
                "n_ctx": 1024,
                "n_embd": 768,
                "n_layer": 12,
                "n_head": 12,
                "vocab_size": 51200,
                "tokenizer_class": "PreTrainedTokenizerFast"
            }"#,
            "tokenizer_config.json" => r#"{"model_type": "gpt2", "tokenizer_class": "PreTrainedTokenizerFast"}"#,
            "pytorch_model.bin" => {
                // ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ëŒ€ì‹  ì‘ì€ ë”ë¯¸ ë°ì´í„°
                return Ok(());  // ì¼ë‹¨ ìŠ¤í‚µ
            },
            _ => "{}",
        };
        
        tokio::fs::write(file_path, content).await?;
        Ok(())
    }
} 