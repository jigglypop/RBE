use std::path::Path;
use std::collections::HashMap;
use anyhow::{Result, anyhow};
use serde_json::Value;
use std::fs;

/// ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë” (ê¸°ì¡´ ë°”ì´ë„ˆë¦¬ ìŠ¤íƒ€ì¼)
pub struct SimpleModelLoader;

impl SimpleModelLoader {
    /// ìƒˆë¡œìš´ ëª¨ë¸ ë¡œë” ìƒì„±
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
    
    /// ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ê¸°ì¡´ numpy/json íŒŒì¼ ë°©ì‹)
    pub fn load_model_weights(&self, model_path: &Path) -> Result<HashMap<String, Vec<f32>>> {
        let weights_dir = model_path.join("weights");
        let metadata_path = weights_dir.join("metadata.json");
        
        if !metadata_path.exists() {
            return Err(anyhow!("ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {:?}", metadata_path));
        }
        
        // ë©”íƒ€ë°ì´í„° ë¡œë“œ
        let metadata_str = fs::read_to_string(&metadata_path)?;
        let metadata: serde_json::Map<String, Value> = serde_json::from_str(&metadata_str)?;
        
        let mut weights = HashMap::new();
        
        println!("ğŸ“Š ë¡œë“œëœ ë ˆì´ì–´:");
        for (layer_name, layer_info) in &metadata {
            if let Some(shape_arr) = layer_info.get("shape").and_then(|s| s.as_array()) {
                let shape: Vec<usize> = shape_arr.iter()
                    .filter_map(|v| v.as_u64().map(|n| n as usize))
                    .collect();
                
                // 2D í…ì„œë§Œ ì²˜ë¦¬
                if shape.len() == 2 {
                    let file_path = weights_dir.join(format!("{}.npy", layer_name));
                    if file_path.exists() {
                        match self.load_numpy_file(&file_path) {
                            Ok(data) => {
                                println!("  âœ… {}: {}Ã—{}", layer_name, shape[0], shape[1]);
                                weights.insert(layer_name.clone(), data);
                            }
                            Err(e) => {
                                println!("  âŒ {}: ë¡œë“œ ì‹¤íŒ¨ - {}", layer_name, e);
                            }
                        }
                    }
                }
            }
        }
        
        println!("âœ… ì´ {} ë ˆì´ì–´ ë¡œë“œ ì™„ë£Œ", weights.len());
        Ok(weights)
    }
    
    /// NumPy íŒŒì¼ ë¡œë“œ (ê°„ë‹¨í•œ ë²„ì „)
    fn load_numpy_file(&self, path: &Path) -> Result<Vec<f32>> {
        let mut file = std::fs::File::open(path)?;
        let (shape, _) = self.read_npy_header(&mut file)?;
        
        let total_size: usize = shape.iter().product();
        let mut buffer = vec![0u8; total_size * 4]; // f32 = 4 bytes
        
        std::io::Read::read_exact(&mut file, &mut buffer)?;
        
        let data: Vec<f32> = buffer.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        Ok(data)
    }
    
    /// NumPy í—¤ë” ì½ê¸°
    fn read_npy_header(&self, file: &mut std::fs::File) -> Result<(Vec<usize>, usize)> {
        use std::io::Read;
        
        let mut magic = [0u8; 6];
        file.read_exact(&mut magic)?;
        
        if &magic != b"\x93NUMPY" {
            return Err(anyhow!("ìœ íš¨í•˜ì§€ ì•Šì€ NumPy íŒŒì¼"));
        }
        
        let mut version = [0u8; 2];
        file.read_exact(&mut version)?;
        
        let header_len = if version[0] == 1 {
            let mut len_bytes = [0u8; 2];
            file.read_exact(&mut len_bytes)?;
            u16::from_le_bytes(len_bytes) as usize
        } else {
            let mut len_bytes = [0u8; 4];
            file.read_exact(&mut len_bytes)?;
            u32::from_le_bytes(len_bytes) as usize
        };
        
        let mut header = vec![0u8; header_len];
        file.read_exact(&mut header)?;
        let header_str = String::from_utf8_lossy(&header);
        
        // shape ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
        let shape_start = header_str.find("'shape': (").unwrap_or(0) + 10;
        let shape_end = header_str[shape_start..].find(')').unwrap_or(0) + shape_start;
        let shape_str = &header_str[shape_start..shape_end];
        
        let shape: Vec<usize> = shape_str.split(", ")
            .filter(|s| !s.is_empty())
            .filter_map(|s| s.trim_end_matches(',').parse().ok())
            .collect();
        
        let total_size = shape.iter().product();
        
        Ok((shape, total_size))
    }
    
    /// ëª¨ë¸ ì„¤ì • ë¡œë“œ (JSON íŒŒì¼ì—ì„œ)
    pub fn load_model_config(&self, model_path: &Path) -> Result<ModelConfig> {
        let config_file = model_path.join("config.json");
        
        if !config_file.exists() {
            // ê¸°ë³¸ê°’ ì„¤ì •
            return Ok(ModelConfig {
                vocab_size: 50257,
                hidden_size: 768,
                num_layers: 12,
                num_heads: 12,
                max_length: 1024,
            });
        }
        
        let config_content = std::fs::read_to_string(config_file)?;
        let config_json: Value = serde_json::from_str(&config_content)?;
        
        Ok(ModelConfig {
            vocab_size: config_json["vocab_size"].as_u64().unwrap_or(50257) as usize,
            hidden_size: config_json["n_embd"].as_u64().unwrap_or(768) as usize,
            num_layers: config_json["n_layer"].as_u64().unwrap_or(12) as usize,
            num_heads: config_json["n_head"].as_u64().unwrap_or(12) as usize,
            max_length: config_json["n_positions"].as_u64().unwrap_or(1024) as usize,
        })
    }
}

/// ëª¨ë¸ ì„¤ì • êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
pub struct ModelConfig {
    pub vocab_size: usize,
    pub hidden_size: usize,
    pub num_layers: usize,
    pub num_heads: usize,
    pub max_length: usize,
}

/// ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € (ê¸°ì¡´ ë°©ì‹)
pub struct SimpleTokenizer {
    vocab: HashMap<u32, String>,
}

impl SimpleTokenizer {
    /// í† í¬ë‚˜ì´ì € ë¡œë“œ
    pub fn load(model_path: &Path) -> Result<Self> {
        let tokenizer_file = model_path.join("tokenizer.json");
        
        if !tokenizer_file.exists() {
            return Err(anyhow!("tokenizer.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {:?}", tokenizer_file));
        }
        
        // tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        let tokenizer = tokenizers::Tokenizer::from_file(&tokenizer_file)
            .map_err(|e| anyhow!("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {}", e))?;
        
        let vocab = tokenizer.get_vocab(false);
        let id_to_token: HashMap<u32, String> = vocab.into_iter()
            .map(|(token, id)| (id, token))
            .collect();
        
        Ok(Self {
            vocab: id_to_token,
        })
    }
    
    /// í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜ (ê°„ë‹¨í•œ êµ¬í˜„)
    pub fn encode(&self, text: &str) -> Result<Vec<i64>> {
        // ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ í† í¬ë‚˜ì´ì§•
        let mut tokens = Vec::new();
        let chars: Vec<char> = text.chars().collect();
        
        for window in chars.chunks(3) {
            let chunk: String = window.iter().collect();
            let hash = chunk.chars().fold(0u32, |acc, c| acc.wrapping_mul(31).wrapping_add(c as u32));
            let token_id = (hash % self.vocab.len() as u32) as i64;
            tokens.push(token_id);
        }
        
        Ok(tokens)
    }
    
    /// í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    pub fn decode(&self, token_ids: &[i64]) -> Result<String> {
        let tokens: Vec<String> = token_ids.iter()
            .filter_map(|&id| {
                if id >= 0 && id < self.vocab.len() as i64 {
                    self.vocab.get(&(id as u32)).cloned()
                } else {
                    Some("[UNK]".to_string())
                }
            })
            .collect();
        
        Ok(tokens.join(" "))
    }
} 