use crate::sllm::{RBEInferenceEngine, SLLMCompressor};
use std::sync::Arc;
use std::path::PathBuf;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};
use warp::{Filter, Reply, Rejection};
use anyhow::Result;
// use std::collections::HashMap;

/// API ì„œë²„ êµ¬ì¡°ì²´
pub struct RBEApiServer {
    /// ì¶”ë¡  ì—”ì§„ (ìŠ¤ë ˆë“œ ì•ˆì „)
    engine: Arc<RwLock<Option<RBEInferenceEngine>>>,
    /// ì„œë²„ ì„¤ì •
    config: ServerConfig,
    /// ë¡œë“œëœ ëª¨ë¸ ì •ë³´
    model_info: Arc<RwLock<Option<ModelInfo>>>,
}

/// ì„œë²„ ì„¤ì •
#[derive(Debug, Clone)]
pub struct ServerConfig {
    pub host: String,
    pub port: u16,
    pub max_tokens: usize,
    pub default_temperature: f32,
    pub default_top_p: f32,
    pub enable_cors: bool,
}

impl Default for ServerConfig {
    fn default() -> Self {
        Self {
            host: "0.0.0.0".to_string(),
            port: 8080,
            max_tokens: 100,
            default_temperature: 0.7,
            default_top_p: 0.9,
            enable_cors: true,
        }
    }
}

/// ëª¨ë¸ ì •ë³´
#[derive(Debug, Clone, Serialize)]
pub struct ModelInfo {
    pub name: String,
    pub compression_ratio: f32,
    pub average_rmse: f32,
    pub vocab_size: usize,
    pub hidden_size: usize,
    pub num_layers: usize,
    pub loaded_at: String,
}

/// í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­
#[derive(Debug, Deserialize)]
pub struct GenerateRequest {
    pub prompt: String,
    #[serde(default)]
    pub max_tokens: Option<usize>,
    #[serde(default)]
    pub temperature: Option<f32>,
    #[serde(default)]
    pub top_p: Option<f32>,
    #[serde(default)]
    pub stream: bool,
}

/// í…ìŠ¤íŠ¸ ìƒì„± ì‘ë‹µ
#[derive(Debug, Serialize)]
pub struct GenerateResponse {
    pub text: String,
    pub tokens_generated: usize,
    pub generation_time_ms: u64,
    pub tokens_per_second: f32,
}

/// ëª¨ë¸ ì••ì¶• ìš”ì²­
#[derive(Debug, Deserialize)]
pub struct CompressRequest {
    pub model_path: String,
    pub output_path: String,
    #[serde(default)]
    pub compression_level: Option<u8>,
    #[serde(default)]
    pub block_size: Option<usize>,
}

/// ëª¨ë¸ ì••ì¶• ì‘ë‹µ
#[derive(Debug, Serialize)]
pub struct CompressResponse {
    pub success: bool,
    pub message: String,
    pub compression_ratio: Option<f32>,
    pub compression_time_seconds: Option<f64>,
}

/// ëª¨ë¸ ë¡œë”© ìš”ì²­
#[derive(Debug, Deserialize)]
pub struct LoadModelRequest {
    pub compressed_model_path: String,
    pub original_model_path: String,
}

/// ë²¤ì¹˜ë§ˆí¬ ìš”ì²­
#[derive(Debug, Deserialize)]
pub struct BenchmarkRequest {
    #[serde(default)]
    pub matrix_sizes: Option<Vec<(usize, usize)>>,
    #[serde(default)]
    pub iterations: Option<usize>,
}

/// API ì—ëŸ¬ ì‘ë‹µ
#[derive(Debug, Serialize)]
pub struct ErrorResponse {
    pub error: String,
    pub code: u16,
}

impl RBEApiServer {
    /// ìƒˆë¡œìš´ API ì„œë²„ ìƒì„±
    pub fn new(config: ServerConfig) -> Self {
        Self {
            engine: Arc::new(RwLock::new(None)),
            config,
            model_info: Arc::new(RwLock::new(None)),
        }
    }
    
    /// ì„œë²„ ì‹œì‘
    pub async fn start(self) -> Result<()> {
        let server = Arc::new(self);
        
        println!("ğŸš€ RBE API ì„œë²„ ì‹œì‘ ì¤‘...");
        println!("   ì£¼ì†Œ: http://{}:{}", server.config.host, server.config.port);
        println!("   ìµœëŒ€ í† í°: {}", server.config.max_tokens);
        
        // ë¼ìš°íŠ¸ ì„¤ì •
        let routes = Self::setup_routes(server.clone());
        
        // CORS ì„¤ì • (ì¼ë‹¨ ê¸°ë³¸ í™œì„±í™”)
        let routes = routes.with(warp::cors()
            .allow_any_origin()
            .allow_headers(vec!["content-type"])
            .allow_methods(vec!["GET", "POST", "PUT", "DELETE"]));
        
        // ì„œë²„ ì‹¤í–‰
        warp::serve(routes)
            .run((
                server.config.host.parse::<std::net::IpAddr>()?,
                server.config.port
            ))
            .await;
        
        Ok(())
    }
    
    /// ë¼ìš°íŠ¸ ì„¤ì •
    fn setup_routes(
        server: Arc<RBEApiServer>,
    ) -> impl Filter<Extract = impl Reply, Error = Rejection> + Clone {
        // í—¬ìŠ¤ì²´í¬
        let health = warp::path("health")
            .and(warp::get())
            .map(|| {
                warp::reply::json(&serde_json::json!({
                    "status": "healthy",
                    "service": "RBE API Server"
                }))
            });
        
        // ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        let model_info = warp::path("model")
            .and(warp::path("info"))
            .and(warp::get())
            .and(with_server(server.clone()))
            .and_then(Self::get_model_info);
        
        // ëª¨ë¸ ë¡œë”©
        let load_model = warp::path("model")
            .and(warp::path("load"))
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(server.clone()))
            .and_then(Self::load_model);
        
        // ëª¨ë¸ ì••ì¶•
        let compress_model = warp::path("model")
            .and(warp::path("compress"))
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(server.clone()))
            .and_then(Self::compress_model);
        
        // í…ìŠ¤íŠ¸ ìƒì„±
        let generate = warp::path("generate")
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(server.clone()))
            .and_then(Self::generate_text);
        
        // ë²¤ì¹˜ë§ˆí¬
        let benchmark = warp::path("benchmark")
            .and(warp::post())
            .and(warp::body::json())
            .and(with_server(server.clone()))
            .and_then(Self::run_benchmark);
        
        // ì •ì  íŒŒì¼ ì„œë¹™ (API ë¬¸ì„œ ë“±)
        let static_files = warp::path("docs")
            .and(warp::fs::dir("docs/api/"));
        
        health
            .or(model_info)
            .or(load_model)
            .or(compress_model)
            .or(generate)
            .or(benchmark)
            .or(static_files)
    }
    
    /// ëª¨ë¸ ì •ë³´ ì¡°íšŒ í•¸ë“¤ëŸ¬
    async fn get_model_info(server: Arc<RBEApiServer>) -> Result<impl Reply, Rejection> {
        let model_info = server.model_info.read().await;
        
        match model_info.as_ref() {
            Some(info) => Ok(warp::reply::json(info)),
            None => Ok(warp::reply::json(&ErrorResponse {
                error: "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤".to_string(),
                code: 404,
            })),
        }
    }
    
    /// ëª¨ë¸ ë¡œë”© í•¸ë“¤ëŸ¬
    async fn load_model(
        request: LoadModelRequest,
        server: Arc<RBEApiServer>,
    ) -> Result<impl Reply, Rejection> {
        println!("ğŸ“¥ ëª¨ë¸ ë¡œë”© ìš”ì²­: {:?}", request);
        
        let compressed_path = PathBuf::from(&request.compressed_model_path);
        let original_path = PathBuf::from(&request.original_model_path);
        
        match RBEInferenceEngine::from_compressed_model(&compressed_path, &original_path).await {
            Ok(engine) => {
                // ëª¨ë¸ ì •ë³´ ìƒì„±
                let info = ModelInfo {
                    name: engine.get_model_name().to_string(),
                    compression_ratio: engine.get_compression_ratio(),
                    average_rmse: engine.get_average_rmse(),
                    vocab_size: engine.get_vocab_size(),
                    hidden_size: engine.get_hidden_size(),
                    num_layers: engine.get_num_layers(),
                    loaded_at: chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC").to_string(),
                };
                
                // ì—”ì§„ê³¼ ì •ë³´ ì €ì¥
                *server.engine.write().await = Some(engine);
                *server.model_info.write().await = Some(info.clone());
                
                println!("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {}", info.name);
                
                Ok(warp::reply::json(&serde_json::json!({
                    "success": true,
                    "message": "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "model_info": info
                })))
            }
            Err(e) => {
                println!("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {}", e);
                Ok(warp::reply::json(&ErrorResponse {
                    error: format!("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {}", e),
                    code: 500,
                }))
            }
        }
    }
    
    /// ëª¨ë¸ ì••ì¶• í•¸ë“¤ëŸ¬
    async fn compress_model(
        request: CompressRequest,
        _server: Arc<RBEApiServer>,
    ) -> Result<impl Reply, Rejection> {
        println!("ğŸ—œï¸ ëª¨ë¸ ì••ì¶• ìš”ì²­: {:?}", request);
        
        let model_path = PathBuf::from(&request.model_path);
        let output_path = PathBuf::from(&request.output_path);
        
        // ì••ì¶• ì„¤ì •
        let mut config = crate::sllm::CompressionConfig::default();
        if let Some(level) = request.compression_level {
            config.compression_level = level;
        }
        if let Some(block_size) = request.block_size {
            config.block_size = block_size;
        }
        
        let compressor = SLLMCompressor::new(config);
        
        match compressor.compress_safetensors_model(&model_path, &output_path).await {
            Ok(compressed_model) => {
                println!("âœ… ëª¨ë¸ ì••ì¶• ì™„ë£Œ");
                
                Ok(warp::reply::json(&CompressResponse {
                    success: true,
                    message: "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì••ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤".to_string(),
                    compression_ratio: Some(compressed_model.total_compression_ratio),
                    compression_time_seconds: Some(compressed_model.compression_time),
                }))
            }
            Err(e) => {
                println!("âŒ ëª¨ë¸ ì••ì¶• ì‹¤íŒ¨: {}", e);
                Ok(warp::reply::json(&CompressResponse {
                    success: false,
                    message: format!("ëª¨ë¸ ì••ì¶• ì‹¤íŒ¨: {}", e),
                    compression_ratio: None,
                    compression_time_seconds: None,
                }))
            }
        }
    }
    
    /// í…ìŠ¤íŠ¸ ìƒì„± í•¸ë“¤ëŸ¬
    async fn generate_text(
        request: GenerateRequest,
        server: Arc<RBEApiServer>,
    ) -> Result<impl Reply, Rejection> {
        println!("ğŸ’­ í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­: '{}'", request.prompt);
        
        let engine_guard = server.engine.read().await;
        let engine = match engine_guard.as_ref() {
            Some(e) => e,
            None => {
                return Ok(warp::reply::json(&ErrorResponse {
                    error: "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤".to_string(),
                    code: 404,
                }))
            }
        };
        
        // ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        let max_tokens = request.max_tokens.unwrap_or(server.config.max_tokens);
        let temperature = request.temperature.unwrap_or(server.config.default_temperature);
        let top_p = request.top_p.unwrap_or(server.config.default_top_p);
        
        let start_time = std::time::Instant::now();
        
        match engine.generate_text(&request.prompt, max_tokens, temperature, top_p) {
            Ok(generated_text) => {
                let generation_time = start_time.elapsed();
                let tokens_generated = generated_text.split_whitespace().count(); // ê°„ë‹¨í•œ í† í° ì¹´ìš´íŠ¸
                let tokens_per_second = tokens_generated as f32 / generation_time.as_secs_f32();
                
                println!("âœ… í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ ({:.2}ì´ˆ)", generation_time.as_secs_f32());
                
                Ok(warp::reply::json(&GenerateResponse {
                    text: generated_text,
                    tokens_generated,
                    generation_time_ms: generation_time.as_millis() as u64,
                    tokens_per_second,
                }))
            }
            Err(e) => {
                println!("âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {}", e);
                Ok(warp::reply::json(&ErrorResponse {
                    error: format!("í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {}", e),
                    code: 500,
                }))
            }
        }
    }
    
    /// ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í•¸ë“¤ëŸ¬
    async fn run_benchmark(
        request: BenchmarkRequest,
        _server: Arc<RBEApiServer>,
    ) -> Result<impl Reply, Rejection> {
        println!("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìš”ì²­");
        
        // ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
        let mut benchmark_config = crate::sllm::BenchmarkConfig::default();
        if let Some(iterations) = request.iterations {
            benchmark_config.iterations = iterations;
        }
        
        let mut benchmark = crate::sllm::SLLMBenchmark::new(benchmark_config);
        
        // í–‰ë ¬ í¬ê¸° ì„¤ì •
        let matrix_sizes = request.matrix_sizes.unwrap_or_else(|| vec![
            (256, 256),
            (512, 512),
            (1024, 1024),
        ]);
        
        // RBE ì••ì¶• ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark.benchmark_rbe_compression(&matrix_sizes);
        
        // í† í° ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        let text_lengths = vec![100, 500, 1000, 5000];
        benchmark.benchmark_token_processing(&text_lengths);
        
        // ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        let context_lengths = vec![128, 256, 512, 1024];
        benchmark.benchmark_inference_speed(&context_lengths);
        
        benchmark.print_summary();
        
        println!("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ");
        
        Ok(warp::reply::json(&serde_json::json!({
            "success": true,
            "message": "ë²¤ì¹˜ë§ˆí¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        })))
    }
}

/// ì„œë²„ ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ í—¬í¼
fn with_server(
    server: Arc<RBEApiServer>,
) -> impl Filter<Extract = (Arc<RBEApiServer>,), Error = std::convert::Infallible> + Clone {
    warp::any().map(move || server.clone())
} 