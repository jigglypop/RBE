use crate::packed_params::{Packed128, HybridEncodedBlock};
use crate::decoder::FusedForwardPass;
use crate::sllm::{CompressedModel, CompressedLayer, SimpleModelLoader, SimpleTokenizer};
use crate::sllm::simple_loader::ModelConfig;
use std::collections::HashMap;
use std::path::Path;
use anyhow::{Result, anyhow};
use nalgebra::{DVector, DMatrix};
use rand::{Rng, SeedableRng};

/// RBE ì¶”ë¡  ì—”ì§„
pub struct RBEInferenceEngine {
    /// ì••ì¶•ëœ ëª¨ë¸
    compressed_model: CompressedModel,
    /// ëª¨ë¸ ì„¤ì •
    config: ModelConfig,
    /// í† í¬ë‚˜ì´ì €
    tokenizer: SimpleTokenizer,
    /// ìœµí•© ìˆœì „íŒŒ ì—”ì§„
    fused_forward: FusedForwardPass,
    /// ì••ì¶•ëœ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë“¤
    layer_weights: HashMap<String, LayerWeights>,
}

/// ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ì •ë³´
#[derive(Debug)]
struct LayerWeights {
    /// RBE ì••ì¶•ëœ ë¸”ë¡ë“¤
    compressed_blocks: Vec<HybridEncodedBlock>,
    /// ì›ë³¸ í˜•íƒœ ì •ë³´
    shape: (usize, usize), // (rows, cols)
    /// ë ˆì´ì–´ íƒ€ì…
    layer_type: LayerType,
}

/// ë ˆì´ì–´ íƒ€ì…
#[derive(Debug, Clone, PartialEq)]
enum LayerType {
    Embedding,
    Attention,
    FeedForward,
    LayerNorm,
    Output,
}

impl RBEInferenceEngine {
    /// ì••ì¶•ëœ ëª¨ë¸ë¡œë¶€í„° ì¶”ë¡  ì—”ì§„ ìƒì„±
    pub async fn from_compressed_model(
        compressed_model_path: &Path,
        original_model_path: &Path,
    ) -> Result<Self> {
        println!("ğŸš€ RBE ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì¤‘...");
        
        // 1. ì••ì¶•ëœ ëª¨ë¸ ë¡œë“œ
        let model_content = std::fs::read_to_string(compressed_model_path)?;
        let compressed_model: CompressedModel = serde_json::from_str(&model_content)?;
        
        println!("ğŸ“Š ì••ì¶•ëœ ëª¨ë¸ ì •ë³´:");
        println!("   - ëª¨ë¸ëª…: {}", compressed_model.model_name);
        println!("   - ë ˆì´ì–´ ìˆ˜: {}", compressed_model.layers.len());
        println!("   - ì••ì¶•ë¥ : {:.1}:1", compressed_model.total_compression_ratio);
        
        // 2. ì›ë³¸ ëª¨ë¸ ì„¤ì • ë¡œë“œ
        let simple_loader = SimpleModelLoader::new()?;
        let config = simple_loader.load_model_config(original_model_path)?;
        
        println!("ğŸ”§ ëª¨ë¸ ì„¤ì •:");
        println!("   - ì–´íœ˜ í¬ê¸°: {}", config.vocab_size);
        println!("   - ì€ë‹‰ì¸µ í¬ê¸°: {}", config.hidden_size);
        println!("   - ë ˆì´ì–´ ìˆ˜: {}", config.num_layers);
        
        // 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
        let tokenizer = SimpleTokenizer::load(original_model_path)?;
        
        // 4. ìœµí•© ìˆœì „íŒŒ ì—”ì§„ ì´ˆê¸°í™”
        let fused_forward = FusedForwardPass::new();
        
        // 5. ë ˆì´ì–´ ê°€ì¤‘ì¹˜ êµ¬ì¡°í™”
        let layer_weights = Self::organize_layer_weights(&compressed_model)?;
        
        println!("âœ… RBE ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!");
        
        Ok(Self {
            compressed_model,
            config,
            tokenizer,
            fused_forward,
            layer_weights,
        })
    }
    
    /// ì›ë³¸ ëª¨ë¸ì„ ì••ì¶•í•˜ì—¬ ì¶”ë¡  ì—”ì§„ ìƒì„±
    pub async fn from_original_model(
        model_path: &Path,
        output_path: &Path,
    ) -> anyhow::Result<Self> {
        println!("ğŸ—œï¸ ì›ë³¸ ëª¨ë¸ ì••ì¶• ì¤‘...");
        
        // 1. ì›ë³¸ ëª¨ë¸ ì••ì¶•
        let compressor = crate::sllm::SLLMCompressor::new(Default::default());
        let compressed_model = compressor.compress_safetensors_model(model_path, output_path).await
            .map_err(|e| anyhow::anyhow!("ì••ì¶• ì‹¤íŒ¨: {}", e))?;
        
        // 2. ì••ì¶•ëœ ëª¨ë¸ë¡œ ì¶”ë¡  ì—”ì§„ ìƒì„±
        Self::from_compressed_model(output_path, model_path).await
    }
    
    /// ë ˆì´ì–´ ê°€ì¤‘ì¹˜ êµ¬ì¡°í™”
    fn organize_layer_weights(compressed_model: &CompressedModel) -> Result<HashMap<String, LayerWeights>> {
        let mut layer_weights = HashMap::new();
        
        for (layer_name, compressed_layer) in &compressed_model.layers {
            let layer_type = Self::classify_layer_type(layer_name);
            
            let weights = LayerWeights {
                compressed_blocks: compressed_layer.compressed_data.clone(),
                shape: (compressed_layer.shape[0], compressed_layer.shape[1]),
                layer_type,
            };
            
            layer_weights.insert(layer_name.clone(), weights);
        }
        
        Ok(layer_weights)
    }
    
    /// ë ˆì´ì–´ íƒ€ì… ë¶„ë¥˜
    fn classify_layer_type(layer_name: &str) -> LayerType {
        if layer_name.contains("embed") {
            LayerType::Embedding
        } else if layer_name.contains("attn") || layer_name.contains("attention") {
            LayerType::Attention
        } else if layer_name.contains("mlp") || layer_name.contains("fc") || layer_name.contains("linear") {
            LayerType::FeedForward
        } else if layer_name.contains("ln") || layer_name.contains("norm") {
            LayerType::LayerNorm
        } else if layer_name.contains("lm_head") || layer_name.contains("output") {
            LayerType::Output
        } else {
            LayerType::FeedForward // ê¸°ë³¸ê°’
        }
    }
    
    /// í…ìŠ¤íŠ¸ ìƒì„±
    pub fn generate_text(
        &self,
        prompt: &str,
        max_length: usize,
        temperature: f32,
        top_p: f32,
    ) -> Result<String> {
        println!("ğŸ’­ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: '{}'", prompt);
        
        // 1. í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
        let mut token_ids = self.tokenizer.encode(prompt)?;
        println!("ğŸ”¤ ì´ˆê¸° í† í° ìˆ˜: {}", token_ids.len());
        
        // 2. í…ìŠ¤íŠ¸ ìƒì„± ë£¨í”„
        for step in 0..max_length {
            if step % 10 == 0 {
                println!("ğŸ“ ìƒì„± ì§„í–‰: {}/{}", step, max_length);
            }
            
            // 3. ë‹¤ìŒ í† í° ì˜ˆì¸¡
            let next_token = self.predict_next_token(&token_ids, temperature, top_p)?;
            
            // 4. í† í° ì¶”ê°€
            token_ids.push(next_token);
            
            // 5. ì¢…ë£Œ ì¡°ê±´ ì²´í¬ (EOS í† í° ë“±)
            if next_token == 50256 { // GPT-2 EOS í† í°
                println!("ğŸ EOS í† í° ê°ì§€, ìƒì„± ì¢…ë£Œ");
                break;
            }
            
            // 6. ìµœëŒ€ ê¸¸ì´ ì²´í¬
            if token_ids.len() >= self.config.max_length {
                println!("ğŸ“ ìµœëŒ€ ê¸¸ì´ ë„ë‹¬, ìƒì„± ì¢…ë£Œ");
                break;
            }
        }
        
        // 7. í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        let generated_text = self.tokenizer.decode(&token_ids)?;
        
        println!("âœ… í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ì´ {} í† í°)", token_ids.len());
        Ok(generated_text)
    }
    
    /// ë‹¤ìŒ í† í° ì˜ˆì¸¡
    fn predict_next_token(
        &self,
        input_tokens: &[i64],
        temperature: f32,
        top_p: f32,
    ) -> Result<i64> {
        // 1. ì„ë² ë”© ë ˆì´ì–´ ì ìš©
        let embeddings = self.apply_embedding(input_tokens)?;
        
        // 2. íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ ì ìš©
        let mut hidden_states = embeddings;
        for layer_idx in 0..self.config.num_layers {
            hidden_states = self.apply_transformer_layer(&hidden_states, layer_idx)?;
        }
        
        // 3. ìµœì¢… ì¶œë ¥ ë ˆì´ì–´ ì ìš©
        let logits = self.apply_output_layer(&hidden_states)?;
        
        // 4. ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“ ì¶”ì¶œ
        let last_token_logits = logits.row(input_tokens.len() - 1);
        
        // 5. ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ìŒ í† í° ì„ íƒ
        let logits_vec: Vec<f32> = last_token_logits.iter().cloned().collect();
        let next_token = self.sample_token(&logits_vec, temperature, top_p)?;
        
        Ok(next_token)
    }
    
    /// ì„ë² ë”© ë ˆì´ì–´ ì ìš©
    fn apply_embedding(&self, token_ids: &[i64]) -> Result<DMatrix<f32>> {
        // ë‹¨ìˆœí™”: ì„ë² ë”©ì€ ëœë¤ ë²¡í„°ë¡œ ê·¼ì‚¬
        let seq_len = token_ids.len();
        let hidden_size = self.config.hidden_size;
        
        let mut embeddings = DMatrix::zeros(seq_len, hidden_size);
        
        for (i, &token_id) in token_ids.iter().enumerate() {
            // í† í° IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ì •ì  "ì„ë² ë”©" ìƒì„±
            let mut rng = rand::rngs::StdRng::seed_from_u64(token_id as u64);
            for j in 0..hidden_size {
                embeddings[(i, j)] = rng.gen_range(-0.1..0.1);
            }
        }
        
        Ok(embeddings)
    }
    
    /// íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì ìš©
    fn apply_transformer_layer(
        &self,
        hidden_states: &DMatrix<f32>,
        layer_idx: usize,
    ) -> Result<DMatrix<f32>> {
        // ê°„ì†Œí™”ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´
        // ì‹¤ì œë¡œëŠ” attention + feedforward êµ¬í˜„ í•„ìš”
        
        let seq_len = hidden_states.nrows();
        let hidden_size = hidden_states.ncols();
        
        // 1. Self-Attention (ê°„ì†Œí™”)
        let attention_output = self.apply_attention(hidden_states, layer_idx)?;
        
        // 2. Add & Norm
        let mut normed1 = &attention_output + hidden_states;
        self.apply_layer_norm(&mut normed1);
        
        // 3. Feed-Forward
        let ff_output = self.apply_feedforward(&normed1, layer_idx)?;
        
        // 4. Add & Norm
        let mut final_output = &ff_output + &normed1;
        self.apply_layer_norm(&mut final_output);
        
        Ok(final_output)
    }
    
    /// Attention ì ìš© (ê°„ì†Œí™”)
    fn apply_attention(
        &self,
        hidden_states: &DMatrix<f32>,
        layer_idx: usize,
    ) -> Result<DMatrix<f32>> {
        // ê°„ì†Œí™”: ë‹¨ìœ„ í–‰ë ¬ë¡œ ê·¼ì‚¬ (ì‹¤ì œë¡œëŠ” RBE ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        Ok(hidden_states.clone())
    }
    
    /// Feed-Forward ì ìš©
    fn apply_feedforward(
        &self,
        hidden_states: &DMatrix<f32>,
        layer_idx: usize,
    ) -> Result<DMatrix<f32>> {
        let layer_name = format!("transformer.h.{}.mlp.c_fc", layer_idx);
        
        if let Some(layer_weights) = self.layer_weights.get(&layer_name) {
            // RBE ê°€ì¤‘ì¹˜ë¡œ ì‹¤ì œ ì—°ì‚° ìˆ˜í–‰
            self.apply_rbe_layer(hidden_states, layer_weights)
        } else {
            // ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë©´ ë‹¨ìœ„ ë³€í™˜
            Ok(hidden_states.clone())
        }
    }
    
    /// RBE ë ˆì´ì–´ ì ìš©
    fn apply_rbe_layer(
        &self,
        input: &DMatrix<f32>,
        layer_weights: &LayerWeights,
    ) -> Result<DMatrix<f32>> {
        let seq_len = input.nrows();
        let input_dim = input.ncols();
        let output_dim = layer_weights.shape.1;
        
        let mut output = DMatrix::zeros(seq_len, output_dim);
        
        // ê° ì‹œí€€ìŠ¤ ìœ„ì¹˜ì— ëŒ€í•´ í–‰ë ¬ ê³±ì…ˆ ìˆ˜í–‰
        for seq_idx in 0..seq_len {
            let input_vec = input.row(seq_idx).transpose();
            let mut output_vec = vec![0.0f32; output_dim];
            
            // RBE ìœµí•© ìˆœì „íŒŒ ì ìš©
            for block in &layer_weights.compressed_blocks {
                // ì‹¤ì œë¡œëŠ” ë¸”ë¡ë³„ë¡œ ê°€ì¤‘ì¹˜ ìƒì„±í•˜ì—¬ ì—°ì‚°
                // í˜„ì¬ëŠ” ê°„ì†Œí™”ëœ êµ¬í˜„
                for i in 0..output_dim.min(input_dim) {
                    output_vec[i] += input_vec[i] * 0.1; // ê°„ì†Œí™”
                }
            }
            
            for (j, &val) in output_vec.iter().enumerate() {
                output[(seq_idx, j)] = val;
            }
        }
        
        Ok(output)
    }
    
    /// ì¶œë ¥ ë ˆì´ì–´ ì ìš©
    fn apply_output_layer(&self, hidden_states: &DMatrix<f32>) -> Result<DMatrix<f32>> {
        let seq_len = hidden_states.nrows();
        let vocab_size = self.config.vocab_size;
        
        // ê°„ì†Œí™”: ëœë¤ ë¡œì§“ ìƒì„±
        let mut logits = DMatrix::zeros(seq_len, vocab_size);
        
        for i in 0..seq_len {
            for j in 0..vocab_size {
                logits[(i, j)] = rand::random::<f32>() - 0.5;
            }
        }
        
        Ok(logits)
    }
    
    /// Layer Normalization ì ìš©
    fn apply_layer_norm(&self, tensor: &mut DMatrix<f32>) {
        let eps = 1e-5;
        
        for mut row in tensor.row_iter_mut() {
            let mean = row.sum() / row.len() as f32;
            let var = row.iter().map(|x| (x - mean).powi(2)).sum::<f32>() / row.len() as f32;
            let std = (var + eps).sqrt();
            
            for x in row.iter_mut() {
                *x = (*x - mean) / std;
            }
        }
    }
    
    /// í† í° ìƒ˜í”Œë§
    fn sample_token(&self, logits: &[f32], temperature: f32, top_p: f32) -> Result<i64> {
        if logits.is_empty() {
            return Err(anyhow!("ë¹ˆ ë¡œì§“ ë²¡í„°"));
        }
        
        // 1. Temperature ì ìš©
        let scaled_logits: Vec<f32> = logits.iter()
            .map(|&x| x / temperature)
            .collect();
        
        // 2. Softmax ì ìš©
        let max_logit = scaled_logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let exp_logits: Vec<f32> = scaled_logits.iter()
            .map(|&x| (x - max_logit).exp())
            .collect();
        let sum_exp: f32 = exp_logits.iter().sum();
        let probs: Vec<f32> = exp_logits.iter()
            .map(|&x| x / sum_exp)
            .collect();
        
        // 3. Top-p ìƒ˜í”Œë§
        let mut indexed_probs: Vec<(usize, f32)> = probs.iter()
            .enumerate()
            .map(|(i, &p)| (i, p))
            .collect();
        indexed_probs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        
        let mut cumulative_prob = 0.0;
        let mut top_p_indices = Vec::new();
        
        for (idx, prob) in indexed_probs {
            cumulative_prob += prob;
            top_p_indices.push(idx);
            if cumulative_prob >= top_p {
                break;
            }
        }
        
        // 4. ëœë¤ ìƒ˜í”Œë§
        let random_val: f32 = rand::random();
        let mut cumulative = 0.0;
        
        for &idx in &top_p_indices {
            cumulative += probs[idx];
            if random_val <= cumulative {
                return Ok(idx as i64);
            }
        }
        
        // í´ë°±: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í°
        Ok(top_p_indices[0] as i64)
    }
    
    /// ëª¨ë¸ ì •ë³´ ì¶œë ¥
    pub fn print_model_info(&self) {
        println!("\nğŸ“‹ === RBE ëª¨ë¸ ì •ë³´ ===");
        println!("ëª¨ë¸ëª…: {}", self.compressed_model.model_name);
        println!("ì••ì¶•ë¥ : {:.1}:1", self.compressed_model.total_compression_ratio);
        println!("í‰ê·  RMSE: {:.6}", self.compressed_model.average_rmse);
        println!("ì–´íœ˜ í¬ê¸°: {}", self.config.vocab_size);
        println!("ì€ë‹‰ì¸µ í¬ê¸°: {}", self.config.hidden_size);
        println!("ë ˆì´ì–´ ìˆ˜: {}", self.config.num_layers);
        
        println!("\nğŸ—œï¸ ì••ì¶•ëœ ë ˆì´ì–´:");
        for (name, weights) in &self.layer_weights {
            println!("  {} [{:?}]: {}Ã—{}", 
                     name, weights.layer_type, weights.shape.0, weights.shape.1);
        }
    }
    
    /// ì••ì¶•ëœ ëª¨ë¸ ì •ë³´ ì ‘ê·¼ì
    pub fn get_model_name(&self) -> &str {
        &self.compressed_model.model_name
    }
    
    pub fn get_compression_ratio(&self) -> f32 {
        self.compressed_model.total_compression_ratio
    }
    
    pub fn get_average_rmse(&self) -> f32 {
        self.compressed_model.average_rmse
    }
    
    pub fn get_vocab_size(&self) -> usize {
        self.config.vocab_size
    }
    
    pub fn get_hidden_size(&self) -> usize {
        self.config.hidden_size
    }
    
    pub fn get_num_layers(&self) -> usize {
        self.config.num_layers
    }
} 