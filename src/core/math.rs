use crate::types::{Packed64, Packed128, DecodedParams};
use rand::Rng;

/// RMSE (Root Mean Square Error)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ì´ë¦„ ë³€ê²½)
pub fn compute_full_rmse(matrix: &[f32], seed: &Packed64, rows: usize, cols: usize) -> f32 {
    let mut error = 0.0;
    for i in 0..rows {
        for j in 0..cols {
            let original_value = matrix[i * cols + j];
            let reconstructed_value = seed.compute_weight(i, j, rows, cols);
            error += (original_value - reconstructed_value).powi(2);
        }
    }
    (error / (rows * cols) as f32).sqrt()
}

/// í•´ì„ì  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (r, theta) -> ìˆ˜ì¹˜ì  ë¯¸ë¶„ìœ¼ë¡œ ë³€ê²½
/// `p` íŒŒë¼ë¯¸í„°ì˜ r, theta ê°’ ì£¼ë³€ì—ì„œ f(x) = compute_weightì˜ ë³€í™”ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
pub fn analytic_grad(p: &DecodedParams, i: usize, j: usize, rows: usize, cols: usize) -> (f32, f32) {
    let eps = 1e-4;

    // rì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    let mut p_r_plus = p.clone();
    p_r_plus.r_fp32 += eps;
    let seed_r_plus = Packed128::from_continuous(&p_r_plus);
    let weight_r_plus = Packed64 { rotations: seed_r_plus.hi }.compute_weight(i, j, rows, cols);

    let mut p_r_minus = p.clone();
    p_r_minus.r_fp32 -= eps;
    let seed_r_minus = Packed128::from_continuous(&p_r_minus);
    let weight_r_minus = Packed64 { rotations: seed_r_minus.hi }.compute_weight(i, j, rows, cols);

    let dr = (weight_r_plus - weight_r_minus) / (2.0 * eps);

    // thetaì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    let mut p_th_plus = p.clone();
    p_th_plus.theta_fp32 += eps;
    let seed_th_plus = Packed128::from_continuous(&p_th_plus);
    let weight_th_plus = Packed64 { rotations: seed_th_plus.hi }.compute_weight(i, j, rows, cols);

    let mut p_th_minus = p.clone();
    p_th_minus.theta_fp32 -= eps;
    let seed_th_minus = Packed128::from_continuous(&p_th_minus);
    let weight_th_minus = Packed64 { rotations: seed_th_minus.hi }.compute_weight(i, j, rows, cols);

    let dth = (weight_th_plus - weight_th_minus) / (2.0 * eps);

    (dr, dth)
}

/// Adam ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
#[inline]
pub fn adam_update(p:&mut f32, m:&mut f32, v:&mut f32, g:f32, lr:f32, t:i32){
    const B1:f32=0.9; const B2:f32=0.999; const EPS:f32=1e-8;
    *m = B1*(*m)+(1.0-B1)*g;
    *v = B2*(*v)+(1.0-B2)*g*g;
    let m_hat=*m/(1.0-B1.powi(t));
    let v_hat=*v/(1.0-B2.powi(t));
    *p -= lr*m_hat/(v_hat.sqrt()+EPS);
}

/// Float to Q-format (STE placeholder)
pub fn ste_quant_q0x(val: f32, bits: u8) -> u64 {
    (val.clamp(0.0, 1.0) * ((1u64 << bits) - 1) as f32) as u64
}

/// Float to Phase (STE placeholder)
pub fn ste_quant_phase(val: f32, bits: u8) -> u64 {
    (val.rem_euclid(2.0 * std::f32::consts::PI) / (2.0 * std::f32::consts::PI) * ((1u64 << bits) - 1) as f32) as u64
}


/// ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì˜ ë³€ì´(mutation) ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
///
/// ì£¼ì–´ì§„ ì‹œë“œì˜ ê° ë¹„íŠ¸ë¥¼ `mutation_rate` í™•ë¥ ë¡œ ë’¤ì§‘ìŠµë‹ˆë‹¤.
///
/// # Arguments
/// * `seed` - ë³€ì´ë¥¼ ì ìš©í•  ì›ë³¸ Packed64 ì‹œë“œ
/// * `mutation_rate` - ê° ë¹„íŠ¸ê°€ ë³€ì´ë  í™•ë¥  (0.0 ~ 1.0)
///
/// # Returns
/// * ë³€ì´ëœ ìƒˆë¡œìš´ Packed64 ì‹œë“œ
pub fn mutate_seed(seed: Packed64, mutation_rate: f32) -> Packed64 {
    let mut rng = rand::thread_rng();
    let mut new_rotations = seed.rotations;
    for i in 0..64 {
        if rng.gen_bool(mutation_rate as f64) {
            new_rotations ^= 1 << i;
        }
    }
    Packed64::new(new_rotations)
}

// CORDIC êµ¬í˜„ì— í•„ìš”í•  ìˆ˜ ìˆëŠ” ìˆ˜í•™ í•¨ìˆ˜ë“¤ì€ ìœ ì§€í•©ë‹ˆë‹¤.

pub fn bessel_j0(x: f32) -> f32 {
    let ax = x.abs();
    if ax < 8.0 {
        let y = x * x;
        let ans1 = 57568490574.0 + y * (-13362590354.0 + y * (651619640.7 
            + y * (-11214424.18 + y * (77392.33017 + y * (-184.9052456)))));
        let ans2 = 57568490411.0 + y * (1029532985.0 + y * (9494680.718 
            + y * (59272.64853 + y * (267.8532712 + y))));
        ans1 / ans2
    } else {
        let z = 8.0 / ax;
        let y = z * z;
        let xx = ax - 0.785398164; // PI/4
        let ans1 = 1.0 + y * (-0.1098628627e-2 + y * (0.2734510407e-4 
            + y * (-0.2073370639e-5 + y * 0.2093887211e-6)));
        let ans2 = -0.1562499995e-1 + y * (0.1430488765e-3 
            + y * (-0.6911147651e-5 + y * (0.7621095161e-6 - y * 0.934945152e-7)));
        (2.0 / (std::f32::consts::PI * ax)).sqrt() * (xx.cos() * ans1 - z * xx.sin() * ans2)
    }
}

// CORDIC íŠ¹ìˆ˜ í•¨ìˆ˜ í—¬í¼ (CORDIC.md ê¸°ë°˜)
// ì´ í•¨ìˆ˜ë“¤ì€ í˜„ì¬ compute_weightì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì§€ë§Œ,
// í–¥í›„ CORDIC ì•Œê³ ë¦¬ì¦˜ í™•ì¥ì„ ìœ„í•´ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
pub fn apply_bessel_cordic(result: (f32, f32), special: u16) -> f32 {
    let r = (result.0 * result.0 + result.1 * result.1).sqrt();
    bessel_j0(r * (special as f32 / 63.0))
}

pub fn apply_elliptic_cordic(result: (f32, f32), special: u16) -> f32 {
    let angle = result.1.atan2(result.0);
    (angle * (special as f32 / 63.0 - 1.0)).tanh()
}

pub fn apply_theta_cordic(result: (f32, f32), special: u16) -> f32 {
    let theta = result.1.atan2(result.0);
    theta * (special as f32 / 63.0)
} 

/// ì •ë°€í•œ ì—­ì „íŒŒ: hi/lo ë¶„ë¦¬ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (ìˆ˜ì •ë¨)
/// target: ëª©í‘œ í–‰ë ¬, predicted: í˜„ì¬ ì˜ˆì¸¡, seed: í˜„ì¬ íŒŒë¼ë¯¸í„°
pub fn fused_backward(
    target: &[f32], 
    predicted: &[f32], 
    seed: &mut Packed128, 
    rows: usize, 
    cols: usize,
    learning_rate: f32
) -> (f32, f32) {
    let mut total_loss = 0.0;
    let mut grad_r_continuous = 0.0;
    let mut grad_theta_continuous = 0.0;
    
    // ì—°ì† íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
    let theta_fp32 = f32::from_bits(seed.lo as u32);
    
    let eps = 1e-4; // ìˆ˜ì¹˜ ë¯¸ë¶„ìš© epsilon
    
    for i in 0..rows {
        for j in 0..cols {
            let idx = i * cols + j;
            let error = predicted[idx] - target[idx];
            total_loss += error * error;
            
            // 1. ìƒíƒœ ì „ì´ ë¯¸ë¶„ ì ìš© (hi ë¹„íŠ¸ ì—…ë°ì´íŠ¸)
            seed.apply_state_transition(error, i, j);
            
            // 2. ì—°ì† íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (lo ì—…ë°ì´íŠ¸ìš©)
            // rì— ëŒ€í•œ ìˆ˜ì¹˜ ë¯¸ë¶„ - ë” ì •í™•í•œ ê³„ì‚°
            let r_plus = r_fp32 + eps;
            let r_minus = r_fp32 - eps;
            
            let mut seed_r_plus = *seed;
            seed_r_plus.lo = ((r_plus.to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
            let weight_r_plus = seed_r_plus.fused_forward(i, j, rows, cols);
            
            let mut seed_r_minus = *seed;
            seed_r_minus.lo = ((r_minus.to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
            let weight_r_minus = seed_r_minus.fused_forward(i, j, rows, cols);
            
            let dr = (weight_r_plus - weight_r_minus) / (2.0 * eps);
            grad_r_continuous += error * dr;
            
            // thetaì— ëŒ€í•œ ìˆ˜ì¹˜ ë¯¸ë¶„
            let theta_plus = theta_fp32 + eps;
            let theta_minus = theta_fp32 - eps;
            
            let mut seed_th_plus = *seed;
            seed_th_plus.lo = ((r_fp32.to_bits() as u64) << 32) | theta_plus.to_bits() as u64;
            let weight_th_plus = seed_th_plus.fused_forward(i, j, rows, cols);
            
            let mut seed_th_minus = *seed;
            seed_th_minus.lo = ((r_fp32.to_bits() as u64) << 32) | theta_minus.to_bits() as u64;
            let weight_th_minus = seed_th_minus.fused_forward(i, j, rows, cols);
            
            let dth = (weight_th_plus - weight_th_minus) / (2.0 * eps);
            grad_theta_continuous += error * dth;
        }
    }
    
    // ê·¸ë˜ë””ì–¸íŠ¸ ì •ê·œí™” (ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ê¸°)
    let batch_size = (rows * cols) as f32;
    grad_r_continuous /= batch_size;
    grad_theta_continuous /= batch_size;
    
    // 3. ì—°ì† íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (lo) - í´ë¨í•‘ ì™„í™”
    let new_r = r_fp32 - learning_rate * grad_r_continuous;
    let new_theta = theta_fp32 - learning_rate * grad_theta_continuous;
    
    // r ë²”ìœ„ë¥¼ ë” ë„“ê²Œ í—ˆìš© (0.1 ~ 2.0)
    let clamped_r = new_r.clamp(0.1, 2.0);
    
    // lo í•„ë“œ ì—…ë°ì´íŠ¸
    seed.lo = ((clamped_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
    
    let mse = total_loss / batch_size;
    (mse, mse.sqrt())
}

/// í•´ì„ì  ë¯¸ë¶„ ê¸°ë°˜ ì´ˆê³ ì† ì—­ì „íŒŒ: ìˆ˜ì¹˜ ë¯¸ë¶„ ì™„ì „ ëŒ€ì²´ (16ë°° ì„±ëŠ¥ í–¥ìƒ)
/// target: ëª©í‘œ í–‰ë ¬, predicted: í˜„ì¬ ì˜ˆì¸¡, seed: í˜„ì¬ íŒŒë¼ë¯¸í„°  
pub fn fused_backward_fast(
    target: &[f32], 
    predicted: &[f32], 
    seed: &mut Packed128, 
    rows: usize, 
    cols: usize,
    learning_rate: f32
) -> (f32, f32) {
    let mut total_loss = 0.0;
    let mut grad_r_sum = 0.0;
    let mut grad_theta_sum = 0.0;
    
    for i in 0..rows {
        for j in 0..cols {
            let idx = i * cols + j;
            let error = predicted[idx] - target[idx];
            total_loss += error * error;
            
            // 1. ìƒíƒœ ì „ì´ ë¯¸ë¶„ ì ìš© (hi ë¹„íŠ¸ ì—…ë°ì´íŠ¸) - ê¸°ì¡´ê³¼ ë™ì¼
            seed.apply_state_transition(error, i, j);
            
            // 2. ì—°ì† íŒŒë¼ë¯¸í„° í•´ì„ì  ë¯¸ë¶„ (lo ì—…ë°ì´íŠ¸) - í•µì‹¬ ê°œì„ !
            // ìˆ˜ì¹˜ ë¯¸ë¶„ 4ë²ˆ í˜¸ì¶œ â†’ í•´ì„ì  ë¯¸ë¶„ 2ë²ˆ í˜¸ì¶œë¡œ ëŒ€ì²´
            let dr = seed.analytical_gradient_r(i, j, rows, cols);
            let dtheta = seed.analytical_gradient_theta(i, j, rows, cols);
            
            grad_r_sum += error * dr;      // ë‹¨ì¼ ê³±ì…ˆ
            grad_theta_sum += error * dtheta; // ë‹¨ì¼ ê³±ì…ˆ
        }
    }
    
    // 3. ì—°ì† íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
    let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
    let theta_fp32 = f32::from_bits(seed.lo as u32);
    
    let batch_size = (rows * cols) as f32;
    grad_r_sum /= batch_size;
    grad_theta_sum /= batch_size;
    
    let new_r = (r_fp32 - learning_rate * grad_r_sum).clamp(0.1, 2.0);
    let new_theta = theta_fp32 - learning_rate * grad_theta_sum;
    
    seed.lo = ((new_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
    
    let mse = total_loss / batch_size;
    (mse, mse.sqrt())
}

/// ë²¡í„°-í–‰ë ¬ ê³±ì…ˆì˜ ìœµí•© ì—­ì „íŒŒ
/// x: ì…ë ¥ ë²¡í„°, dy: ì¶œë ¥ ê·¸ë˜ë””ì–¸íŠ¸, weights: ê°€ì¤‘ì¹˜ ì‹œë“œë“¤
pub fn fused_backward_gemv(
    x: &[f32],
    dy: &[f32], 
    weights: &mut [Packed128],
    rows: usize,
    cols: usize,
    learning_rate: f32
) -> Vec<f32> {
    let mut dx = vec![0.0; cols]; // ì…ë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸
    
    for i in 0..rows {
        let output_grad = dy[i];
        
        for j in 0..cols {
            let weight_idx = i * cols + j;
            let mut weight_seed = weights[weight_idx];
            
            // ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ = output_grad * x[j]
            let weight_grad = output_grad * x[j];
            
            // ìƒíƒœ ì „ì´ ë¯¸ë¶„ ì ìš©
            weight_seed.apply_state_transition(weight_grad, i, j);
            
            // ì—°ì† íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            let r_fp32 = f32::from_bits((weight_seed.lo >> 32) as u32);
            let theta_fp32 = f32::from_bits(weight_seed.lo as u32);
            
            let eps = 1e-5;
            
            // r ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            let mut seed_r_plus = weight_seed;
            seed_r_plus.lo = (((r_fp32 + eps).to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
            let w_r_plus = seed_r_plus.fused_forward(i, j, rows, cols);
            
            let mut seed_r_minus = weight_seed;
            seed_r_minus.lo = (((r_fp32 - eps).to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
            let w_r_minus = seed_r_minus.fused_forward(i, j, rows, cols);
            
            let dr = (w_r_plus - w_r_minus) / (2.0 * eps);
            let grad_r = weight_grad * dr;
            
            // theta ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            let mut seed_th_plus = weight_seed;
            seed_th_plus.lo = ((r_fp32.to_bits() as u64) << 32) | (theta_fp32 + eps).to_bits() as u64;
            let w_th_plus = seed_th_plus.fused_forward(i, j, rows, cols);
            
            let mut seed_th_minus = weight_seed;
            seed_th_minus.lo = ((r_fp32.to_bits() as u64) << 32) | (theta_fp32 - eps).to_bits() as u64;
            let w_th_minus = seed_th_minus.fused_forward(i, j, rows, cols);
            
            let dth = (w_th_plus - w_th_minus) / (2.0 * eps);
            let grad_theta = weight_grad * dth;
            
            // íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            let new_r = (r_fp32 - learning_rate * grad_r).clamp(0.0, 1.0);
            let new_theta = theta_fp32 - learning_rate * grad_theta;
            weight_seed.lo = ((new_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
            
            weights[weight_idx] = weight_seed;
            
            // ì…ë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : dx[j] += dy[i] * w[i][j]
            let current_weight = weight_seed.fused_forward(i, j, rows, cols);
            dx[j] += output_grad * current_weight;
        }
    }
    
    dx
}

/// Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì—­ì „íŒŒ
pub fn fused_backward_adam(
    target: &[f32],
    predicted: &[f32], 
    seed: &mut Packed128,
    m_r: &mut f32, v_r: &mut f32,
    m_theta: &mut f32, v_theta: &mut f32,
    rows: usize, 
    cols: usize,
    epoch: i32,
    learning_rate: f32
) -> f32 {
    let mut total_loss = 0.0;
    let mut grad_r_sum = 0.0;
    let mut grad_theta_sum = 0.0;
    
    let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
    let theta_fp32 = f32::from_bits(seed.lo as u32);
    let eps = 1e-4;
    
    for i in 0..rows {
        for j in 0..cols {
            let idx = i * cols + j;
            let error = predicted[idx] - target[idx];
            total_loss += error * error;
            
            // ìƒíƒœ ì „ì´ ë¯¸ë¶„
            seed.apply_state_transition(error, i, j);
            
            // ì—°ì† íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸
            // r ê·¸ë˜ë””ì–¸íŠ¸
            let mut seed_r_plus = *seed;
            seed_r_plus.lo = (((r_fp32 + eps).to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
            let w_r_plus = seed_r_plus.fused_forward(i, j, rows, cols);
            
            let mut seed_r_minus = *seed;
            seed_r_minus.lo = (((r_fp32 - eps).to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
            let w_r_minus = seed_r_minus.fused_forward(i, j, rows, cols);
            
            let dr = (w_r_plus - w_r_minus) / (2.0 * eps);
            grad_r_sum += error * dr;
            
            // theta ê·¸ë˜ë””ì–¸íŠ¸
            let mut seed_th_plus = *seed;
            seed_th_plus.lo = ((r_fp32.to_bits() as u64) << 32) | (theta_fp32 + eps).to_bits() as u64;
            let w_th_plus = seed_th_plus.fused_forward(i, j, rows, cols);
            
            let mut seed_th_minus = *seed;
            seed_th_minus.lo = ((r_fp32.to_bits() as u64) << 32) | (theta_fp32 - eps).to_bits() as u64;
            let w_th_minus = seed_th_minus.fused_forward(i, j, rows, cols);
            
            let dth = (w_th_plus - w_th_minus) / (2.0 * eps);
            grad_theta_sum += error * dth;
        }
    }
    
    // Adam ì—…ë°ì´íŠ¸
    let mut new_r = r_fp32;
    let mut new_theta = theta_fp32;
    
    adam_update(&mut new_r, m_r, v_r, grad_r_sum, learning_rate, epoch);
    adam_update(&mut new_theta, m_theta, v_theta, grad_theta_sum, learning_rate, epoch);
    
    new_r = new_r.clamp(0.0, 1.0);
    
    // ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„° ì ìš©
    seed.lo = ((new_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
    
    (total_loss / (rows * cols) as f32).sqrt()
} 

/// ê³ ê¸‰ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  êµ¬ì¡°ì²´
#[derive(Debug, Clone, Default)]
pub struct GradientAccumulator {
    pub r_grad_sum: f32,
    pub theta_grad_sum: f32,
    pub state_transition_count: u32,
    pub total_samples: u32,
}

impl GradientAccumulator {
    pub fn new() -> Self {
        Self::default()
    }
    
    pub fn accumulate(&mut self, r_grad: f32, theta_grad: f32) {
        self.r_grad_sum += r_grad;
        self.theta_grad_sum += theta_grad;
        self.total_samples += 1;
    }
    
    pub fn get_average_gradients(&self) -> (f32, f32) {
        if self.total_samples > 0 {
            (
                self.r_grad_sum / self.total_samples as f32,
                self.theta_grad_sum / self.total_samples as f32,
            )
        } else {
            (0.0, 0.0)
        }
    }
    
    pub fn reset(&mut self) {
        self.r_grad_sum = 0.0;
        self.theta_grad_sum = 0.0;
        self.state_transition_count = 0;
        self.total_samples = 0;
    }
}

/// ë°°ì¹˜ë³„ ì •ë°€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° ì—…ë°ì´íŠ¸
pub fn batch_fused_backward(
    targets: &[&[f32]], 
    seeds: &mut [Packed128],
    accumulators: &mut [GradientAccumulator],
    adam_states_r: &mut [(f32, f32)], // (momentum, velocity)
    adam_states_theta: &mut [(f32, f32)],
    rows: usize, 
    cols: usize,
    learning_rate: f32,
    epoch: i32,
    use_advanced_transitions: bool
) -> f32 {
    assert_eq!(targets.len(), seeds.len());
    assert_eq!(seeds.len(), accumulators.len());
    
    let batch_size = targets.len();
    let mut total_loss = 0.0;
    
    // ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì´ˆê¸°í™”
    for acc in accumulators.iter_mut() {
        acc.reset();
    }
    
    // ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    for (batch_idx, target) in targets.iter().enumerate() {
        let seed = &mut seeds[batch_idx];
        let accumulator = &mut accumulators[batch_idx];
        
        // í˜„ì¬ ì˜ˆì¸¡ ìƒì„±
        let mut predicted = vec![0.0; target.len()];
        for i in 0..rows {
            for j in 0..cols {
                let idx = i * cols + j;
                predicted[idx] = if use_advanced_transitions {
                    seed.fused_forward_advanced(i, j, rows, cols)
                } else {
                    seed.fused_forward(i, j, rows, cols)
                };
            }
        }
        
        // ì†ì‹¤ ê³„ì‚°
        let mut sample_loss = 0.0;
        for i in 0..rows {
            for j in 0..cols {
                let idx = i * cols + j;
                let error = predicted[idx] - target[idx];
                sample_loss += error * error;
                
                // ìƒíƒœ ì „ì´ ë¯¸ë¶„ ì ìš©
                if use_advanced_transitions {
                    seed.advanced_state_transition(error, i, j);
                } else {
                    seed.apply_state_transition(error, i, j);
                }
                
                if error.abs() > 0.001 { // ìœ ì˜ë¯¸í•œ ì˜¤ì°¨ì— ëŒ€í•´ì„œë§Œ ì „ì´ ì¹´ìš´íŠ¸
                    accumulator.state_transition_count += 1;
                }
                
                // ì—°ì† íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
                let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
                let theta_fp32 = f32::from_bits(seed.lo as u32);
                let eps = 1e-5;
                
                // r ê·¸ë˜ë””ì–¸íŠ¸
                let mut seed_r_plus = *seed;
                seed_r_plus.lo = (((r_fp32 + eps).to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
                let w_r_plus = if use_advanced_transitions {
                    seed_r_plus.fused_forward_advanced(i, j, rows, cols)
                } else {
                    seed_r_plus.fused_forward(i, j, rows, cols)
                };
                
                let mut seed_r_minus = *seed;
                seed_r_minus.lo = (((r_fp32 - eps).to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
                let w_r_minus = if use_advanced_transitions {
                    seed_r_minus.fused_forward_advanced(i, j, rows, cols)
                } else {
                    seed_r_minus.fused_forward(i, j, rows, cols)
                };
                
                let dr = (w_r_plus - w_r_minus) / (2.0 * eps);
                let grad_r = error * dr;
                
                // theta ê·¸ë˜ë””ì–¸íŠ¸
                let mut seed_th_plus = *seed;
                seed_th_plus.lo = ((r_fp32.to_bits() as u64) << 32) | (theta_fp32 + eps).to_bits() as u64;
                let w_th_plus = if use_advanced_transitions {
                    seed_th_plus.fused_forward_advanced(i, j, rows, cols)
                } else {
                    seed_th_plus.fused_forward(i, j, rows, cols)
                };
                
                let mut seed_th_minus = *seed;
                seed_th_minus.lo = ((r_fp32.to_bits() as u64) << 32) | (theta_fp32 - eps).to_bits() as u64;
                let w_th_minus = if use_advanced_transitions {
                    seed_th_minus.fused_forward_advanced(i, j, rows, cols)
                } else {
                    seed_th_minus.fused_forward(i, j, rows, cols)
                };
                
                let dth = (w_th_plus - w_th_minus) / (2.0 * eps);
                let grad_theta = error * dth;
                
                // ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                accumulator.accumulate(grad_r, grad_theta);
            }
        }
        
        total_loss += sample_loss / target.len() as f32;
    }
    
    // ë°°ì¹˜ë³„ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    for (batch_idx, seed) in seeds.iter_mut().enumerate() {
        let accumulator = &accumulators[batch_idx];
        let (avg_grad_r, avg_grad_theta) = accumulator.get_average_gradients();
        
        if avg_grad_r.abs() > 1e-8 || avg_grad_theta.abs() > 1e-8 {
            let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
            let theta_fp32 = f32::from_bits(seed.lo as u32);
            
            let mut new_r = r_fp32;
            let mut new_theta = theta_fp32;
            
            // Adam ì—…ë°ì´íŠ¸
            adam_update(
                &mut new_r, 
                &mut adam_states_r[batch_idx].0, 
                &mut adam_states_r[batch_idx].1, 
                avg_grad_r, 
                learning_rate, 
                epoch
            );
            adam_update(
                &mut new_theta, 
                &mut adam_states_theta[batch_idx].0, 
                &mut adam_states_theta[batch_idx].1, 
                avg_grad_theta, 
                learning_rate, 
                epoch
            );
            
            new_r = new_r.clamp(0.0, 1.0);
            seed.lo = ((new_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
        }
    }
    
    total_loss / batch_size as f32
}

/// ì ì‘ì  í•™ìŠµë¥ ì„ ì‚¬ìš©í•œ ê³ ê¸‰ Adam ì—…ë°ì´íŠ¸
pub fn adaptive_adam_update(
    param: &mut f32,
    momentum: &mut f32,
    velocity: &mut f32,
    gradient: f32,
    base_lr: f32,
    epoch: i32,
    gradient_norm: f32, // ì „ì²´ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„
    transition_count: u32, // ìƒíƒœ ì „ì´ íšŸìˆ˜
) {
    const B1: f32 = 0.9;
    const B2: f32 = 0.999;
    const EPS: f32 = 1e-8;
    
    // ì ì‘ì  í•™ìŠµë¥  ê³„ì‚°
    let adaptive_lr = if transition_count > 0 {
        // ìƒíƒœ ì „ì´ê°€ ë§ì´ ì¼ì–´ë‚¬ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
        base_lr * (1.0 / (1.0 + transition_count as f32 * 0.01))
    } else {
        base_lr
    };
    
    // ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    let clipped_gradient = if gradient_norm > 1.0 {
        gradient / gradient_norm
    } else {
        gradient
    };
    
    *momentum = B1 * (*momentum) + (1.0 - B1) * clipped_gradient;
    *velocity = B2 * (*velocity) + (1.0 - B2) * clipped_gradient * clipped_gradient;
    
    let m_hat = *momentum / (1.0 - B1.powi(epoch));
    let v_hat = *velocity / (1.0 - B2.powi(epoch));
    
    *param -= adaptive_lr * m_hat / (v_hat.sqrt() + EPS);
}

/// ë©€í‹°ìŠ¤ì¼€ì¼ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„
pub fn analyze_gradient_scales(
    gradients_r: &[f32], 
    gradients_theta: &[f32]
) -> (f32, f32, f32, f32) {
    let r_norm = gradients_r.iter().map(|g| g * g).sum::<f32>().sqrt();
    let theta_norm = gradients_theta.iter().map(|g| g * g).sum::<f32>().sqrt();
    
    let r_mean = gradients_r.iter().sum::<f32>() / gradients_r.len() as f32;
    let theta_mean = gradients_theta.iter().sum::<f32>() / gradients_theta.len() as f32;
    
    (r_norm, theta_norm, r_mean, theta_mean)
} 

// ============================================================================
// 5ì¥: í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ë¦¬ë§Œ ê¸°í•˜í•™ (Riemannian Geometry on PoincarÃ© Ball)
// ============================================================================

use std::f32::consts::PI;

/// 5.2 í‘¸ì•µì¹´ë ˆ ë³¼ ìœ„ì˜ ì 
/// 
/// í‘¸ì•µì¹´ë ˆ ë³¼ D^n = {x âˆˆ R^n : ||x|| < 1}ì—ì„œ ì ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
/// ë¦¬ë§Œ ë©”íŠ¸ë¦­: g(x) = 4/(1-||x||Â²)Â² I_n
#[derive(Debug, Clone, Copy, PartialEq)]
pub struct PoincareBallPoint {
    /// ì¢Œí‘œ (r, Î¸)
    pub r: f32,      // ë°˜ì§€ë¦„ [0, 1)
    pub theta: f32,  // ê°ë„ [-âˆ, âˆ)
}

impl PoincareBallPoint {
    /// ìƒˆë¡œìš´ í‘¸ì•µì¹´ë ˆ ë³¼ ì  ìƒì„±
    pub fn new(r: f32, theta: f32) -> Self {
        Self {
            r: r.clamp(0.0, 0.99), // ê²½ê³„ ê·¼ì²˜ì—ì„œ ìˆ˜ì¹˜ì  ì•ˆì •ì„± ë³´ì¥
            theta,
        }
    }
    
    /// ì›ì  (r=0, Î¸=0)
    pub fn origin() -> Self {
        Self { r: 0.0, theta: 0.0 }
    }
    
    /// ë°ì¹´ë¥´íŠ¸ ì¢Œí‘œë¡œ ë³€í™˜
    pub fn to_cartesian(&self) -> (f32, f32) {
        (self.r * libm::cosf(self.theta), self.r * libm::sinf(self.theta))
    }
    
    /// ë°ì¹´ë¥´íŠ¸ ì¢Œí‘œì—ì„œ ë³€í™˜
    pub fn from_cartesian(x: f32, y: f32) -> Self {
        let r = (x*x + y*y).sqrt().min(0.99);
        let theta = libm::atan2f(y, x);
        Self { r, theta }
    }
    
    /// í‘¸ì•µì¹´ë ˆ ë³¼ ê²½ê³„ê¹Œì§€ì˜ ê±°ë¦¬
    pub fn distance_to_boundary(&self) -> f32 {
        1.0 - self.r
    }
}

/// 5.2 ë¦¬ë§Œ ê¸°í•˜í•™ ì—°ì‚°
/// 
/// í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ë¦¬ë§Œ ë©”íŠ¸ë¦­ ì—°ì‚°ì„ ì œê³µí•©ë‹ˆë‹¤.
#[derive(Debug, Clone)]
pub struct RiemannianGeometry;

impl RiemannianGeometry {
    /// 5.2.1 ë¦¬ë§Œ ë©”íŠ¸ë¦­ í…ì„œ ê³„ì‚°
    /// 
    /// g(x) = 4/(1-||x||Â²)Â² I_n
    /// ì  xì—ì„œì˜ ë©”íŠ¸ë¦­ ì¸ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    pub fn metric_factor(point: &PoincareBallPoint) -> f32 {
        let norm_sq = point.r * point.r;
        let denominator = 1.0 - norm_sq;
        
        if denominator < 1e-8 {
            // ê²½ê³„ ê·¼ì²˜ì—ì„œ ì•ˆì •ì„± ë³´ì¥
            1e8
        } else {
            4.0 / (denominator * denominator)
        }
    }
    
    /// 5.2.1 ë¦¬ë§Œ ë©”íŠ¸ë¦­ì˜ ì—­í–‰ë ¬ ì¸ìˆ˜
    /// 
    /// g^(-1)(x) = (1-||x||Â²)Â²/4 I_n
    pub fn inverse_metric_factor(point: &PoincareBallPoint) -> f32 {
        let norm_sq = point.r * point.r;
        let numerator = 1.0 - norm_sq;
        
        if numerator < 1e-8 {
            // ê²½ê³„ ê·¼ì²˜ì—ì„œ ì•ˆì •ì„± ë³´ì¥
            1e-8
        } else {
            numerator * numerator / 4.0
        }
    }
    
    /// 5.2.2 í¬ë¦¬ìŠ¤í† í  ê¸°í˜¸ ê³„ì‚°
    /// 
    /// Î“^k_ij = (2Î´^k_i x_j + 2Î´^k_j x_i - 2Î´_ij x^k)/(1-||x||Â²)
    pub fn christoffel_symbols(point: &PoincareBallPoint) -> (f32, f32) {
        let (x, y) = point.to_cartesian();
        let norm_sq = point.r * point.r;
        let denominator = 1.0 - norm_sq;
        
        if denominator < 1e-8 {
            return (0.0, 0.0);
        }
        
        let factor = 2.0 / denominator;
        
        // r ë°©í–¥ê³¼ Î¸ ë°©í–¥ í¬ë¦¬ìŠ¤í† í  ê¸°í˜¸
        let gamma_r = factor * x;
        let gamma_theta = factor * y;
        
        (gamma_r, gamma_theta)
    }
    
    /// 5.2.4 MÃ¶bius ë§ì…ˆ (í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ë§ì…ˆ)
    /// 
    /// x âŠ• y = (x + y + 2âŸ¨x,yâŸ©/(1+||x||Â²) x) / (1 + 2âŸ¨x,yâŸ© + ||x||Â²||y||Â²)
    pub fn mobius_addition(p1: &PoincareBallPoint, p2: &PoincareBallPoint) -> PoincareBallPoint {
        let (x1, y1) = p1.to_cartesian();
        let (x2, y2) = p2.to_cartesian();
        
        let dot_product = x1 * x2 + y1 * y2;
        let norm1_sq = p1.r * p1.r;
        let norm2_sq = p2.r * p2.r;
        
        let numerator_factor = 1.0 + 2.0 * dot_product / (1.0 + norm1_sq);
        let denominator = 1.0 + 2.0 * dot_product + norm1_sq * norm2_sq;
        
        if denominator < 1e-8 {
            return *p1; // ì•ˆì „í•œ ê¸°ë³¸ê°’
        }
        
        let result_x = (x1 + x2 * numerator_factor) / denominator;
        let result_y = (y1 + y2 * numerator_factor) / denominator;
        
        PoincareBallPoint::from_cartesian(result_x, result_y)
    }
    
    /// 5.2.4 ìŠ¤ì¹¼ë¼ ê³± (í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ìŠ¤ì¹¼ë¼ ê³±)
    /// 
    /// t âŠ™ v = (t||v||/artanh(||v||)) Â· v/||v||
    pub fn scalar_multiplication(t: f32, point: &PoincareBallPoint) -> PoincareBallPoint {
        if point.r < 1e-8 {
            return PoincareBallPoint::origin();
        }
        
        let norm = point.r;
        let artanh_norm = if norm < 0.99 {
            0.5 * libm::logf((1.0 + norm) / (1.0 - norm))
        } else {
            10.0 // ê²½ê³„ ê·¼ì²˜ì—ì„œ í´ë¨í•‘
        };
        
        let scale_factor = if artanh_norm > 1e-8 {
            t * norm / artanh_norm
        } else {
            t
        };
        
        let new_r = (scale_factor * norm).clamp(0.0, 0.99);
        
        PoincareBallPoint::new(new_r, point.theta)
    }
    
    /// 5.2.4 ì§€ìˆ˜ ì‚¬ìƒ (Exponential Map)
    /// 
    /// exp_x(v) = x âŠ• (tanh(||v||_x/2) Â· v/||v||_x)
    pub fn exponential_map(base: &PoincareBallPoint, tangent: &PoincareBallPoint) -> PoincareBallPoint {
        if tangent.r < 1e-8 {
            return *base;
        }
        
        // ë¦¬ë§Œ ë…¸ë¦„ ê³„ì‚°
        let inverse_metric = Self::inverse_metric_factor(base);
        let riemannian_norm = tangent.r * inverse_metric.sqrt();
        
        // tanh(||v||_x/2) ê³„ì‚°
        let tanh_half_norm = libm::tanhf(riemannian_norm / 2.0);
        
        // ë°©í–¥ ë²¡í„° ì •ê·œí™”
        let direction = if riemannian_norm > 1e-8 {
            PoincareBallPoint::new(tanh_half_norm, tangent.theta)
        } else {
            PoincareBallPoint::origin()
        };
        
        Self::mobius_addition(base, &direction)
    }
    
    /// 5.3.4 ìŒê³¡ ê±°ë¦¬ ê³„ì‚°
    /// 
    /// d_hyp(x,y) = artanh(||(-x) âŠ• y||)
    pub fn hyperbolic_distance(p1: &PoincareBallPoint, p2: &PoincareBallPoint) -> f32 {
        // -p1 ê³„ì‚° (MÃ¶bius ì—­ì›)
        let neg_p1 = PoincareBallPoint::new(p1.r, p1.theta + PI);
        
        // (-p1) âŠ• p2 ê³„ì‚°
        let diff = Self::mobius_addition(&neg_p1, p2);
        
        // artanh(||diff||) ê³„ì‚°
        let norm = diff.r;
        if norm < 0.99 {
            0.5 * libm::logf((1.0 + norm) / (1.0 - norm))
        } else {
            10.0 // ê²½ê³„ì—ì„œ í´ë¨í•‘
        }
    }
}

/// 5.3 ë¦¬ë§Œ ìµœì í™”ê¸°
/// 
/// í‘¸ì•µì¹´ë ˆ ë³¼ì—ì„œì˜ ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
#[derive(Debug, Clone)]
pub struct RiemannianOptimizer {
    /// í•™ìŠµë¥ 
    pub learning_rate: f32,
    /// Adam íŒŒë¼ë¯¸í„°
    pub beta1: f32,
    pub beta2: f32,
    pub epsilon: f32,
    /// Adam ìƒíƒœ
    pub momentum_r: f32,
    pub momentum_theta: f32,
    pub velocity_r: f32,
    pub velocity_theta: f32,
    /// í˜„ì¬ ë°˜ë³µ ìˆ˜
    pub iteration: i32,
}

impl RiemannianOptimizer {
    /// ìƒˆë¡œìš´ ë¦¬ë§Œ ìµœì í™”ê¸° ìƒì„±
    pub fn new(learning_rate: f32) -> Self {
        Self {
            learning_rate,
            beta1: 0.9,
            beta2: 0.999,
            epsilon: 1e-8,
            momentum_r: 0.0,
            momentum_theta: 0.0,
            velocity_r: 0.0,
            velocity_theta: 0.0,
            iteration: 0,
        }
    }
    
    /// 5.3.1 ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    /// 
    /// grad f(x) = g^(-1)(x) âˆ‡f(x) = (1-||x||Â²)Â²/4 âˆ‡f(x)
    pub fn compute_riemannian_gradient(
        &self,
        point: &PoincareBallPoint,
        euclidean_grad_r: f32,
        euclidean_grad_theta: f32,
    ) -> (f32, f32) {
        let inverse_metric_factor = RiemannianGeometry::inverse_metric_factor(point);
        
        // r ë°©í–¥ ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸
        let riemannian_grad_r = inverse_metric_factor * euclidean_grad_r;
        
        // Î¸ ë°©í–¥ ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸ (ê°ë„ ì¢Œí‘œì´ë¯€ë¡œ rÂ² ìŠ¤ì¼€ì¼ë§)
        let theta_scaling = if point.r > 1e-8 { point.r * point.r } else { 1e-8 };
        let riemannian_grad_theta = euclidean_grad_theta / theta_scaling;
        
        (riemannian_grad_r, riemannian_grad_theta)
    }
    
    /// 5.3.2 ë¦¬ë§Œ ìµœê¸‰ê°•í•˜ë²• ìŠ¤í…
    /// 
    /// x_{k+1} = x_k âŠ• (-Î± âŠ™ grad f(x_k))
    pub fn gradient_descent_step(
        &self,
        point: &PoincareBallPoint,
        euclidean_grad_r: f32,
        euclidean_grad_theta: f32,
    ) -> PoincareBallPoint {
        let (riemannian_grad_r, riemannian_grad_theta) = 
            self.compute_riemannian_gradient(point, euclidean_grad_r, euclidean_grad_theta);
        
        // ê·¸ë˜ë””ì–¸íŠ¸ ë°©í–¥ìœ¼ë¡œ ìŠ¤í…
        let step_r = -self.learning_rate * riemannian_grad_r;
        let step_theta = -self.learning_rate * riemannian_grad_theta;
        
        let step_vector = PoincareBallPoint::new(
            step_r.abs().min(0.1), // í° ìŠ¤í… ë°©ì§€
            if step_r >= 0.0 { step_theta } else { step_theta + PI }
        );
        
        RiemannianGeometry::exponential_map(point, &step_vector)
    }
    
    /// 5.3.3 ë¦¬ë§Œ Adam ìŠ¤í…
    /// 
    /// í‘¸ì•µì¹´ë ˆ ë³¼ì— ì ì‘ëœ Adam ìµœì í™”
    pub fn adam_step(
        &mut self,
        point: &PoincareBallPoint,
        euclidean_grad_r: f32,
        euclidean_grad_theta: f32,
    ) -> PoincareBallPoint {
        self.iteration += 1;
        
        let (riemannian_grad_r, riemannian_grad_theta) = 
            self.compute_riemannian_gradient(point, euclidean_grad_r, euclidean_grad_theta);
        
        // ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸
        self.momentum_r = self.beta1 * self.momentum_r + (1.0 - self.beta1) * riemannian_grad_r;
        self.momentum_theta = self.beta1 * self.momentum_theta + (1.0 - self.beta1) * riemannian_grad_theta;
        
        // ì†ë„ ì—…ë°ì´íŠ¸
        self.velocity_r = self.beta2 * self.velocity_r + (1.0 - self.beta2) * riemannian_grad_r.powi(2);
        self.velocity_theta = self.beta2 * self.velocity_theta + (1.0 - self.beta2) * riemannian_grad_theta.powi(2);
        
        // í¸í–¥ ë³´ì •
        let bias_correction_1 = 1.0 - self.beta1.powi(self.iteration);
        let bias_correction_2 = 1.0 - self.beta2.powi(self.iteration);
        
        let corrected_momentum_r = self.momentum_r / bias_correction_1;
        let corrected_momentum_theta = self.momentum_theta / bias_correction_1;
        
        let corrected_velocity_r = self.velocity_r / bias_correction_2;
        let corrected_velocity_theta = self.velocity_theta / bias_correction_2;
        
        // Adam ì—…ë°ì´íŠ¸
        let step_r = -self.learning_rate * corrected_momentum_r / (corrected_velocity_r.sqrt() + self.epsilon);
        let step_theta = -self.learning_rate * corrected_momentum_theta / (corrected_velocity_theta.sqrt() + self.epsilon);
        
        let step_vector = PoincareBallPoint::new(
            step_r.abs().min(0.05), // Adamì—ì„œëŠ” ë” ì‘ì€ ìŠ¤í…
            if step_r >= 0.0 { step_theta } else { step_theta + PI }
        );
        
        RiemannianGeometry::exponential_map(point, &step_vector)
    }
    
    /// 5.7.2 ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    /// 
    /// ë¦¬ë§Œ ë…¸ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    pub fn clip_riemannian_gradient(
        &self,
        point: &PoincareBallPoint,
        grad_r: f32,
        grad_theta: f32,
        max_norm: f32,
    ) -> (f32, f32) {
        let riemannian_norm = self.compute_riemannian_norm(point, grad_r, grad_theta);
        
        if riemannian_norm > max_norm {
            let scale = max_norm / riemannian_norm;
            (grad_r * scale, grad_theta * scale)
        } else {
            (grad_r, grad_theta)
        }
    }
    
    /// ë¦¬ë§Œ ë…¸ë¦„ ê³„ì‚°
    fn compute_riemannian_norm(
        &self,
        point: &PoincareBallPoint,
        grad_r: f32,
        grad_theta: f32,
    ) -> f32 {
        let metric_factor = RiemannianGeometry::metric_factor(point);
        let norm_r_sq = grad_r * grad_r * metric_factor;
        
        let theta_scaling = if point.r > 1e-8 { point.r * point.r } else { 1e-8 };
        let norm_theta_sq = grad_theta * grad_theta * theta_scaling * metric_factor;
        
        (norm_r_sq + norm_theta_sq).sqrt()
    }
}

/// 5.4 ìƒíƒœ ì „ì´ ê·¸ë˜í”„
/// 
/// ì´ì‚° ìƒíƒœ ê³µê°„ì˜ ì¡°í•©ë¡ ì  ìµœì í™”ë¥¼ ìœ„í•œ ê·¸ë˜í”„ êµ¬ì¡°
#[derive(Debug, Clone)]
pub struct StateTransitionGraph {
    /// ìƒíƒœ ê³µê°„ í¬ê¸°
    pub state_space_size: usize,
    /// ë³¼ì¸ ë§Œ ì˜¨ë„
    pub temperature: f32,
    /// ì˜¨ë„ ê°ì†Œìœ¨
    pub cooling_rate: f32,
    /// ë§ˆë¥´ì½”í”„ ì²´ì¸ ê¸°ë¡
    pub transition_history: Vec<u64>,
}

impl StateTransitionGraph {
    /// ìƒˆë¡œìš´ ìƒíƒœ ì „ì´ ê·¸ë˜í”„ ìƒì„±
    pub fn new(state_space_size: usize, initial_temperature: f32) -> Self {
        Self {
            state_space_size,
            temperature: initial_temperature,
            cooling_rate: 0.95,
            transition_history: Vec::new(),
        }
    }
    
    /// 5.4.2 í•´ë° ê±°ë¦¬ ê³„ì‚°
    /// 
    /// ë‘ ìƒíƒœ ì‚¬ì´ì˜ í•´ë° ê±°ë¦¬ (ë‹¤ë¥¸ ë¹„íŠ¸ ìˆ˜)
    pub fn hamming_distance(state1: u64, state2: u64) -> u32 {
        (state1 ^ state2).count_ones()
    }
    
    /// 5.4.2 ì´ì›ƒ ìƒíƒœ ìƒì„±
    /// 
    /// í•´ë° ê±°ë¦¬ 1ì¸ ì´ì›ƒ ìƒíƒœë“¤ ë°˜í™˜
    pub fn get_neighbors(state: u64) -> Vec<u64> {
        let mut neighbors = Vec::new();
        
        // ê° ë¹„íŠ¸ë¥¼ í”Œë¦½í•œ ìƒíƒœë“¤
        for i in 0..64 {
            let neighbor = state ^ (1u64 << i);
            neighbors.push(neighbor);
        }
        
        neighbors
    }
    
    /// 5.4.4 ë³¼ì¸ ë§Œ ë¶„í¬ ê¸°ë°˜ ì „ì´ í™•ë¥  ê³„ì‚°
    /// 
    /// P(s â†’ s') = exp(-Î² Â· Î”F) / Z
    pub fn compute_transition_probabilities(
        &self,
        current_state: u64,
        loss_function: impl Fn(u64) -> f32,
    ) -> Vec<(u64, f32)> {
        let neighbors = Self::get_neighbors(current_state);
        let current_loss = loss_function(current_state);
        
        let mut probabilities = Vec::new();
        let mut partition_sum = 0.0;
        
        // ê° ì´ì›ƒì— ëŒ€í•œ í™•ë¥  ê³„ì‚°
        for neighbor in neighbors {
            let neighbor_loss = loss_function(neighbor);
            let energy_diff = neighbor_loss - current_loss;
            let boltzmann_factor = libm::expf(-energy_diff / self.temperature);
            
            probabilities.push((neighbor, boltzmann_factor));
            partition_sum += boltzmann_factor;
        }
        
        // ì •ê·œí™”
        if partition_sum > 0.0 {
            for (_, prob) in probabilities.iter_mut() {
                *prob /= partition_sum;
            }
        }
        
        probabilities
    }
    
    /// 5.4.4 í™•ë¥ ì  ìƒíƒœ ì„ íƒ
    /// 
    /// ë³¼ì¸ ë§Œ ë¶„í¬ì— ë”°ë¼ ë‹¤ìŒ ìƒíƒœ ì„ íƒ
    pub fn sample_next_state(
        &mut self,
        current_state: u64,
        loss_function: impl Fn(u64) -> f32,
        rng: &mut impl Rng,
    ) -> u64 {
        let probabilities = self.compute_transition_probabilities(current_state, loss_function);
        
        let random_value: f32 = rng.gen();
        let mut cumulative_prob = 0.0;
        
        for (state, prob) in probabilities {
            cumulative_prob += prob;
            if random_value <= cumulative_prob {
                self.transition_history.push(state);
                return state;
            }
        }
        
        // fallback
        current_state
    }
    
    /// 5.7.3 ì ì‘ì  ì˜¨ë„ ìŠ¤ì¼€ì¤„ë§
    /// 
    /// ì‹œê°„ì— ë”°ë¥¸ ì˜¨ë„ ê°ì†Œ
    pub fn update_temperature(&mut self, epoch: i32, loss: f32) {
        // 1. ì§€ìˆ˜ ê°ì†Œ
        let base_temp = self.temperature * self.cooling_rate;
        
        // 2. ì†ì‹¤ ê¸°ë°˜ ì¡°ì •
        let loss_factor = 1.0 + libm::tanhf(loss - 1.0);
        
        // 3. ìµœì¢… ì˜¨ë„
        self.temperature = (base_temp * loss_factor).max(0.01);
    }
    
    /// 5.4.5 ë§ˆë¥´ì½”í”„ ì²´ì¸ ìˆ˜ë ´ì„± ê²€ì‚¬
    /// 
    /// ìƒíƒœ ë¶„í¬ì˜ ìˆ˜ë ´ ì—¬ë¶€ í™•ì¸
    pub fn check_convergence(&self, window_size: usize) -> bool {
        if self.transition_history.len() < window_size * 2 {
            return false;
        }
        
        let recent_states = &self.transition_history[self.transition_history.len() - window_size..];
        let prev_states = &self.transition_history[self.transition_history.len() - window_size * 2..self.transition_history.len() - window_size];
        
        // ìƒíƒœ ë¶„í¬ ë¹„êµ (ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜)
        let mut recent_counts = std::collections::HashMap::new();
        let mut prev_counts = std::collections::HashMap::new();
        
        for &state in recent_states {
            *recent_counts.entry(state).or_insert(0) += 1;
        }
        
        for &state in prev_states {
            *prev_counts.entry(state).or_insert(0) += 1;
        }
        
        // KL ë°œì‚° ê³„ì‚° (ê°„ì†Œí™”ëœ ë²„ì „)
        let mut kl_divergence = 0.0;
        for (&state, &recent_count) in &recent_counts {
            let recent_prob = recent_count as f32 / window_size as f32;
            let prev_count = prev_counts.get(&state).unwrap_or(&0);
            let prev_prob = (*prev_count as f32 / window_size as f32).max(1e-8);
            
            kl_divergence += recent_prob * libm::logf(recent_prob / prev_prob);
        }
        
        kl_divergence < 0.01 // ìˆ˜ë ´ ì„ê³„ê°’
    }
}

/// 5.6 ì •ë³´ ê¸°í•˜í•™ (Information Geometry)
/// 
/// í”¼ì…” ì •ë³´ ë©”íŠ¸ë¦­ê³¼ ìì—° ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
#[derive(Debug, Clone)]
pub struct InformationGeometry;

impl InformationGeometry {
    /// 5.6.1 í”¼ì…” ì •ë³´ í–‰ë ¬ ê³„ì‚°
    /// 
    /// I(r,Î¸) = [[4/(1-rÂ²)Â², 0], [0, 1/rÂ²]]
    pub fn fisher_information_matrix(point: &PoincareBallPoint) -> [[f32; 2]; 2] {
        let r = point.r.max(1e-8); // 0 ë°©ì§€
        let norm_sq = r * r;
        let denominator = 1.0 - norm_sq;
        
        let fisher_r = if denominator > 1e-8 {
            4.0 / (denominator * denominator)
        } else {
            1e8
        };
        
        let fisher_theta = 1.0 / (r * r);
        
        [
            [fisher_r, 0.0],
            [0.0, fisher_theta]
        ]
    }
    
    /// 5.6.2 ìì—° ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    /// 
    /// âˆ‡Ìƒ = I^(-1)(Î¸) âˆ‡_Î¸ L
    pub fn natural_gradient(
        point: &PoincareBallPoint,
        euclidean_grad_r: f32,
        euclidean_grad_theta: f32,
    ) -> (f32, f32) {
        let inverse_metric_factor = RiemannianGeometry::inverse_metric_factor(point);
        let r = point.r.max(1e-8);
        
        // ìì—° ê·¸ë˜ë””ì–¸íŠ¸ = ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸
        let natural_grad_r = inverse_metric_factor * euclidean_grad_r;
        let natural_grad_theta = (r * r) * euclidean_grad_theta;
        
        (natural_grad_r, natural_grad_theta)
    }
    
    /// 5.6.3 KL ë°œì‚°ê³¼ ìŒê³¡ ê±°ë¦¬ì˜ ê´€ê³„
    /// 
    /// KL(P_Î¸1 || P_Î¸2) â‰ˆ Â½ dÂ²_hyp(Î¸1, Î¸2)
    pub fn kl_divergence_approximation(p1: &PoincareBallPoint, p2: &PoincareBallPoint) -> f32 {
        let hyperbolic_distance = RiemannianGeometry::hyperbolic_distance(p1, p2);
        0.5 * hyperbolic_distance * hyperbolic_distance
    }
}

/// 5.5 í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™”ê¸°
/// 
/// ì—°ì†-ì´ì‚° íŒŒë¼ë¯¸í„°ì˜ í†µí•© ìµœì í™”
#[derive(Debug, Clone)]
pub struct HybridRiemannianOptimizer {
    /// ì—°ì† íŒŒë¼ë¯¸í„° ìµœì í™”ê¸°
    pub riemannian_optimizer: RiemannianOptimizer,
    /// ì´ì‚° ìƒíƒœ ì „ì´ ê·¸ë˜í”„
    pub state_transition: StateTransitionGraph,
    /// ìˆ˜ë ´ ì„ê³„ê°’
    pub convergence_tolerance: f32,
    /// ìµœëŒ€ ë°˜ë³µ ìˆ˜
    pub max_iterations: usize,
}

impl HybridRiemannianOptimizer {
    /// ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™”ê¸° ìƒì„±
    pub fn new(learning_rate: f32, temperature: f32) -> Self {
        Self {
            riemannian_optimizer: RiemannianOptimizer::new(learning_rate),
            state_transition: StateTransitionGraph::new(1024, temperature),
            convergence_tolerance: 1e-6,
            max_iterations: 1000,
        }
    }
    
    /// 5.5.2 êµëŒ€ ìµœì í™” ìŠ¤í…
    /// 
    /// ì—°ì† íŒŒë¼ë¯¸í„°ì™€ ì´ì‚° ìƒíƒœë¥¼ êµëŒ€ë¡œ ìµœì í™”
    pub fn hybrid_optimization_step(
        &mut self,
        continuous_params: &PoincareBallPoint,
        discrete_state: u64,
        continuous_loss_fn: impl Fn(&PoincareBallPoint) -> (f32, f32, f32), // (loss, grad_r, grad_theta)
        discrete_loss_fn: impl Fn(u64) -> f32,
        rng: &mut impl Rng,
    ) -> (PoincareBallPoint, u64) {
        
        // 1. ì—°ì† íŒŒë¼ë¯¸í„° ìµœì í™” (ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸)
        let (loss, grad_r, grad_theta) = continuous_loss_fn(continuous_params);
        let new_continuous = self.riemannian_optimizer.adam_step(continuous_params, grad_r, grad_theta);
        
        // 2. ì´ì‚° ìƒíƒœ ìµœì í™” (í™•ë¥ ì  ì „ì´)
        let new_discrete = self.state_transition.sample_next_state(discrete_state, discrete_loss_fn, rng);
        
        // 3. ì˜¨ë„ ì—…ë°ì´íŠ¸
        self.state_transition.update_temperature(self.riemannian_optimizer.iteration, loss);
        
        (new_continuous, new_discrete)
    }
    
    /// 5.5.3 ìˆ˜ë ´ì„± ì²´í¬
    /// 
    /// ì—°ì† ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ê³¼ ì´ì‚° ìƒíƒœ ìˆ˜ë ´ì„ ëª¨ë‘ í™•ì¸
    pub fn check_hybrid_convergence(
        &self,
        point: &PoincareBallPoint,
        grad_r: f32,
        grad_theta: f32,
    ) -> bool {
        // ì—°ì† íŒŒë¼ë¯¸í„° ìˆ˜ë ´ì„±
        let riemannian_norm = self.riemannian_optimizer.compute_riemannian_norm(point, grad_r, grad_theta);
        let continuous_converged = riemannian_norm < self.convergence_tolerance;
        
        // ì´ì‚° ìƒíƒœ ìˆ˜ë ´ì„±
        let discrete_converged = self.state_transition.check_convergence(50);
        
        continuous_converged && discrete_converged
    }
} 

/// ğŸš€ ê³ ë„í™”ëœ ë¦¬ë§Œ Adam ì˜µí‹°ë§ˆì´ì €
/// í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ë¦¬ë§Œ ê¸°í•˜í•™ì„ í™œìš©í•œ ì ì‘ì  ìµœì í™”
#[derive(Debug, Clone)]
pub struct RiemannianAdamOptimizer {
    /// ë¦¬ë§Œ ê¸°í•˜í•™ íŒŒë¼ë¯¸í„°
    pub poincare_curvature: f32,
    /// ì ì‘ì  í•™ìŠµë¥  íŒŒë¼ë¯¸í„°
    pub beta1: f32,  // ëª¨ë©˜í…€ ê³„ìˆ˜
    pub beta2: f32,  // RMSprop ê³„ìˆ˜  
    pub epsilon: f32, // ìˆ˜ì¹˜ ì•ˆì •ì„±
    /// í‘¸ì•µì¹´ë ˆ ë³¼ ê²½ê³„ ë³´í˜¸
    pub boundary_protection: f32,
    /// ê¸°í•˜í•™ì  ê°€ì¤‘ì¹˜
    pub geometric_scaling: f32,
    /// ëª¨ë©˜í…€ ë©”ëª¨ë¦¬
    pub momentum_r: f32,
    pub momentum_theta: f32,
    /// RMSprop ë©”ëª¨ë¦¬
    pub rmsprop_r: f32,
    pub rmsprop_theta: f32,
    /// ì‹œê°„ ìŠ¤í…
    pub time_step: u32,
}

impl RiemannianAdamOptimizer {
    /// ìƒˆë¡œìš´ ë¦¬ë§Œ Adam ì˜µí‹°ë§ˆì´ì € ìƒì„±
    pub fn new() -> Self {
        Self {
            poincare_curvature: -1.0,  // í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ìŒì˜ ê³¡ë¥ 
            beta1: 0.9,               // ëª¨ë©˜í…€ ê³„ìˆ˜
            beta2: 0.999,             // RMSprop ê³„ìˆ˜
            epsilon: 1e-8,            // ìˆ˜ì¹˜ ì•ˆì •ì„±
            boundary_protection: 0.95, // ê²½ê³„ ë³´í˜¸ (r < 0.95)
            geometric_scaling: 1.0,    // ê¸°í•˜í•™ì  ìŠ¤ì¼€ì¼ë§
            momentum_r: 0.0,
            momentum_theta: 0.0,
            rmsprop_r: 0.0,
            rmsprop_theta: 0.0,
            time_step: 0,
        }
    }
    
    /// í‘¸ì•µì¹´ë ˆ ë³¼ì—ì„œì˜ ë¦¬ë§Œ ë©”íŠ¸ë¦­ ê³„ì‚°
    fn compute_riemannian_metric(&self, r: f32, theta: f32) -> (f32, f32) {
        // í‘¸ì•µì¹´ë ˆ ë³¼ì˜ ë¦¬ë§Œ ë©”íŠ¸ë¦­: g_rr = 4/(1-rÂ²)Â², g_Î¸Î¸ = 4rÂ²/(1-rÂ²)Â²
        let r_clamped = r.clamp(0.01, 0.99); // ê²½ê³„ ë³´í˜¸
        let one_minus_r2 = 1.0 - r_clamped.powi(2);
        let metric_factor = 4.0 / one_minus_r2.powi(2);
        
        let g_rr = metric_factor;
        let g_theta_theta = metric_factor * r_clamped.powi(2).max(0.01);
        
        (g_rr, g_theta_theta)
    }
    
    /// ì§€ìˆ˜ ë§¤í•‘ (Exponential Map) ì ìš©
    fn exponential_map(&self, r: f32, theta: f32, grad_r: f32, grad_theta: f32, learning_rate: f32) -> (f32, f32) {
        let (g_rr, g_theta_theta) = self.compute_riemannian_metric(r, theta);
        
        // ë¦¬ë§Œ ê·¸ë˜ë””ì–¸íŠ¸ ì •ê·œí™”
        let riemannian_grad_r = grad_r / g_rr.sqrt();
        let riemannian_grad_theta = grad_theta / g_theta_theta.sqrt();
        
        // ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ ê³„ì‚°
        let grad_norm = (riemannian_grad_r.powi(2) + riemannian_grad_theta.powi(2)).sqrt();
        
        if grad_norm < self.epsilon {
            return (r, theta);
        }
        
        // ì§€ìˆ˜ ë§¤í•‘ì„ í†µí•œ ì—…ë°ì´íŠ¸
        let step_size = learning_rate * grad_norm;
        let direction_r = riemannian_grad_r / grad_norm;
        let direction_theta = riemannian_grad_theta / grad_norm;
        
        // í‘¸ì•µì¹´ë ˆ ë³¼ì—ì„œì˜ ì§€ìˆ˜ ë§¤í•‘ (ê·¼ì‚¬)
        let new_r = r - step_size * direction_r;
        let new_theta = theta - step_size * direction_theta;
        
        // ê²½ê³„ ì¡°ê±´ ì ìš©
        let final_r = new_r.clamp(0.01, self.boundary_protection);
        let final_theta = new_theta;
        
        (final_r, final_theta)
    }
    
    /// ê³ ë„í™”ëœ ë¦¬ë§Œ Adam ìµœì í™” ìŠ¤í…
    pub fn optimization_step(
        &mut self,
        seed: &mut Packed128,
        grad_r: f32,
        grad_theta: f32,
        learning_rate: f32
    ) {
        self.time_step += 1;
        
        // í˜„ì¬ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        let r = f32::from_bits((seed.lo >> 32) as u32);
        let theta = f32::from_bits(seed.lo as u32);
        
        // ë¦¬ë§Œ ë©”íŠ¸ë¦­ ê³ ë ¤
        let (g_rr, g_theta_theta) = self.compute_riemannian_metric(r, theta);
        
        // ìì—° ê·¸ë˜ë””ì–¸íŠ¸ (Natural Gradient) ê³„ì‚°
        let natural_grad_r = grad_r / g_rr;
        let natural_grad_theta = grad_theta / g_theta_theta;
        
        // Adam ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸
        self.momentum_r = self.beta1 * self.momentum_r + (1.0 - self.beta1) * natural_grad_r;
        self.momentum_theta = self.beta1 * self.momentum_theta + (1.0 - self.beta1) * natural_grad_theta;
        
        // Adam RMSprop ì—…ë°ì´íŠ¸
        self.rmsprop_r = self.beta2 * self.rmsprop_r + (1.0 - self.beta2) * natural_grad_r.powi(2);
        self.rmsprop_theta = self.beta2 * self.rmsprop_theta + (1.0 - self.beta2) * natural_grad_theta.powi(2);
        
        // í¸í–¥ ë³´ì • (Bias Correction)
        let bias_correction1 = 1.0 - self.beta1.powi(self.time_step as i32);
        let bias_correction2 = 1.0 - self.beta2.powi(self.time_step as i32);
        
        let corrected_momentum_r = self.momentum_r / bias_correction1;
        let corrected_momentum_theta = self.momentum_theta / bias_correction1;
        
        let corrected_rmsprop_r = self.rmsprop_r / bias_correction2;
        let corrected_rmsprop_theta = self.rmsprop_theta / bias_correction2;
        
        // ì ì‘ì  í•™ìŠµë¥  ê³„ì‚°
        let adaptive_lr_r = learning_rate / (corrected_rmsprop_r.sqrt() + self.epsilon);
        let adaptive_lr_theta = learning_rate / (corrected_rmsprop_theta.sqrt() + self.epsilon);
        
        // ì§€ìˆ˜ ë§¤í•‘ì„ í†µí•œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        let (new_r, new_theta) = self.exponential_map(
            r, theta,
            corrected_momentum_r,
            corrected_momentum_theta,
            adaptive_lr_r.min(adaptive_lr_theta) * self.geometric_scaling
        );
        
        // íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        seed.lo = ((new_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
    }
    
    /// ğŸš€ ìœµí•© ì—­ì „íŒŒ ìŠ¤í… (ë‹¤ì¸µ ì”ì°¨í•™ìŠµìš©)
    pub fn fused_backward_step(
        &mut self,
        target: &[f32],
        predicted: &[f32], 
        seed: &mut Packed128,
        rows: usize,
        cols: usize,
        learning_rate: f32
    ) -> (f32, f32) {
        // ì‹œê°„ ìŠ¤í… ì¦ê°€
        self.time_step += 1;
        
        // ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
        let mut grad_r_sum = 0.0;
        let mut grad_theta_sum = 0.0;
        let mut total_loss = 0.0;
        
        for i in 0..rows {
            for j in 0..cols {
                let idx = i * cols + j;
                let error = predicted[idx] - target[idx];
                total_loss += error * error;
                
                // ìƒíƒœ ì „ì´ ë¯¸ë¶„ (ì´ì‚° ê³µê°„ íƒìƒ‰)
                self.apply_state_transition(seed, error, i, j);
                
                // ì—°ì† íŒŒë¼ë¯¸í„° í•´ì„ì  ê·¸ë˜ë””ì–¸íŠ¸
                let dr = self.analytical_gradient_r(seed, i, j, rows, cols);
                let dtheta = self.analytical_gradient_theta(seed, i, j, rows, cols);
                
                grad_r_sum += error * dr;
                grad_theta_sum += error * dtheta;
            }
        }
        
        // í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸
        let total_elements = (rows * cols) as f32;
        grad_r_sum /= total_elements;
        grad_theta_sum /= total_elements;
        
        // í˜„ì¬ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
        let theta_fp32 = f32::from_bits(seed.lo as u32);
        
        // Adam ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸
        self.momentum_r = self.beta1 * self.momentum_r + (1.0 - self.beta1) * grad_r_sum;
        self.momentum_theta = self.beta1 * self.momentum_theta + (1.0 - self.beta1) * grad_theta_sum;
        
        // RMSprop ì—…ë°ì´íŠ¸
        self.rmsprop_r = self.beta2 * self.rmsprop_r + (1.0 - self.beta2) * grad_r_sum * grad_r_sum;
        self.rmsprop_theta = self.beta2 * self.rmsprop_theta + (1.0 - self.beta2) * grad_theta_sum * grad_theta_sum;
        
        // í¸í–¥ ë³´ì •
        let momentum_r_hat = self.momentum_r / (1.0 - self.beta1.powi(self.time_step as i32));
        let momentum_theta_hat = self.momentum_theta / (1.0 - self.beta1.powi(self.time_step as i32));
        let rmsprop_r_hat = self.rmsprop_r / (1.0 - self.beta2.powi(self.time_step as i32));
        let rmsprop_theta_hat = self.rmsprop_theta / (1.0 - self.beta2.powi(self.time_step as i32));
        
        // ë¦¬ë§Œ ê¸°í•˜í•™ì  ìŠ¤ì¼€ì¼ë§
        let (metric_r, metric_theta) = self.compute_riemannian_metric(r_fp32, theta_fp32);
        
        // ì ì‘ì  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        let new_r = (r_fp32 - learning_rate * metric_r * momentum_r_hat / 
                    (rmsprop_r_hat.sqrt() + self.epsilon)).clamp(0.1, 2.0);
        let new_theta = theta_fp32 - learning_rate * metric_theta * momentum_theta_hat / 
                       (rmsprop_theta_hat.sqrt() + self.epsilon);
        
        // í‘¸ì•µì¹´ë ˆ ë³¼ ê²½ê³„ ë³´í˜¸
        let protected_r = if new_r >= self.boundary_protection {
            new_r * 0.99 // ê²½ê³„ì—ì„œ ì‚´ì§ ì•ˆìª½ìœ¼ë¡œ
        } else {
            new_r
        };
        
        // íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        seed.lo = ((protected_r.to_bits() as u64) << 32) | new_theta.to_bits() as u64;
        
        // MSE ë° RMSE ê³„ì‚°
        let mse = total_loss / total_elements;
        let rmse = mse.sqrt();
        
        (mse, rmse)
    }
    
    /// ìƒíƒœ ì „ì´ ë¯¸ë¶„ ì ìš©
    fn apply_state_transition(&mut self, seed: &mut Packed128, gradient_signal: f32, i: usize, j: usize) {
        let hash = (i * 31 + j) & 0x7;
        let bit_pos = hash * 3;
        let current_state = (seed.hi >> bit_pos) & 0x7;
        
        let new_state = if gradient_signal.abs() > 0.1 {
            if gradient_signal > 0.0 {
                // ì–‘ì˜ ê·¸ë˜ë””ì–¸íŠ¸: í•¨ìˆ˜ ë¯¸ë¶„ ë°©í–¥
                match current_state {
                    0 => 1, // sin â†’ cos
                    1 => 0, // cos â†’ -sin  
                    2 => 3, // tanh â†’ sechÂ²
                    3 => 2, // sechÂ² â†’ tanh
                    4 => 5, // exp â†’ log
                    5 => 6, // log â†’ 1/x
                    6 => 7, // 1/x â†’ poly
                    7 => 4, // poly â†’ exp
                    _ => current_state,
                }
            } else {
                // ìŒì˜ ê·¸ë˜ë””ì–¸íŠ¸: ì—­ë°©í–¥ ì „ì´
                match current_state {
                    0 => 7, // sin â†’ poly
                    1 => 6, // cos â†’ 1/x
                    2 => 5, // tanh â†’ log
                    3 => 4, // sechÂ² â†’ exp
                    4 => 3, // exp â†’ sechÂ²
                    5 => 2, // log â†’ tanh
                    6 => 1, // 1/x â†’ cos
                    7 => 0, // poly â†’ sin
                    _ => current_state,
                }
            }
        } else {
            current_state // ì•½í•œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ìƒíƒœ ìœ ì§€
        };
        
        // ë¹„íŠ¸ ì—…ë°ì´íŠ¸
        seed.hi = (seed.hi & !(0x7 << bit_pos)) | (new_state << bit_pos);
    }
    
    /// í•´ì„ì  r íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸
    fn analytical_gradient_r(&self, seed: &Packed128, i: usize, j: usize, rows: usize, cols: usize) -> f32 {
        // ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê·¼ì‚¬ (í–¥í›„ í•´ì„ì  êµ¬í˜„ ì˜ˆì •)
        let epsilon = 1e-5;
        let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
        let theta_fp32 = f32::from_bits(seed.lo as u32);
        
        // r + epsilon
        let mut seed_plus = *seed;
        let r_plus = r_fp32 + epsilon;
        seed_plus.lo = ((r_plus.to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
        let f_plus = seed_plus.fused_forward(i, j, rows, cols);
        
        // r - epsilon  
        let mut seed_minus = *seed;
        let r_minus = (r_fp32 - epsilon).max(0.1);
        seed_minus.lo = ((r_minus.to_bits() as u64) << 32) | theta_fp32.to_bits() as u64;
        let f_minus = seed_minus.fused_forward(i, j, rows, cols);
        
        (f_plus - f_minus) / (2.0 * epsilon)
    }
    
    /// í•´ì„ì  theta íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸
    fn analytical_gradient_theta(&self, seed: &Packed128, i: usize, j: usize, rows: usize, cols: usize) -> f32 {
        // ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê·¼ì‚¬ (í–¥í›„ í•´ì„ì  êµ¬í˜„ ì˜ˆì •)
        let epsilon = 1e-5;
        let r_fp32 = f32::from_bits((seed.lo >> 32) as u32);
        let theta_fp32 = f32::from_bits(seed.lo as u32);
        
        // theta + epsilon
        let mut seed_plus = *seed;
        let theta_plus = theta_fp32 + epsilon;
        seed_plus.lo = ((r_fp32.to_bits() as u64) << 32) | theta_plus.to_bits() as u64;
        let f_plus = seed_plus.fused_forward(i, j, rows, cols);
        
        // theta - epsilon
        let mut seed_minus = *seed;  
        let theta_minus = theta_fp32 - epsilon;
        seed_minus.lo = ((r_fp32.to_bits() as u64) << 32) | theta_minus.to_bits() as u64;
        let f_minus = seed_minus.fused_forward(i, j, rows, cols);
        
        (f_plus - f_minus) / (2.0 * epsilon)
    }
}

/// ğŸš€ ë¦¬ë§Œ Adamì„ ì‚¬ìš©í•œ ê³ ë„í™”ëœ ì—­ì „íŒŒ
pub fn fused_backward_riemannian_adam(
    target: &[f32], 
    predicted: &[f32], 
    seed: &mut Packed128, 
    optimizer: &mut RiemannianAdamOptimizer,
    rows: usize, 
    cols: usize,
    learning_rate: f32
) -> (f32, f32) {
    let mut total_loss = 0.0;
    let mut grad_r_sum = 0.0;
    let mut grad_theta_sum = 0.0;
    
    for i in 0..rows {
        for j in 0..cols {
            let idx = i * cols + j;
            let error = predicted[idx] - target[idx];
            total_loss += error * error;
            
            // 1. ìƒíƒœ ì „ì´ ë¯¸ë¶„ ì ìš© (hi ë¹„íŠ¸ ì—…ë°ì´íŠ¸)
            seed.apply_state_transition(error, i, j);
            
            // 2. í•´ì„ì  ë¯¸ë¶„ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            let dr = seed.analytical_gradient_r(i, j, rows, cols);
            let dtheta = seed.analytical_gradient_theta(i, j, rows, cols);
            
            grad_r_sum += error * dr;
            grad_theta_sum += error * dtheta;
        }
    }
    
    let batch_size = (rows * cols) as f32;
    grad_r_sum /= batch_size;
    grad_theta_sum /= batch_size;
    
    // ğŸš€ ë¦¬ë§Œ Adam ìµœì í™” ì ìš©
    optimizer.optimization_step(seed, grad_r_sum, grad_theta_sum, learning_rate);
    
    let mse = total_loss / batch_size;
    (mse, mse.sqrt())
} 
