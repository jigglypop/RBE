use anyhow::Result;
use candle_core::{Device, Tensor, DType};
use candle_nn;
use tokenizers::Tokenizer;
use std::io::{self, Write};
use std::time::Instant;
use std::path::Path;
use std::fs;
use std::collections::HashMap;
use rbe_llm::packed_params::HybridEncodedBlock;
use rbe_llm::decoder::FusedForwardPass;

/// Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
struct CandleRBEModel {
    tokenizer: Tokenizer,
    device: Device,
    vocab_size: usize,
    
    // RBE ì••ì¶•ëœ ë ˆì´ì–´ë“¤
    compressed_layers: HashMap<String, Vec<HybridEncodedBlock>>,
    
    // ì›ë³¸ LayerNorm íŒŒë¼ë¯¸í„°ë“¤ (ì••ì¶• ì•ˆí•¨)
    layer_norms: HashMap<String, Vec<f32>>,
    
    // RBE ë””ì½”ë”
    fused_forward: FusedForwardPass,
}

impl CandleRBEModel {
    /// Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë“œ
    fn load_hybrid(
        tokenizer_path: &str,
        compressed_dir: &str,
        weights_dir: &str,
    ) -> Result<Self> {
        println!("ğŸ”— Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë”©...");
        
        let device = Device::Cpu;
        println!("âœ… ë””ë°”ì´ìŠ¤: CPU");
        
        // í† í¬ë‚˜ì´ì € ë¡œë“œ
        let tokenizer = Tokenizer::from_file(tokenizer_path)
            .map_err(|e| anyhow::anyhow!("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {:?}", e))?;
        let vocab_size = tokenizer.get_vocab_size(false);
        println!("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {} ì–´íœ˜", vocab_size);
        
        // RBE ì••ì¶•ëœ ë ˆì´ì–´ë“¤ ë¡œë“œ
        let compressed_layers = Self::load_compressed_layers(compressed_dir)?;
        println!("âœ… ì••ì¶• ë ˆì´ì–´ ë¡œë“œ ì™„ë£Œ: {} ê°œ", compressed_layers.len());
        
        // ì›ë³¸ LayerNorm íŒŒë¼ë¯¸í„°ë“¤ ë¡œë“œ
        let layer_norms = Self::load_layer_norms(weights_dir)?;
        println!("âœ… LayerNorm íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ: {} ê°œ", layer_norms.len());
        
        // RBE ë””ì½”ë” ì´ˆê¸°í™”
        let fused_forward = FusedForwardPass::new();
        println!("âœ… RBE ìœµí•© ë””ì½”ë” ì´ˆê¸°í™” ì™„ë£Œ");
        
        println!("ğŸ¯ Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!");
        
        Ok(Self {
            tokenizer,
            device,
            vocab_size,
            compressed_layers,
            layer_norms,
            fused_forward,
        })
    }
    
    /// ì••ì¶•ëœ ë ˆì´ì–´ë“¤ ë¡œë“œ (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
    fn load_compressed_layers(compressed_dir: &str) -> Result<HashMap<String, Vec<HybridEncodedBlock>>> {
        println!("ğŸ“¦ ì••ì¶• ë ˆì´ì–´ë“¤ ë¡œë”© ì¤‘: {}", compressed_dir);
        
        let mut layers = HashMap::new();
        
        // ì••ì¶• íŒŒì¼ë“¤ì´ ì¡´ì¬í•œë‹¤ë©´ ë¡œë“œ
        if Path::new(compressed_dir).exists() {
            // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì••ì¶• íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë¡œë“œ
            println!("   âš ï¸ ì„ì‹œ: ì••ì¶• ë””ë ‰í„°ë¦¬ ì¡´ì¬í•˜ì§€ë§Œ ë”ë¯¸ ë°ì´í„° ì‚¬ìš©");
        } else {
            println!("   âš ï¸ ì••ì¶• ë””ë ‰í„°ë¦¬ ì—†ìŒ, ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸");
        }
        
        // 12ê°œ ë ˆì´ì–´ì˜ ë”ë¯¸ ì••ì¶• ë¸”ë¡ë“¤ ìƒì„±
        for layer_idx in 0..12 {
            // Attention ê°€ì¤‘ì¹˜
            let attn_key = format!("transformer.h.{}.attn.c_attn.weight", layer_idx);
            layers.insert(attn_key, vec![]); // ì‹¤ì œë¡œëŠ” ì••ì¶•ëœ ë¸”ë¡ë“¤
            
            // MLP ê°€ì¤‘ì¹˜
            let mlp_key = format!("transformer.h.{}.mlp.c_fc.weight", layer_idx);
            layers.insert(mlp_key, vec![]); // ì‹¤ì œë¡œëŠ” ì••ì¶•ëœ ë¸”ë¡ë“¤
        }
        
        Ok(layers)
    }
    
    /// LayerNorm íŒŒë¼ë¯¸í„°ë“¤ ë¡œë“œ (ì›ë³¸ ìœ ì§€)
    fn load_layer_norms(weights_dir: &str) -> Result<HashMap<String, Vec<f32>>> {
        println!("ğŸ”¢ LayerNorm íŒŒë¼ë¯¸í„°ë“¤ ë¡œë”© ì¤‘: {}", weights_dir);
        
        let mut layer_norms = HashMap::new();
        
        // LayerNormì€ ì••ì¶•í•˜ì§€ ì•Šê³  ì›ë³¸ ìœ ì§€
        for layer_idx in 0..12 {
            // ê° ë ˆì´ì–´ì˜ LayerNorm íŒŒë¼ë¯¸í„°ë“¤
            let ln1_weight_key = format!("transformer.h.{}.ln_1.weight", layer_idx);
            let ln1_bias_key = format!("transformer.h.{}.ln_1.bias", layer_idx);
            let ln2_weight_key = format!("transformer.h.{}.ln_2.weight", layer_idx);
            let ln2_bias_key = format!("transformer.h.{}.ln_2.bias", layer_idx);
            
            // ë”ë¯¸ ë°ì´í„° (ì‹¤ì œë¡œëŠ” numpy íŒŒì¼ì—ì„œ ë¡œë“œ)
            layer_norms.insert(ln1_weight_key, vec![1.0f32; 768]);
            layer_norms.insert(ln1_bias_key, vec![0.0f32; 768]);
            layer_norms.insert(ln2_weight_key, vec![1.0f32; 768]);
            layer_norms.insert(ln2_bias_key, vec![0.0f32; 768]);
        }
        
        // ìµœì¢… LayerNorm
        layer_norms.insert("transformer.ln_f.weight".to_string(), vec![1.0f32; 768]);
        layer_norms.insert("transformer.ln_f.bias".to_string(), vec![0.0f32; 768]);
        
        Ok(layer_norms)
    }
    
    /// í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ìƒì„± (Candle + RBE)
    fn generate_hybrid(&self, prompt: &str, max_tokens: usize) -> Result<String> {
        println!("\nğŸ’­ Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ìƒì„±: '{}'", prompt);
        
        // í† í¬ë‚˜ì´ì§•
        let encoding = self.tokenizer.encode(prompt, false)
            .map_err(|e| anyhow::anyhow!("í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {:?}", e))?;
        let mut token_ids: Vec<u32> = encoding.get_ids().to_vec();
        
        println!("ğŸ”¤ ì´ˆê¸° í† í° ìˆ˜: {}", token_ids.len());
        
        // ìƒì„± ë£¨í”„
        for step in 0..max_tokens {
            let next_token = self.hybrid_forward(&token_ids)?;
            
            // EOS ì²´í¬
            if next_token == 1 || next_token == 0 {
                break;
            }
            
            token_ids.push(next_token);
            
            if step % 3 == 0 {
                let partial = self.tokenizer.decode(&token_ids, true)
                    .unwrap_or_else(|_| "ë””ì½”ë”© ì˜¤ë¥˜".to_string());
                println!("ğŸ“ ë‹¨ê³„ {}: {}", step, partial);
            }
        }
        
        // ìµœì¢… ë””ì½”ë”©
        let result = self.tokenizer.decode(&token_ids, true)
            .map_err(|e| anyhow::anyhow!("ë””ì½”ë”© ì‹¤íŒ¨: {:?}", e))?;
        
        Ok(result)
    }
    
    /// í•˜ì´ë¸Œë¦¬ë“œ forward pass (Candle í…ì„œ + RBE ë””ì½”ë”©)
    fn hybrid_forward(&self, token_ids: &[u32]) -> Result<u32> {
        let input_len = token_ids.len();
        
        // 1. Candleë¡œ ì„ë² ë”© ì²˜ë¦¬
        let hidden_states = self.candle_embedding(token_ids)?;
        
        // 2. RBEë¡œ ì••ì¶•ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ ì²˜ë¦¬
        let processed_states = self.rbe_transformer_layers(&hidden_states)?;
        
        // 3. Candleë¡œ ìµœì¢… ì¶œë ¥ ì²˜ë¦¬
        let next_token = self.candle_output_head(&processed_states)?;
        
        Ok(next_token % (self.vocab_size as u32))
    }
    
    /// Candleë¡œ ì‹¤ì œ ì„ë² ë”© ì²˜ë¦¬
    fn candle_embedding(&self, token_ids: &[u32]) -> Result<Tensor> {
        let seq_len = token_ids.len();
        let hidden_size = 768;
        
        // ì‹¤ì œ í† í° ì„ë² ë”© ë¡œë“œ ì‹œë„
        let embeddings = if let Ok(embedding_weights) = self.load_real_embeddings() {
            println!("âœ… ì‹¤ì œ í† í° ì„ë² ë”© ì‚¬ìš©");
            let mut embeddings = vec![0.0f32; seq_len * hidden_size];
            
            for (i, &token_id) in token_ids.iter().enumerate() {
                let token_idx = (token_id as usize) % (self.vocab_size.min(embedding_weights.len() / hidden_size));
                
                // ì‹¤ì œ ì„ë² ë”© ê°’ ë³µì‚¬
                for j in 0..hidden_size {
                    let embed_idx = token_idx * hidden_size + j;
                    if embed_idx < embedding_weights.len() {
                        embeddings[i * hidden_size + j] = embedding_weights[embed_idx];
                    }
                }
            }
            embeddings
        } else {
            println!("âš ï¸ ì‹¤ì œ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨, ê²°ì •ì  ì„ë² ë”© ì‚¬ìš©");
            let mut embeddings = vec![0.0f32; seq_len * hidden_size];
            
            for (i, &token_id) in token_ids.iter().enumerate() {
                let token_idx = (token_id % (self.vocab_size as u32)) as usize;
                
                // ê²°ì •ì  ì„ë² ë”© ìƒì„± (ì¼ê´€ì„± ìˆëŠ” ê°’)
                for j in 0..hidden_size {
                    let embed_val = ((token_idx * 37 + j * 17) as f32 * 0.001).sin() * 0.1;
                    embeddings[i * hidden_size + j] = embed_val;
                }
            }
            embeddings
        };
        
        let tensor = Tensor::from_slice(&embeddings, (seq_len, hidden_size), &self.device)?
            .to_dtype(DType::F32)?;
        
        Ok(tensor)
    }
    
    /// ì‹¤ì œ ì„ë² ë”© ê°€ì¤‘ì¹˜ ë¡œë“œ
    fn load_real_embeddings(&self) -> Result<Vec<f32>> {
        // PyTorch ëª¨ë¸ íŒŒì¼ì—ì„œ ì§ì ‘ ì„ë² ë”© ë¡œë“œ
        let model_path = "./models/skt-kogpt2-base-v2/pytorch_model.bin";
        
        if !Path::new(model_path).exists() {
            return Err(anyhow::anyhow!("ëª¨ë¸ íŒŒì¼ ì—†ìŒ"));
        }
        
        // ê°„ë‹¨í•œ PyTorch í…ì„œ ì½ê¸° (ì‹¤ì œë¡œëŠ” pickle íŒŒì‹± í•„ìš”)
        // ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ì½”ë“œì˜ numpy ë¡œë”© ë°©ì‹ ì‚¬ìš©
        self.try_load_from_weights_dir()
    }
    
    /// weights ë””ë ‰í„°ë¦¬ì—ì„œ ì„ë² ë”© ë¡œë“œ ì‹œë„
    fn try_load_from_weights_dir(&self) -> Result<Vec<f32>> {
        let weights_dir = "./models/skt-kogpt2-base-v2/weights";
        let embedding_file = format!("{}/transformer_wte_weight.npy", weights_dir);
        
        if Path::new(&embedding_file).exists() {
            println!("ğŸ“ ê¸°ì¡´ ì„ë² ë”© íŒŒì¼ ë°œê²¬: {}", embedding_file);
            self.load_numpy_file(&embedding_file)
        } else {
            Err(anyhow::anyhow!("ì„ë² ë”© íŒŒì¼ ì—†ìŒ"))
        }
    }
    
    /// numpy íŒŒì¼ ë¡œë“œ
    fn load_numpy_file(&self, path: &str) -> Result<Vec<f32>> {
        use std::fs::File;
        use std::io::Read;
        
        let mut file = File::open(path)?;
        
        // ê°„ë‹¨í•œ numpy í—¤ë” ìŠ¤í‚µ (ì‹¤ì œë¡œëŠ” ì •í™•í•œ íŒŒì‹± í•„ìš”)
        let mut header = [0u8; 1024];
        file.read(&mut header)?;
        
        // ë°ì´í„° ë¶€ë¶„ ì°¾ê¸° (ì„ì‹œ)
        let mut data_bytes = Vec::new();
        file.read_to_end(&mut data_bytes)?;
        
        // float32ë¡œ ë³€í™˜
        let data: Vec<f32> = data_bytes.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        println!("âœ… numpy íŒŒì¼ ë¡œë“œ: {} ìš”ì†Œ", data.len());
        Ok(data)
    }
    
    /// RBEë¡œ ì••ì¶•ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ ì²˜ë¦¬
    fn rbe_transformer_layers(&self, hidden_states: &Tensor) -> Result<Tensor> {
        println!("ğŸ§  ì‹¤ì œ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ ì²˜ë¦¬ ì¤‘...");
        
        let (seq_len, hidden_size) = hidden_states.dims2()?;
        let mut current_states = hidden_states.clone();
        
        // ê° ë ˆì´ì–´ë³„ë¡œ ì‹¤ì œ ê°€ì¤‘ì¹˜ ì ìš©
        for layer_idx in 0..12 {  // GPT-2 12 ë ˆì´ì–´
            current_states = self.apply_transformer_layer(&current_states, layer_idx)?;
            
            if layer_idx % 4 == 3 {
                println!("  âœ… ë ˆì´ì–´ {}/12 ì™„ë£Œ", layer_idx + 1);
            }
        }
        
        println!("âœ… ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì²˜ë¦¬ ì™„ë£Œ: {}x{}", seq_len, hidden_size);
        Ok(current_states)
    }
    
    /// ë‹¨ì¼ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì ìš©
    fn apply_transformer_layer(&self, hidden_states: &Tensor, layer_idx: usize) -> Result<Tensor> {
        let (seq_len, hidden_size) = hidden_states.dims2()?;
        
        // 1. LayerNorm + Self-Attention
        let normed_input = self.apply_layer_norm(hidden_states, layer_idx, "ln_1")?;
        let attn_output = self.apply_self_attention(&normed_input, layer_idx)?;
        let after_attn = (hidden_states + &attn_output)?;
        
        // 2. LayerNorm + FFN
        let normed_ffn = self.apply_layer_norm(&after_attn, layer_idx, "ln_2")?;
        let ffn_output = self.apply_ffn(&normed_ffn, layer_idx)?;
        let final_output = (after_attn + ffn_output)?;
        
        Ok(final_output)
    }
    
    /// LayerNorm ì ìš©
    fn apply_layer_norm(&self, input: &Tensor, layer_idx: usize, ln_type: &str) -> Result<Tensor> {
        let weight_key = format!("transformer.h.{}.{}.weight", layer_idx, ln_type);
        let bias_key = format!("transformer.h.{}.{}.bias", layer_idx, ln_type);
        
        if let (Some(weight), Some(bias)) = (self.layer_norms.get(&weight_key), self.layer_norms.get(&bias_key)) {
            // ì‹¤ì œ LayerNorm ì ìš©
            self.layer_norm_with_weights(input, weight, bias)
        } else {
            // ê¸°ë³¸ ì •ê·œí™”
            Ok(input.clone())
        }
    }
    
    /// Self-Attention ì ìš©
    fn apply_self_attention(&self, input: &Tensor, layer_idx: usize) -> Result<Tensor> {
        // ì‹¤ì œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¡œë“œ ë° ì ìš©
        if let Ok(attn_weights) = self.load_attention_weights(layer_idx) {
            self.compute_attention_with_weights(input, &attn_weights)
        } else {
            // ê°„ë‹¨í•œ ì–´í…ì…˜ ì‹œë®¬ë ˆì´ì…˜
            let (seq_len, hidden_size) = input.dims2()?;
                         let identity_like = Tensor::eye(hidden_size, DType::F32, &self.device)
                 .map_err(|e| anyhow::anyhow!("ì•„ì´ë´í‹°í‹° ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì‹¤íŒ¨: {}", e))?;
             input.matmul(&identity_like).map_err(|e| anyhow::anyhow!("ë§¤íŠ¸ë¦­ìŠ¤ ê³±ì…ˆ ì‹¤íŒ¨: {}", e))
        }
    }
    
    /// FFN ì ìš©
    fn apply_ffn(&self, input: &Tensor, layer_idx: usize) -> Result<Tensor> {
        // ì‹¤ì œ FFN ê°€ì¤‘ì¹˜ ë¡œë“œ ë° ì ìš©
        if let Ok((fc_weights, proj_weights)) = self.load_ffn_weights(layer_idx) {
            // FC layer
            let fc_output = input.matmul(&fc_weights)?;
            let gelu_output = self.gelu_activation(&fc_output)?;
            
            // Projection layer
            gelu_output.matmul(&proj_weights)
        } else {
            // ê°„ë‹¨í•œ FFN ì‹œë®¬ë ˆì´ì…˜
            Ok(input.clone())
        }
    }
    
    /// GELU í™œì„±í™” í•¨ìˆ˜ (ê°„ì†Œí™”)
    fn gelu_activation(&self, x: &Tensor) -> Result<Tensor> {
        // ê°„ì†Œí™”ëœ GELU ê·¼ì‚¬: x * sigmoid(1.702 * x)
        let scale_tensor = Tensor::new(1.702f32, x.device()).map_err(|e| anyhow::anyhow!("ìŠ¤ì¼€ì¼ í…ì„œ ìƒì„± ì‹¤íŒ¨: {}", e))?;
        let scaled_x = x.mul(&scale_tensor).map_err(|e| anyhow::anyhow!("ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {}", e))?;
        let sigmoid = scaled_x.sigmoid().map_err(|e| anyhow::anyhow!("ì‹œê·¸ëª¨ì´ë“œ ì‹¤íŒ¨: {}", e))?;
        x.mul(&sigmoid).map_err(|e| anyhow::anyhow!("ê³±ì…ˆ ì‹¤íŒ¨: {}", e))
    }
    
    /// LayerNorm ê°€ì¤‘ì¹˜ë¡œ ì •ê·œí™”
    fn layer_norm_with_weights(&self, input: &Tensor, weight: &[f32], bias: &[f32]) -> Result<Tensor> {
        let (seq_len, hidden_size) = input.dims2()?;
        
        // ê°„ë‹¨í•œ ì •ê·œí™” (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ LayerNorm êµ¬í˜„ í•„ìš”)
        let weight_tensor = Tensor::from_slice(weight, (1, hidden_size), &self.device)?.to_dtype(DType::F32)?;
        let bias_tensor = Tensor::from_slice(bias, (1, hidden_size), &self.device)?.to_dtype(DType::F32)?;
        
        // input * weight + bias (ê°„ì†Œí™”ëœ ë²„ì „)
        let scaled = input.broadcast_mul(&weight_tensor).map_err(|e| anyhow::anyhow!("ë¸Œë¡œë“œìºìŠ¤íŠ¸ ê³±ì…ˆ ì‹¤íŒ¨: {}", e))?;
        scaled.broadcast_add(&bias_tensor).map_err(|e| anyhow::anyhow!("ë¸Œë¡œë“œìºìŠ¤íŠ¸ ë§ì…ˆ ì‹¤íŒ¨: {}", e).into())
    }
    
    /// ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¡œë“œ
    fn load_attention_weights(&self, layer_idx: usize) -> Result<Tensor> {
        let weights_file = format!("./models/skt-kogpt2-base-v2/weights/transformer_h_{}_attn_c_attn_weight.npy", layer_idx);
        
        if Path::new(&weights_file).exists() {
            let data = self.load_numpy_file(&weights_file)?;
            let rows = 768;
            let cols = 2304; // QKV combined
            
                         if data.len() >= rows * cols {
                 let tensor = Tensor::from_slice(&data[..rows*cols], (rows, cols), &self.device)
                     .map_err(|e| anyhow::anyhow!("í…ì„œ ìƒì„± ì‹¤íŒ¨: {}", e))?
                     .to_dtype(DType::F32)
                     .map_err(|e| anyhow::anyhow!("íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {}", e))?;
                 Ok(tensor)
             } else {
                 Err(anyhow::anyhow!("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜"))
             }
        } else {
            Err(anyhow::anyhow!("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ"))
        }
    }
    
    /// FFN ê°€ì¤‘ì¹˜ ë¡œë“œ
    fn load_ffn_weights(&self, layer_idx: usize) -> Result<(Tensor, Tensor)> {
        let fc_file = format!("./models/skt-kogpt2-base-v2/weights/transformer_h_{}_mlp_c_fc_weight.npy", layer_idx);
        let proj_file = format!("./models/skt-kogpt2-base-v2/weights/transformer_h_{}_mlp_c_proj_weight.npy", layer_idx);
        
        if Path::new(&fc_file).exists() && Path::new(&proj_file).exists() {
            let fc_data = self.load_numpy_file(&fc_file)?;
            let proj_data = self.load_numpy_file(&proj_file)?;
            
                         let fc_tensor = Tensor::from_slice(&fc_data[..768*3072], (768, 3072), &self.device)
                 .map_err(|e| anyhow::anyhow!("FC í…ì„œ ìƒì„± ì‹¤íŒ¨: {}", e))?
                 .to_dtype(DType::F32)
                 .map_err(|e| anyhow::anyhow!("FC íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {}", e))?;
             let proj_tensor = Tensor::from_slice(&proj_data[..3072*768], (3072, 768), &self.device)
                 .map_err(|e| anyhow::anyhow!("í”„ë¡œì ì…˜ í…ì„œ ìƒì„± ì‹¤íŒ¨: {}", e))?
                 .to_dtype(DType::F32)
                 .map_err(|e| anyhow::anyhow!("í”„ë¡œì ì…˜ íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {}", e))?;
            
            Ok((fc_tensor, proj_tensor))
        } else {
            Err(anyhow::anyhow!("FFN ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ"))
        }
    }
    
    /// ì–´í…ì…˜ ê³„ì‚°
    fn compute_attention_with_weights(&self, input: &Tensor, attn_weights: &Tensor) -> Result<Tensor> {
        // ê°„ì†Œí™”ëœ ì–´í…ì…˜ ê³„ì‚°
        let qkv = input.matmul(attn_weights)?;
        
        // ì‹¤ì œë¡œëŠ” Q, K, V ë¶„ë¦¬í•˜ê³  scaled dot-product attention ìˆ˜í–‰
        // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì„ í˜• ë³€í™˜ë§Œ ì ìš©
        let output_dim = input.dims2()?.1;
        let identity = Tensor::eye(output_dim, DType::F32, &self.device)?;
        input.matmul(&identity)
    }
    
    /// Candleë¡œ ìµœì¢… ì¶œë ¥ í—¤ë“œ ì²˜ë¦¬
    fn candle_output_head(&self, hidden_states: &Tensor) -> Result<u32> {
        let (seq_len, hidden_size) = hidden_states.dims2()?;
        
        // ë§ˆì§€ë§‰ í† í°ì˜ hidden state ì¶”ì¶œ
        let last_hidden = hidden_states.narrow(0, seq_len - 1, 1)?;
        
        // LM Head ì‹œë®¬ë ˆì´ì…˜
        let lm_head_weight = Tensor::randn(0.0f32, 0.1f32, (hidden_size, self.vocab_size), &self.device)?
            .to_dtype(DType::F32)?;
        
        let logits = last_hidden.matmul(&lm_head_weight)?;
        
        // Argmax ìƒ˜í”Œë§
        let probs = candle_nn::ops::softmax(&logits, 1)?;
        let next_token_tensor = probs.argmax(1)?;
        let next_token_vec = next_token_tensor.to_vec1::<u32>()?;
        
        Ok(next_token_vec[0])
    }
    
    /// ëª¨ë¸ ì •ë³´ ì¶œë ¥
    fn print_info(&self) {
        println!("\nğŸ“Š Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ë³´:");
        println!("  ğŸ”— êµ¬ì¡°: Candle í…ì„œ + RBE ì••ì¶• ë³µì›");
        println!("  ğŸ”¤ ì–´íœ˜ í¬ê¸°: {}", self.vocab_size);
        println!("  ğŸ“¦ ì••ì¶• ë ˆì´ì–´: {} ê°œ", self.compressed_layers.len());
        println!("  ğŸ”¢ LayerNorm íŒŒë¼ë¯¸í„°: {} ê°œ", self.layer_norms.len());
        println!("  âš¡ ë””ë°”ì´ìŠ¤: {:?}", self.device);
        println!("  ğŸ•¯ï¸ Candle í”„ë ˆì„ì›Œí¬");
        println!("  ğŸŒ€ RBE ì••ì¶• ê¸°ìˆ ");
        println!("  âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸");
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ‡°ğŸ‡· === Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ===");
    println!("Candle í…ì„œ ì—°ì‚° + RBE ì••ì¶• ê¸°ìˆ  ê²°í•©\n");
    
    let tokenizer_path = "./models/skt-kogpt2-base-v2/tokenizer.json";
    let compressed_dir = "./models/skt-kogpt2-base-v2_compressed";
    let weights_dir = "./models/skt-kogpt2-base-v2/weights";
    
    // íŒŒì¼ ì¡´ì¬ í™•ì¸
    let tokenizer_exists = Path::new(tokenizer_path).exists();
    let compressed_exists = Path::new(compressed_dir).exists();
    let weights_exists = Path::new(weights_dir).exists();
    
    println!("ğŸ“‹ íŒŒì¼ í™•ì¸:");
    println!("   - í† í¬ë‚˜ì´ì €: {} ({})", tokenizer_path, 
             if tokenizer_exists { "ì¡´ì¬" } else { "âŒ ì—†ìŒ" });
    println!("   - ì••ì¶• ë””ë ‰í„°ë¦¬: {} ({})", compressed_dir, 
             if compressed_exists { "ì¡´ì¬" } else { "âŒ ì—†ìŒ - ë”ë¯¸ ë°ì´í„° ì‚¬ìš©" });
    println!("   - ì›ë³¸ ê°€ì¤‘ì¹˜: {} ({})", weights_dir, 
             if weights_exists { "ì¡´ì¬" } else { "âŒ ì—†ìŒ - ë”ë¯¸ ë°ì´í„° ì‚¬ìš©" });
    
    if !tokenizer_exists {
        return Err(anyhow::anyhow!("í† í¬ë‚˜ì´ì € íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."));
    }
    
    // Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë“œ
    let model = CandleRBEModel::load_hybrid(tokenizer_path, compressed_dir, weights_dir)?;
    model.print_info();
    
    println!("\nğŸ’¬ Candle + RBE í•˜ì´ë¸Œë¦¬ë“œ ëŒ€í™” ì‹œì‘! (ì¢…ë£Œ: 'exit')");
    println!("âš ï¸ í˜„ì¬ëŠ” êµ¬ì¡° í…ŒìŠ¤íŠ¸ ì¤‘ (ì‹¤ì œ ì••ì¶• ëª¨ë¸ ì—°ë™ í•„ìš”)");

    let stdin = io::stdin();
    loop {
        print!("\nì§ˆë¬¸: ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        stdin.read_line(&mut input)?;
        let input = input.trim();
        
        if input == "exit" || input == "quit" || input == "ì¢…ë£Œ" {
            println!("ğŸ‘‹ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.");
            break;
        }
        
        if input.is_empty() {
            continue;
        }

        let start = Instant::now();
        
        match model.generate_hybrid(input, 12) {
            Ok(response) => {
                let duration = start.elapsed();
                
                println!("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë‹µë³€: {}", response);
                println!("â±ï¸ ìƒì„± ì‹œê°„: {:.2}ì´ˆ", duration.as_secs_f32());
                println!("ğŸ”— Candle + RBE ê¸°ìˆ  ì‚¬ìš©");
            }
            Err(e) => {
                println!("âŒ ì˜¤ë¥˜: {}", e);
            }
        }
    }

    Ok(())
} 