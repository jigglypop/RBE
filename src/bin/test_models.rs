use RBE_LLM::types::HybridEncodedBlock;
use std::fs;
use anyhow::Result;
use nalgebra::{DMatrix, DVector};
use std::time::Instant;
use std::collections::HashMap;
use tokenizers::Tokenizer;
use indicatif::{ProgressBar, ProgressStyle};

// ëª¨ë¸ í”„ë¡œíŒŒì¼ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
struct ModelProfile {
    name: String,
    file_path: String,
}

// í…ŒìŠ¤íŠ¸ ê²°ê³¼ êµ¬ì¡°ì²´
#[derive(Debug)]
struct TestResult {
    model_name: String,
    question: String,
    response: String,
    inference_time_ms: f64,
    tokens_per_second: f32,
}

fn load_compressed_model(filepath: &str) -> Result<Vec<HybridEncodedBlock>> {
    let json_content = fs::read_to_string(filepath)?;
    let data: serde_json::Value = serde_json::from_str(&json_content)?;
    let blocks_data = data.get("blocks")
        .ok_or_else(|| anyhow::anyhow!("JSONì—ì„œ 'blocks' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))?;
    serde_json::from_value(blocks_data.clone())
        .map_err(|e| anyhow::anyhow!("ë¸”ë¡ ë°ì´í„° íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {}", e))
}

fn load_vocabulary(filepath: &str) -> Result<HashMap<u32, String>> {
    let tokenizer = Tokenizer::from_file(filepath)
        .map_err(|e| anyhow::anyhow!("tokenizer.json ë¡œë“œ ì‹¤íŒ¨: {}. íŒŒì¼ ê²½ë¡œ: {}", e, filepath))?;
    let vocab = tokenizer.get_vocab(false);
    let id_to_token: HashMap<u32, String> = vocab.into_iter()
        .map(|(token, id)| (id, token))
        .collect();
    Ok(id_to_token)
}

fn reconstruct_matrix(blocks: &[HybridEncodedBlock], rows: usize, cols: usize) -> DMatrix<f32> {
    let mut matrix = DMatrix::zeros(rows, cols);
    
    if blocks.is_empty() {
        return matrix;
    }
    
    // ë¸”ë¡ í¬ê¸° í™•ì¸
    let block_rows = blocks[0].rows;
    let block_cols = blocks[0].cols;
    let blocks_per_row = (cols + block_cols - 1) / block_cols;
    
    for (block_idx, block) in blocks.iter().enumerate() {
        let block_row = block_idx / blocks_per_row;
        let block_col = block_idx % blocks_per_row;
        let start_row = block_row * block_rows;
        let start_col = block_col * block_cols;
        
        let block_matrix = block.decode();
        
        for i in 0..block.rows {
            for j in 0..block.cols {
                let global_i = start_row + i;
                let global_j = start_col + j;
                if global_i < rows && global_j < cols {
                    matrix[(global_i, global_j)] = block_matrix[i * block.cols + j];
                }
            }
        }
    }
    
    matrix
}

fn prompt_to_vector(prompt: &str, dim: usize) -> DVector<f32> {
    let mut vec = DVector::zeros(dim);
    let chars: Vec<char> = prompt.chars().collect();
    
    for (i, ch) in chars.iter().enumerate() {
        let hash = (*ch as u32).wrapping_mul(31_u32.wrapping_pow(i as u32));
        let idx = (hash as usize) % dim;
        vec[idx] += 1.0;
    }
    
    if vec.norm() > 0.0 {
        vec.normalize_mut();
    }
    vec
}

fn vector_to_text(vector: &DVector<f32>, id_to_vocab: &HashMap<u32, String>) -> String {
    let mut indices: Vec<_> = (0..vector.len()).collect();
    indices.sort_unstable_by(|&a, &b| 
        vector[b].partial_cmp(&vector[a]).unwrap_or(std::cmp::Ordering::Equal));

    let top_tokens = indices.iter()
        .take(10)
        .map(|&i| {
            let vocab_id = (i as u32) % (id_to_vocab.len() as u32);
            id_to_vocab.get(&vocab_id).cloned().unwrap_or_else(|| "[UNK]".to_string())
        })
        .collect::<Vec<_>>()
        .join(" ");
    
    top_tokens
}

fn test_model(
    model_profile: &ModelProfile,
    questions: &[&str],
    vocab: &HashMap<u32, String>,
    progress: &ProgressBar,
) -> Result<Vec<TestResult>> {
    progress.set_message(format!("í…ŒìŠ¤íŠ¸ ì¤‘: {}", model_profile.name));
    
    // ëª¨ë¸ ë¡œë“œ
    let blocks = load_compressed_model(&model_profile.file_path)?;
    
    // ì‹¤ì œ ë¸”ë¡ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–‰ë ¬ í¬ê¸° ê²°ì •
    if blocks.is_empty() {
        return Err(anyhow::anyhow!("ì••ì¶•ëœ ë¸”ë¡ì´ ì—†ìŠµë‹ˆë‹¤."));
    }
    
    // ì²« ë²ˆì§¸ ë¸”ë¡ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    let first_block = &blocks[0];
    let block_size = first_block.rows; // ë¸”ë¡ í¬ê¸°
    let num_blocks_sqrt = (blocks.len() as f64).sqrt() as usize;
    let matrix_size = num_blocks_sqrt * block_size;
    
    let matrix = reconstruct_matrix(&blocks, matrix_size, matrix_size);
    
    let mut results = Vec::new();
    
    // ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for question in questions {
        let start = Instant::now();
        
        // ì…ë ¥ ë²¡í„° ìƒì„± ë° ì¶”ë¡ 
        let input_vector = prompt_to_vector(question, matrix_size);
        let output_vector = &matrix * input_vector;
        let response = vector_to_text(&output_vector, vocab);
        
        let inference_time = start.elapsed();
        let inference_ms = inference_time.as_secs_f64() * 1000.0;
        let tokens_per_second = 10.0 / inference_time.as_secs_f32(); // 10 í† í° ìƒì„± ê°€ì •
        
        results.push(TestResult {
            model_name: model_profile.name.clone(),
            question: question.to_string(),
            response,
            inference_time_ms: inference_ms,
            tokens_per_second,
        });
    }
    
    Ok(results)
}

#[tokio::main]
async fn main() -> Result<()> {
    println!("\n=== RBE ì••ì¶• ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ===\n");
    
    // í•œê¸€ ì§ˆë¬¸ ì„¸íŠ¸ (5ê°œ)
    let questions = vec![
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
        "ì¸ê³µì§€ëŠ¥ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¯¸ë˜ì˜ ê¸°ìˆ ì€ ì–´ë–»ê²Œ ë°œì „í• ê¹Œìš”?",
    ];
    
    // í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ í”„ë¡œíŒŒì¼ë“¤
    let model_profiles = vec![
        ModelProfile {
            name: "ê·¹í•œ ì••ì¶• (256x256, 50ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_256x256_w50.rbe".to_string(),
        },
        ModelProfile {
            name: "ì´ˆê³ ì••ì¶• (256x256, 100ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_256x256_w100.rbe".to_string(),
        },
        ModelProfile {
            name: "ê³ ì••ì¶• (256x256, 200ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_256x256_w200.rbe".to_string(),
        },
        ModelProfile {
            name: "í‘œì¤€ ì••ì¶• (256x256, 500ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_256x256_w500.rbe".to_string(),
        },
        ModelProfile {
            name: "ê· í˜• ì••ì¶• (128x128, 500ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_128x128_w500.rbe".to_string(),
        },
        ModelProfile {
            name: "ê³ í’ˆì§ˆ (64x64, 1000ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_64x64_w1000.rbe".to_string(),
        },
        ModelProfile {
            name: "ì´ˆê³ í’ˆì§ˆ (32x32, 2000ê³„ìˆ˜)".to_string(),
            file_path: "./models/skt-kogpt2-base-v2_compressed/kogpt2_32x32_w2000.rbe".to_string(),
        },
    ];
    
    // ì–´íœ˜ì§‘ ë¡œë“œ
    println!("ì–´íœ˜ì§‘ ë¡œë“œ ì¤‘...");
    let vocab_file = "./models/skt-kogpt2-base-v2/tokenizer.json";
    let vocab = load_vocabulary(vocab_file)?;
    println!("ì–´íœ˜ì§‘ ë¡œë“œ ì™„ë£Œ: {}ê°œ í† í°\n", vocab.len());
    
    // ì§„í–‰ë¥  í‘œì‹œ
    let progress = ProgressBar::new(model_profiles.len() as u64);
    progress.set_style(
        ProgressStyle::default_bar()
            .template("[{bar:40}] {pos}/{len} {msg}")
            .unwrap()
    );
    
    // ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
    let mut all_results = Vec::new();
    
    for model_profile in &model_profiles {
        match test_model(model_profile, &questions, &vocab, &progress) {
            Ok(results) => {
                all_results.extend(results);
                progress.inc(1);
            }
            Err(e) => {
                eprintln!("ëª¨ë¸ {} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {}", model_profile.name, e);
                progress.inc(1);
                continue;
            }
        }
    }
    
    progress.finish_with_message("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!");
    
    // ê²°ê³¼ ì¶œë ¥
    println!("\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===\n");
    
    // ì§ˆë¬¸ë³„ë¡œ ê²°ê³¼ ê·¸ë£¹í™” ë° ì¶œë ¥
    for question in &questions {
        println!("\nì§ˆë¬¸: {}", question);
        println!("{:-<80}", "");
        
        let mut question_results: Vec<&TestResult> = all_results.iter()
            .filter(|r| r.question == *question)
            .collect();
        
        // ì¶”ë¡  ì†ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        question_results.sort_by(|a, b| 
            a.inference_time_ms.partial_cmp(&b.inference_time_ms).unwrap());
        
        for result in question_results {
            println!("ëª¨ë¸: {}", result.model_name);
            println!("ì‘ë‹µ: {}", result.response);
            println!("ì¶”ë¡  ì‹œê°„: {:.2}ms ({:.1} tokens/s)", 
                result.inference_time_ms, result.tokens_per_second);
            println!();
        }
    }
    
    // ì „ì²´ í†µê³„
    println!("\n=== ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ ===");
    println!("{:-<80}", "");
    println!("{:<40} | {:<15} | {:<15}", "ëª¨ë¸", "í‰ê·  ì¶”ë¡ ì‹œê°„(ms)", "í‰ê·  í† í°/ì´ˆ");
    println!("{:-<80}", "");
    
    for profile in &model_profiles {
        let model_results: Vec<&TestResult> = all_results.iter()
            .filter(|r| r.model_name == profile.name)
            .collect();
        
        if !model_results.is_empty() {
            let avg_time = model_results.iter()
                .map(|r| r.inference_time_ms)
                .sum::<f64>() / model_results.len() as f64;
            
            let avg_tokens = model_results.iter()
                .map(|r| r.tokens_per_second)
                .sum::<f32>() / model_results.len() as f32;
            
            println!("{:<40} | {:<15.2} | {:<15.1}", 
                profile.name, avg_time, avg_tokens);
        }
    }
    
    // ê²°ê³¼ ì €ì¥
    let results_json = serde_json::json!({
        "test_date": std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs(),
        "questions": questions,
        "results": all_results.iter().map(|r| {
            serde_json::json!({
                "model": r.model_name,
                "question": r.question,
                "response": r.response,
                "inference_time_ms": r.inference_time_ms,
                "tokens_per_second": r.tokens_per_second,
            })
        }).collect::<Vec<_>>(),
    });
    
    fs::write("./models/skt-kogpt2-base-v2_compressed/test_results.json", 
        serde_json::to_string_pretty(&results_json)?)?;
    
    println!("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!");
    println!("ğŸ“Š ìƒì„¸ ê²°ê³¼: ./models/skt-kogpt2-base-v2_compressed/test_results.json");
    
    Ok(())
} 