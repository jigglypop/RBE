use tokenizers::Tokenizer;
use std::io::{self, Write};
use std::time::Instant;
use nalgebra::DMatrix;
use std::fs;
use anyhow::Result;
use serde_json::Value;
use std::collections::HashMap;
use std::path::Path;

/// ì›ë³¸ GPT-2 ëª¨ë¸ (numpy íŒŒì¼ ì§ì ‘ ë¡œë“œ)
struct OriginalGPT2Model {
    tokenizer: Tokenizer,
    config: GPT2Config,
    
    // ì„ë² ë”© ë ˆì´ì–´ë“¤
    token_embeddings: DMatrix<f32>,      // wte: 51200 x 768
    position_embeddings: DMatrix<f32>,   // wpe: 1024 x 768
    
    // 12ê°œ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
    transformer_layers: Vec<TransformerLayer>,
    
    // ìµœì¢… ë ˆì´ì–´ë“¤
    final_ln_weight: Vec<f32>,           // ln_f.weight: 768
    final_ln_bias: Vec<f32>,             // ln_f.bias: 768
    lm_head: DMatrix<f32>,               // lm_head: 768 x 51200
}

#[derive(Debug)]
struct GPT2Config {
    vocab_size: usize,       // 51200
    n_embd: usize,          // 768
    n_layer: usize,         // 12
    n_head: usize,          // 12
    n_positions: usize,     // 1024
}

#[derive(Debug)]
struct TransformerLayer {
    // Pre-attention LayerNorm
    ln_1_weight: Vec<f32>,
    ln_1_bias: Vec<f32>,
    
    // Multi-head Self-Attention
    attn_c_attn: DMatrix<f32>,     // QKV combined: 768 x 2304
    attn_c_proj: DMatrix<f32>,     // Output projection: 768 x 768
    
    // Pre-FFN LayerNorm
    ln_2_weight: Vec<f32>,
    ln_2_bias: Vec<f32>,
    
    // Feed-Forward Network
    mlp_c_fc: DMatrix<f32>,        // Up projection: 768 x 3072
    mlp_c_proj: DMatrix<f32>,      // Down projection: 3072 x 768
}

impl OriginalGPT2Model {
    /// ì›ë³¸ numpy íŒŒì¼ë“¤ë¡œë¶€í„° ì™„ì „ ë¬´ì†ì‹¤ ë¡œë“œ
    fn load_from_numpy(weights_dir: &str, tokenizer_path: &str) -> Result<Self> {
        println!("ğŸš€ ì›ë³¸ GPT-2 ëª¨ë¸ ë¡œë”© (numpy ì§ì ‘ ë¡œë“œ)");
        println!("   - ì›ë³¸ ê°€ì¤‘ì¹˜: {}", weights_dir);
        println!("   - í† í¬ë‚˜ì´ì €: {}", tokenizer_path);
        
        // 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        let tokenizer = Tokenizer::from_file(tokenizer_path)
            .map_err(|e| anyhow::anyhow!("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {:?}", e))?;
        println!("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {} ì–´íœ˜", tokenizer.get_vocab_size(false));
        
        // 2. ëª¨ë¸ ì„¤ì •
        let config = GPT2Config {
            vocab_size: 51200,
            n_embd: 768,
            n_layer: 12,
            n_head: 12,
            n_positions: 1024,
        };
        
        // 3. ì›ë³¸ numpy ë©”íƒ€ë°ì´í„° ë¡œë“œ
        let weights_path = Path::new(weights_dir);
        let metadata_path = weights_path.join("metadata.json");
        let metadata_str = fs::read_to_string(&metadata_path)?;
        let metadata: HashMap<String, Value> = serde_json::from_str(&metadata_str)?;
        
        println!("âœ… ì›ë³¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {} ê°œ", metadata.len());
        
        // 4. ì„ë² ë”© ë ˆì´ì–´ ë¡œë“œ
        println!("ğŸ”— ì„ë² ë”© ë ˆì´ì–´ ë¡œë“œ ì¤‘...");
        let token_embeddings = Self::load_original_matrix(
            &metadata, weights_path, "transformer.wte.weight", 51200, 768)?;
        let position_embeddings = Self::load_original_matrix(
            &metadata, weights_path, "transformer.wpe.weight", 1024, 768)?;
        
        println!("âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ");
        println!("   - í† í° ì„ë² ë”©: {} x {}", token_embeddings.nrows(), token_embeddings.ncols());
        println!("   - ìœ„ì¹˜ ì„ë² ë”©: {} x {}", position_embeddings.nrows(), position_embeddings.ncols());
        
        // 5. íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ ë¡œë“œ
        println!("ğŸ”„ 12ê°œ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ë¡œë“œ ì¤‘...");
        let mut transformer_layers = Vec::new();
        
        for layer_idx in 0..config.n_layer {
            println!("  ğŸ“‹ ë ˆì´ì–´ {} ë¡œë“œ ì¤‘...", layer_idx);
            
            let layer_prefix = format!("transformer.h.{}", layer_idx);
            
            // LayerNorm íŒŒë¼ë¯¸í„°ë“¤
            let ln_1_weight = Self::load_original_vector(
                &metadata, weights_path, &format!("{}.ln_1.weight", layer_prefix), 768)?;
            let ln_1_bias = Self::load_original_vector(
                &metadata, weights_path, &format!("{}.ln_1.bias", layer_prefix), 768)?;
            let ln_2_weight = Self::load_original_vector(
                &metadata, weights_path, &format!("{}.ln_2.weight", layer_prefix), 768)?;
            let ln_2_bias = Self::load_original_vector(
                &metadata, weights_path, &format!("{}.ln_2.bias", layer_prefix), 768)?;
            
            // Attention ê°€ì¤‘ì¹˜ë“¤
            let attn_c_attn = Self::load_original_matrix(
                &metadata, weights_path, &format!("{}.attn.c_attn.weight", layer_prefix), 768, 2304)?;
            let attn_c_proj = Self::load_original_matrix(
                &metadata, weights_path, &format!("{}.attn.c_proj.weight", layer_prefix), 768, 768)?;
            
            // FFN ê°€ì¤‘ì¹˜ë“¤
            let mlp_c_fc = Self::load_original_matrix(
                &metadata, weights_path, &format!("{}.mlp.c_fc.weight", layer_prefix), 768, 3072)?;
            let mlp_c_proj = Self::load_original_matrix(
                &metadata, weights_path, &format!("{}.mlp.c_proj.weight", layer_prefix), 3072, 768)?;
            
            transformer_layers.push(TransformerLayer {
                ln_1_weight,
                ln_1_bias,
                attn_c_attn,
                attn_c_proj,
                ln_2_weight,
                ln_2_bias,
                mlp_c_fc,
                mlp_c_proj,
            });
            
            println!("  âœ… ë ˆì´ì–´ {} ì™„ë£Œ", layer_idx);
        }
        
        // 6. ìµœì¢… ë ˆì´ì–´ë“¤ ë¡œë“œ
        println!("ğŸ¯ ìµœì¢… ë ˆì´ì–´ë“¤ ë¡œë“œ ì¤‘...");
        let final_ln_weight = Self::load_original_vector(
            &metadata, weights_path, "transformer.ln_f.weight", 768)?;
        let final_ln_bias = Self::load_original_vector(
            &metadata, weights_path, "transformer.ln_f.bias", 768)?;
        let lm_head = Self::load_original_matrix(
            &metadata, weights_path, "lm_head.weight", 768, 51200)?;
        
        println!("âœ… ì›ë³¸ GPT-2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ!");
        println!("   - íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´: {} ê°œ", transformer_layers.len());
        println!("   - LM Head: {} x {}", lm_head.nrows(), lm_head.ncols());
        println!("   - ğŸ¯ 100% ì›ë³¸ ë¬´ì†ì‹¤");
        
        Ok(Self {
            tokenizer,
            config,
            token_embeddings,
            position_embeddings,
            transformer_layers,
            final_ln_weight,
            final_ln_bias,
            lm_head,
        })
    }
    
    /// ì›ë³¸ numpy íŒŒì¼ì—ì„œ ë§¤íŠ¸ë¦­ìŠ¤ ë¡œë“œ (ì™„ì „ ë¬´ì†ì‹¤)
    fn load_original_matrix(
        metadata: &HashMap<String, Value>,
        weights_dir: &Path,
        layer_name: &str,
        expected_rows: usize,
        expected_cols: usize,
    ) -> Result<DMatrix<f32>> {
        
        if let Some(layer_info) = metadata.get(layer_name) {
            if let Some(info_obj) = layer_info.as_object() {
                if let (Some(shape_val), Some(file_val)) = 
                    (info_obj.get("shape"), info_obj.get("file")) {
                    
                    let file_name = file_val.as_str().unwrap();
                    let npy_path = weights_dir.join(file_name);
                    
                    // shape ì •ë³´ í™•ì¸
                    let shape = shape_val.as_array().unwrap();
                    let actual_rows = shape[0].as_u64().unwrap() as usize;
                    let actual_cols = shape[1].as_u64().unwrap() as usize;
                    
                    println!("    ğŸ“ ë¡œë“œ: {} â†’ {}Ã—{}", layer_name, actual_rows, actual_cols);
                    
                    // numpy íŒŒì¼ ì½ê¸°
                    let (data, _) = Self::read_npy_data(&npy_path)?;
                    
                    // ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± (row-major)
                    let mut matrix = DMatrix::from_row_slice(actual_rows, actual_cols, &data);
                    
                    // PyTorch ê°€ì¤‘ì¹˜ëŠ” ì „ì¹˜ëœ í˜•íƒœë¡œ ì €ì¥ë¨ -> í•„ìš” ì‹œ ì „ì¹˜
                    if actual_rows != expected_rows || actual_cols != expected_cols {
                        if actual_rows == expected_cols && actual_cols == expected_rows {
                            println!("    ğŸ”„ ì „ì¹˜ ì ìš©: {}Ã—{} â†’ {}Ã—{}", actual_rows, actual_cols, expected_rows, expected_cols);
                            matrix = matrix.transpose();
                        } else {
                            println!("    âš ï¸ í¬ê¸° ë¶ˆì¼ì¹˜: ì˜ˆìƒ {}Ã—{}, ì‹¤ì œ {}Ã—{}", 
                                    expected_rows, expected_cols, actual_rows, actual_cols);
                        }
                    }
                    
                    return Ok(matrix);
                }
            }
        }
        
        Err(anyhow::anyhow!("ë ˆì´ì–´ {}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", layer_name))
    }
    
    /// ì›ë³¸ numpy íŒŒì¼ì—ì„œ 1D ë²¡í„° ë¡œë“œ (ì™„ì „ ë¬´ì†ì‹¤)
    fn load_original_vector(
        metadata: &HashMap<String, Value>,
        weights_dir: &Path,
        layer_name: &str,
        expected_size: usize,
    ) -> Result<Vec<f32>> {
        
        if let Some(layer_info) = metadata.get(layer_name) {
            if let Some(info_obj) = layer_info.as_object() {
                if let Some(file_val) = info_obj.get("file") {
                    let file_name = file_val.as_str().unwrap();
                    let npy_path = weights_dir.join(file_name);
                    
                    println!("    ğŸ“ ë¡œë“œ: {} ({} ê°œ)", layer_name, expected_size);
                    
                    // numpy íŒŒì¼ ì½ê¸°
                    let (data, _) = Self::read_npy_data(&npy_path)?;
                    
                    // í¬ê¸° ê²€ì¦
                    if data.len() != expected_size {
                        println!("    âš ï¸ í¬ê¸° ë¶ˆì¼ì¹˜: ì˜ˆìƒ {}, ì‹¤ì œ {}", expected_size, data.len());
                    }
                    
                    return Ok(data);
                }
            }
        }
        
        Err(anyhow::anyhow!("ë ˆì´ì–´ {}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", layer_name))
    }
    
    /// numpy íŒŒì¼ ì½ê¸° (ì •í™•í•œ í—¤ë” íŒŒì‹±)
    fn read_npy_data(path: &Path) -> Result<(Vec<f32>, Vec<usize>)> {
        use std::fs::File;
        use std::io::{Read, Seek, SeekFrom};
        
        let mut file = File::open(path)?;
        
        // numpy í—¤ë” ì½ê¸°
        let mut magic = [0u8; 6];
        file.read_exact(&mut magic)?;
        
        if &magic != b"\x93NUMPY" {
            return Err(anyhow::anyhow!("ì˜¬ë°”ë¥¸ numpy íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤"));
        }
        
        // ë²„ì „ ì •ë³´
        let mut version = [0u8; 2];
        file.read_exact(&mut version)?;
        
        // í—¤ë” ê¸¸ì´
        let mut header_len_bytes = [0u8; 2];
        file.read_exact(&mut header_len_bytes)?;
        let header_len = u16::from_le_bytes(header_len_bytes) as usize;
        
        // í—¤ë” ë‚´ìš© ì½ê¸°
        let mut header_bytes = vec![0u8; header_len];
        file.read_exact(&mut header_bytes)?;
        let header_str = String::from_utf8_lossy(&header_bytes);
        
        // shapeì™€ dtype íŒŒì‹± (ê°„ë‹¨ ë²„ì „)
        let shape: Vec<usize> = if header_str.contains("(") {
            header_str.split("(").nth(1).unwrap()
                .split(")").next().unwrap()
                .split(",")
                .filter_map(|s| s.trim().parse().ok())
                .collect()
        } else {
            vec![]
        };
        
        let total_elements: usize = shape.iter().product();
        
        // float32 ë°ì´í„° ì½ê¸°
        let mut data_bytes = vec![0u8; total_elements * 4];
        file.read_exact(&mut data_bytes)?;
        
        let data: Vec<f32> = data_bytes.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        println!("    âœ… numpy ë¡œë“œ: {} ìš”ì†Œ, shape: {:?}", data.len(), shape);
        
        Ok((data, shape))
    }

    /// ì›ë³¸ GPT-2 Forward Pass
    fn generate_text(&self, prompt: &str, max_tokens: usize) -> Result<String> {
        println!("\nğŸ’­ ì›ë³¸ GPT-2ë¡œ í…ìŠ¤íŠ¸ ìƒì„±: '{}'", prompt);
        
        // 1. í† í¬ë‚˜ì´ì§•
        let encoding = self.tokenizer.encode(prompt, false)
            .map_err(|e| anyhow::anyhow!("í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {:?}", e))?;
        let mut token_ids: Vec<u32> = encoding.get_ids().to_vec();
        
        println!("ğŸ”¤ ì´ˆê¸° í† í° ìˆ˜: {}", token_ids.len());
        
        // 2. ìƒì„± ë£¨í”„
        for step in 0..max_tokens {
            let next_token = self.forward_pass(&token_ids)?;
            
            // EOS ì²´í¬
            if next_token == 1 || next_token == 0 { // GPT-2 EOS
                break;
            }
            
            token_ids.push(next_token);
            
            // ì§„í–‰ ìƒí™© ì¶œë ¥
            if step % 3 == 0 {
                let partial = self.tokenizer.decode(&token_ids, true)
                    .unwrap_or_else(|_| "ë””ì½”ë”© ì˜¤ë¥˜".to_string());
                println!("ğŸ“ ë‹¨ê³„ {}: {}", step, partial);
            }
        }
        
        // 3. ìµœì¢… ë””ì½”ë”©
        let result = self.tokenizer.decode(&token_ids, true)
            .map_err(|e| anyhow::anyhow!("ë””ì½”ë”© ì‹¤íŒ¨: {:?}", e))?;
        
        Ok(result)
    }
    
    /// ì›ë³¸ GPT-2 Forward Pass
    fn forward_pass(&self, token_ids: &[u32]) -> Result<u32> {
        let seq_len = token_ids.len().min(self.config.n_positions);
        let recent_tokens = &token_ids[token_ids.len().saturating_sub(seq_len)..];
        
        // 1. ì„ë² ë”©: í† í° + ìœ„ì¹˜
        let mut hidden_states = self.create_embeddings(recent_tokens);
        
        // 2. 12ê°œ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í†µê³¼
        for (layer_idx, layer) in self.transformer_layers.iter().enumerate() {
            hidden_states = self.apply_transformer_layer(&hidden_states, layer, layer_idx)?;
        }
        
        // 3. ìµœì¢… LayerNorm
        self.apply_layer_norm(&mut hidden_states, &self.final_ln_weight, &self.final_ln_bias);
        
        // 4. LM Headë¡œ ë¡œì§“ ê³„ì‚°
        let last_hidden = hidden_states.row(hidden_states.nrows() - 1);
        let logits = &self.lm_head.transpose() * last_hidden.transpose();
        
        // 5. ìƒ˜í”Œë§
        let next_token = self.sample_token(&logits.as_slice())?;
        
        Ok(next_token)
    }
    
    /// ì„ë² ë”© ìƒì„± (í† í° + ìœ„ì¹˜)
    fn create_embeddings(&self, token_ids: &[u32]) -> DMatrix<f32> {
        let seq_len = token_ids.len();
        let mut embeddings = DMatrix::zeros(seq_len, self.config.n_embd);
        
        for (pos, &token_id) in token_ids.iter().enumerate() {
            let token_idx = (token_id as usize) % self.token_embeddings.nrows();
            let pos_idx = pos % self.position_embeddings.nrows();
            
            // í† í° ì„ë² ë”© + ìœ„ì¹˜ ì„ë² ë”©
            for j in 0..self.config.n_embd {
                embeddings[(pos, j)] = self.token_embeddings[(token_idx, j)] 
                                     + self.position_embeddings[(pos_idx, j)];
            }
        }
        
        embeddings
    }
    
    /// íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì ìš©
    fn apply_transformer_layer(
        &self,
        input: &DMatrix<f32>,
        layer: &TransformerLayer,
        _layer_idx: usize,
    ) -> Result<DMatrix<f32>> {
        
        // 1. Pre-LayerNorm for Attention
        let mut attn_input = input.clone();
        self.apply_layer_norm(&mut attn_input, &layer.ln_1_weight, &layer.ln_1_bias);
        
        // 2. Multi-Head Self-Attention
        let attn_output = self.apply_attention(&attn_input, &layer.attn_c_attn, &layer.attn_c_proj)?;
        
        // 3. Residual Connection
        let after_attn = &attn_output + input;
        
        // 4. Pre-LayerNorm for FFN
        let mut ffn_input = after_attn.clone();
        self.apply_layer_norm(&mut ffn_input, &layer.ln_2_weight, &layer.ln_2_bias);
        
        // 5. Feed-Forward Network
        let ffn_output = self.apply_ffn(&ffn_input, &layer.mlp_c_fc, &layer.mlp_c_proj)?;
        
        // 6. Residual Connection
        let final_output = &ffn_output + &after_attn;
        
        Ok(final_output)
    }
    
    /// Multi-Head Self-Attention
    fn apply_attention(
        &self,
        input: &DMatrix<f32>,
        c_attn: &DMatrix<f32>,
        c_proj: &DMatrix<f32>,
    ) -> Result<DMatrix<f32>> {
        let seq_len = input.nrows();
        let d_model = input.ncols();
        let n_head = self.config.n_head;
        let head_dim = d_model / n_head;
        
        // QKV ê³„ì‚°
        let qkv = input * c_attn; // seq_len x 2304
        
        let mut output = DMatrix::zeros(seq_len, d_model);
        
        for head in 0..n_head {
            let head_start = head * head_dim;
            
            // Q, K, V ì¶”ì¶œ
            let q = qkv.columns(head_start, head_dim);
            let k = qkv.columns(head_start + d_model, head_dim);
            let v = qkv.columns(head_start + 2 * d_model, head_dim);
            
            // Attention ê³„ì‚°
            let scores = &q * k.transpose() / (head_dim as f32).sqrt();
            
            // Causal masking
            let mut masked_scores = scores;
            for i in 0..seq_len {
                for j in (i+1)..seq_len {
                    masked_scores[(i, j)] = f32::NEG_INFINITY;
                }
            }
            
            // Softmax
            let attn_weights = self.softmax_2d(&masked_scores);
            
            // Apply attention
            let head_output = &attn_weights * &v;
            
            // ê²°ê³¼ í•©ì¹˜ê¸°
            for i in 0..seq_len {
                for j in 0..head_dim {
                    output[(i, head_start + j)] = head_output[(i, j)];
                }
            }
        }
        
        // Output projection
        let final_output = &output * c_proj;
        
        Ok(final_output)
    }
    
    /// Feed-Forward Network
    fn apply_ffn(
        &self,
        input: &DMatrix<f32>,
        c_fc: &DMatrix<f32>,
        c_proj: &DMatrix<f32>,
    ) -> Result<DMatrix<f32>> {
        
        // ì²« ë²ˆì§¸ linear: 768 -> 3072
        let intermediate = input * c_fc;
        
        // GELU í™œì„±í™”
        let activated = self.gelu(&intermediate);
        
        // ë‘ ë²ˆì§¸ linear: 3072 -> 768
        let output = &activated * c_proj;
        
        Ok(output)
    }
    
    /// GELU í™œì„±í™” í•¨ìˆ˜
    fn gelu(&self, input: &DMatrix<f32>) -> DMatrix<f32> {
        input.map(|x| {
            0.5 * x * (1.0 + (x * 0.7978845608 * (1.0 + 0.044715 * x * x)).tanh())
        })
    }
    
    /// Layer Normalization
    fn apply_layer_norm(&self, tensor: &mut DMatrix<f32>, weight: &[f32], bias: &[f32]) {
        let eps = 1e-5;
        
        for mut row in tensor.row_iter_mut() {
            let mean = row.sum() / row.len() as f32;
            let variance = row.iter()
                .map(|x| (x - mean).powi(2))
                .sum::<f32>() / row.len() as f32;
            let std = (variance + eps).sqrt();
            
            for (j, x) in row.iter_mut().enumerate() {
                let normalized = (*x - mean) / std;
                let w = weight.get(j).unwrap_or(&1.0);
                let b = bias.get(j).unwrap_or(&0.0);
                *x = normalized * w + b;
            }
        }
    }
    
    /// 2D Softmax
    fn softmax_2d(&self, input: &DMatrix<f32>) -> DMatrix<f32> {
        let mut output = input.clone();
        
        for mut row in output.row_iter_mut() {
            let max_val = row.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
            let exp_sum: f32 = row.iter()
                .map(|&x| (x - max_val).exp())
                .sum();
            
            for x in row.iter_mut() {
                *x = (*x - max_val).exp() / exp_sum;
            }
        }
        
        output
    }
    
    /// í† í° ìƒ˜í”Œë§
    fn sample_token(&self, logits: &[f32]) -> Result<u32> {
        if logits.is_empty() {
            return Ok(0);
        }
        
        let temperature = 0.8;
        let scaled_logits: Vec<f32> = logits.iter()
            .map(|&x| x / temperature)
            .collect();
        
        let max_logit = scaled_logits.iter()
            .cloned()
            .fold(f32::NEG_INFINITY, f32::max);
        
        let exp_logits: Vec<f32> = scaled_logits.iter()
            .map(|&x| (x - max_logit).exp())
            .collect();
        
        let sum_exp: f32 = exp_logits.iter().sum();
        if sum_exp <= 0.0 {
            return Ok(0);
        }
        
        let probabilities: Vec<f32> = exp_logits.iter()
            .map(|&x| x / sum_exp)
            .collect();
        
        let random_val: f32 = rand::random();
        let mut cumulative = 0.0f32;
        
        for (i, &prob) in probabilities.iter().enumerate() {
            cumulative += prob;
            if random_val <= cumulative {
                return Ok(i as u32);
            }
        }
        
        Ok((probabilities.len() - 1) as u32)
    }
    
    /// ëª¨ë¸ ì •ë³´ ì¶œë ¥
    fn print_model_info(&self) {
        println!("\nğŸ“Š ì›ë³¸ GPT-2 ëª¨ë¸ ì •ë³´:");
        println!("  ğŸ”¤ ì–´íœ˜ í¬ê¸°: {}", self.config.vocab_size);
        println!("  ğŸ§  ì€ë‹‰ì¸µ í¬ê¸°: {}", self.config.n_embd);
        println!("  ğŸ“š ë ˆì´ì–´ ìˆ˜: {}", self.config.n_layer);
        println!("  ğŸ‘¥ ì–´í…ì…˜ í—¤ë“œ: {}", self.config.n_head);
        println!("  ğŸ“ ìµœëŒ€ ìœ„ì¹˜: {}", self.config.n_positions);
        println!("  âœ… 100% ì›ë³¸ ë¬´ì†ì‹¤ ë¡œë“œ");
        println!("  ğŸ¯ ì••ì¶• ì—†ëŠ” ìˆœìˆ˜ ëª¨ë¸");
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ‡°ğŸ‡· === ì›ë³¸ GPT-2 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===");
    println!("ì••ì¶• ì—†ëŠ” ìˆœìˆ˜ ì›ë³¸ ëª¨ë¸ ë™ì‘ í™•ì¸\n");
    
    let original_weights_dir = "./models/skt-kogpt2-base-v2/weights";
    let tokenizer_path = "./models/skt-kogpt2-base-v2/tokenizer.json";
    
    // ì›ë³¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
    let weights_dir_exists = Path::new(original_weights_dir).exists();
    let tokenizer_exists = Path::new(tokenizer_path).exists();
    
    println!("ğŸ“‹ ì›ë³¸ ëª¨ë¸ íŒŒì¼ í™•ì¸:");
    println!("   - ì›ë³¸ ê°€ì¤‘ì¹˜: {} ({})", original_weights_dir, 
             if weights_dir_exists { "ì¡´ì¬" } else { "âŒ ì—†ìŒ" });
    println!("   - í† í¬ë‚˜ì´ì €: {} ({})", tokenizer_path, 
             if tokenizer_exists { "ì¡´ì¬" } else { "âŒ ì—†ìŒ" });
    
    if !weights_dir_exists || !tokenizer_exists {
        return Err(anyhow::anyhow!("ì›ë³¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. extract_weights.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."));
    }
    
    // ì›ë³¸ GPT-2 ëª¨ë¸ ë¡œë“œ
    let model = OriginalGPT2Model::load_from_numpy(original_weights_dir, tokenizer_path)?;
    model.print_model_info();
    
    println!("\nğŸ’¬ ì›ë³¸ GPT-2ë¡œ í•œêµ­ì–´ ëŒ€í™” ì‹œì‘! (ì¢…ë£Œ: 'exit')");

    let stdin = io::stdin();
    loop {
        print!("\nì§ˆë¬¸: ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        stdin.read_line(&mut input)?;
        let input = input.trim();
        
        if input == "exit" || input == "quit" || input == "ì¢…ë£Œ" {
            println!("ğŸ‘‹ ì›ë³¸ GPT-2 ì—”ì§„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.");
            break;
        }
        
        if input.is_empty() {
            continue;
        }

        let start = Instant::now();
        
        match model.generate_text(input, 25) {
            Ok(response) => {
                let duration = start.elapsed();
                
                let generated_part = if response.starts_with(input) {
                    response[input.len()..].trim()
                } else {
                    &response
                };
                
                if !generated_part.is_empty() {
                    println!("ğŸ¯ ì›ë³¸ GPT-2 ë‹µë³€: {}", generated_part);
                } else {
                    println!("ğŸ¯ ì›ë³¸ GPT-2 ë‹µë³€: {}", response);
                }
                
                println!("â±ï¸ ìƒì„± ì‹œê°„: {:.2}ì´ˆ", duration.as_secs_f32());
                println!("âœ¨ 100% ì›ë³¸ ëª¨ë¸, ì••ì¶• ì—†ìŒ");
            }
            Err(e) => {
                println!("âŒ ì˜¤ë¥˜: {}", e);
            }
        }
    }

    Ok(())
} 