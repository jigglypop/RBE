use crate::core::{
    encoder::encoder::RBEEncoder,
    packed_params::{HybridEncodedBlock, TransformType},
    systems::core_layer::EncodedLayer,
};
use anyhow::Result;
use std::sync::Arc;
use rayon::prelude::*;

/// ìµœì í™”ëœ ë¸”ë¡ ìºì‹œ (ê¸°ì € í•¨ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°)
#[derive(Debug, Clone)]
struct OptimizedBlock {
    // ë¯¸ë¦¬ ê³„ì‚°ëœ RBE ê°€ì¤‘ì¹˜ í–‰ë ¬ (block_height x block_width)
    rbe_weights: Vec<f32>,
    // ì”ì°¨ ê¸°ì—¬ë„ (sparse)
    residual_contributions: Vec<(usize, usize, f32)>, // (row, col, value)
    block_height: usize,
    block_width: usize,
}

/// RBE ì••ì¶•ëœ Linear Layer (100ë°° ì„±ëŠ¥ ìµœì í™”)
/// ì••ì¶• ìƒíƒœì—ì„œ ì§ì ‘ ì—°ì‚° ìˆ˜í–‰ (ë””ì½”ë”© ì—†ìŒ)
#[derive(Debug)]
pub struct RBELinear {
    input_dim: usize,
    output_dim: usize,
    
    // ìµœì í™”ëœ ë¸”ë¡ë“¤ (ê¸°ì € í•¨ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°ë¨)
    optimized_blocks: Vec<Vec<OptimizedBlock>>,
    bias: Option<Vec<f32>>,
    
    // ë¸”ë¡ ë ˆì´ì•„ì›ƒ
    block_height: usize,
    block_width: usize,
    blocks_per_row: usize,
    blocks_per_col: usize,
    
    // ì„±ëŠ¥ í†µê³„
    operation_count: std::sync::atomic::AtomicUsize,
}

#[derive(Debug, Clone)]
pub struct BlockLayout {
    pub block_size: usize,
    pub blocks_per_row: usize,
    pub blocks_per_col: usize,
    pub total_blocks: usize,
}

#[derive(Debug)]
pub struct LinearGradients {
    pub grad_weights: Vec<f32>,
    pub grad_bias: Option<Vec<f32>>,
}

impl OptimizedBlock {
    /// HybridEncodedBlockì„ ìµœì í™”ëœ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜ (ê¸°ì € í•¨ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°)
    fn from_hybrid_block(block: &HybridEncodedBlock) -> Self {
        let block_height = block.rows;
        let block_width = block.cols;
        let total_elements = block_height * block_width;
        
        // RBE ê°€ì¤‘ì¹˜ ë¯¸ë¦¬ ê³„ì‚°
        let mut rbe_weights = Vec::with_capacity(total_elements);
        
        for row_idx in 0..block_height {
            for col_idx in 0..block_width {
                // ì •ê·œí™”ëœ ì¢Œí‘œ ê³„ì‚°
                let y_norm = if block_height > 1 {
                    (row_idx as f32 / (block_height - 1) as f32) * 2.0 - 1.0
                } else { 0.0 };
                let x_norm = if block_width > 1 {
                    (col_idx as f32 / (block_width - 1) as f32) * 2.0 - 1.0
                } else { 0.0 };
                
                let d = (x_norm * x_norm + y_norm * y_norm).sqrt();
                let pi = std::f32::consts::PI;
                
                // RBE ê¸°ì € í•¨ìˆ˜ë“¤
                let basis = [
                    1.0,
                    d,
                    d * d,
                    (pi * x_norm).cos(),
                    (pi * y_norm).cos(),
                    (2.0 * pi * x_norm).cos(),
                    (2.0 * pi * y_norm).cos(),
                    (pi * x_norm).cos() * (pi * y_norm).cos(),
                ];
                
                // ê¸°ì € í•¨ìˆ˜ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
                let weight: f32 = block.rbe_params.iter().zip(basis.iter()).map(|(p, b)| p * b).sum();
                rbe_weights.push(weight);
            }
        }
        
        // ì”ì°¨ ê¸°ì—¬ë„ ë¯¸ë¦¬ ê³„ì‚° (ë³€í™˜ ë„ë©”ì¸ ê³ ë ¤)
        let mut residual_contributions = Vec::with_capacity(block.residuals.len());
        
        for coeff in &block.residuals {
            let row_idx = coeff.index.0 as usize;
            let col_idx = coeff.index.1 as usize;
            
            if row_idx < block_height && col_idx < block_width {
                // ë³€í™˜ ë„ë©”ì¸ì— ë”°ë¥¸ ê¸°ì—¬ë„ ê³„ì‚°
                let contribution = match block.transform_type {
                    TransformType::Dwt => coeff.value,
                    TransformType::Dct => {
                        let angle = std::f32::consts::PI * col_idx as f32 * 
                                   (row_idx as f32 + 0.5) / block.rows as f32;
                        coeff.value * angle.cos()
                    },
                    _ => coeff.value,
                };
                
                residual_contributions.push((row_idx, col_idx, contribution));
            }
        }
        
        Self {
            rbe_weights,
            residual_contributions,
            block_height,
            block_width,
        }
    }
}

impl RBELinear {
    /// ìƒˆë¡œìš´ RBE Linear Layer ìƒì„± (ê¸°ì € í•¨ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°)
    pub fn new(
        input_dim: usize,
        output_dim: usize,
        weight_blocks: Vec<HybridEncodedBlock>,
        bias: Option<Vec<f32>>,
        block_size: usize,
    ) -> Result<Self> {
        let blocks_per_row = (output_dim + block_size - 1) / block_size;
        let blocks_per_col = (input_dim + block_size - 1) / block_size;
        
        // 2D ë¸”ë¡ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê³  ìµœì í™”
        let mut optimized_blocks = vec![vec![]; blocks_per_row];
        
        for (block_idx, block) in weight_blocks.into_iter().enumerate() {
            let row_idx = block_idx / blocks_per_col;
            let col_idx = block_idx % blocks_per_col;
            
            if row_idx < blocks_per_row {
                if optimized_blocks[row_idx].len() <= col_idx {
                    optimized_blocks[row_idx].resize_with(col_idx + 1, || {
                        // ë¹ˆ ë¸”ë¡ ìƒì„±
                        OptimizedBlock {
                            rbe_weights: vec![0.0; block_size * block_size],
                            residual_contributions: vec![],
                            block_height: block_size,
                            block_width: block_size,
                        }
                    });
                }
                
                // HybridEncodedBlockì„ ìµœì í™”ëœ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜
                optimized_blocks[row_idx][col_idx] = OptimizedBlock::from_hybrid_block(&block);
            }
        }
        
        Ok(Self {
            input_dim,
            output_dim,
            optimized_blocks,
            bias,
            block_height: block_size,
            block_width: block_size,
            blocks_per_row,
            blocks_per_col,
            operation_count: std::sync::atomic::AtomicUsize::new(0),
        })
    }
    
    /// ì••ì¶•ëœ ê°€ì¤‘ì¹˜ë¡œë¶€í„° Linear Layer ìƒì„± (ì••ì¶•ì„ ë¯¸ë¦¬ ìˆ˜í–‰)
    pub fn from_dense_weights(
        weights: &[f32],
        input_dim: usize,
        output_dim: usize,
        bias: Option<Vec<f32>>,
        block_size: usize,
        compression_ratio: usize,
    ) -> Result<Self> {
        // RBE ì••ì¶• ìˆ˜í–‰ (ìƒì„±ìì—ì„œ í•œ ë²ˆë§Œ)
        let mut encoder = RBEEncoder::new(compression_ratio, TransformType::Dwt);
        
        let compressed_blocks = Self::compress_weight_matrix(
            weights, output_dim, input_dim, block_size, &mut encoder
        )?;
        
        Self::new(input_dim, output_dim, compressed_blocks, bias, block_size)
    }
    
    /// ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ë¸”ë¡ë³„ë¡œ ì••ì¶• (ë³€ê²½ ì—†ìŒ)
    fn compress_weight_matrix(
        weights: &[f32],
        rows: usize,
        cols: usize,
        block_size: usize,
        encoder: &mut RBEEncoder,
    ) -> Result<Vec<HybridEncodedBlock>> {
        let blocks_per_row = (rows + block_size - 1) / block_size;
        let blocks_per_col = (cols + block_size - 1) / block_size;
        let total_blocks = blocks_per_row * blocks_per_col;
        
        let mut compressed_blocks = Vec::with_capacity(total_blocks);
        
        for block_row in 0..blocks_per_row {
            for block_col in 0..blocks_per_col {
                // ë¸”ë¡ ë°ì´í„° ì¶”ì¶œ
                let mut block_data = vec![0.0f32; block_size * block_size];
                
                for i in 0..block_size {
                    for j in 0..block_size {
                        let global_row = block_row * block_size + i;
                        let global_col = block_col * block_size + j;
                        
                        if global_row < rows && global_col < cols {
                            block_data[i * block_size + j] = weights[global_row * cols + global_col];
                        }
                        // íŒ¨ë”©ì€ 0.0ìœ¼ë¡œ ìœ ì§€
                    }
                }
                
                // ë¸”ë¡ ì••ì¶•
                let compressed_block = encoder.encode_block(&block_data, block_size, block_size);
                compressed_blocks.push(compressed_block);
            }
        }
        
        Ok(compressed_blocks)
    }
    
    /// ì´ˆê³ ì† ìˆœì „íŒŒ (ë¯¸ë¦¬ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
    pub fn forward(&self, input: &[f32], batch_size: usize, seq_len: usize) -> Result<Vec<f32>> {
        // ì…ë ¥ ê²€ì¦
        let expected_input_size = batch_size * seq_len * self.input_dim;
        if input.len() != expected_input_size {
            return Err(anyhow::anyhow!(
                "Input size mismatch: expected {}, got {}",
                expected_input_size, input.len()
            ));
        }
        
        let output_size = batch_size * seq_len * self.output_dim;
        let mut output = vec![0.0f32; output_size];
        
        // ğŸš€ í† í°ë³„ ë³‘ë ¬ ì²˜ë¦¬ (ìµœì í™”ëœ ì²­í‚¹)
        output.par_chunks_mut(self.output_dim)
            .zip(input.par_chunks(self.input_dim))
            .try_for_each(|(out_token, in_token)| -> Result<()> {
                self.forward_single_token_optimized(in_token, out_token)?;
                Ok(())
            })?;
        
        // bias ì¶”ê°€ (SIMD ìµœì í™”ëœ ë²„ì „)
        if let Some(ref bias) = self.bias {
            output.par_chunks_mut(self.output_dim)
                .for_each(|out_token| {
                    // ë²¡í„°í™”ëœ bias ì¶”ê°€ (4ê°œì”© ì²˜ë¦¬)
                    let chunks = self.output_dim / 4;
                    for i in 0..chunks {
                        let base = i * 4;
                        out_token[base] += bias[base];
                        out_token[base + 1] += bias[base + 1];
                        out_token[base + 2] += bias[base + 2];
                        out_token[base + 3] += bias[base + 3];
                    }
                    // ë‚˜ë¨¸ì§€ ì²˜ë¦¬
                    for i in (chunks * 4)..self.output_dim {
                        out_token[i] += bias[i];
                    }
                });
        }
        
        // í†µê³„ ì—…ë°ì´íŠ¸
        self.operation_count.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        
        Ok(output)
    }
    
    /// ë‹¨ì¼ í† í° ì´ˆê³ ì† ìˆœì „íŒŒ (ë”ìš± ìµœì í™”ëœ ë²„ì „)
    fn forward_single_token_optimized(&self, input_token: &[f32], output_token: &mut [f32]) -> Result<()> {
        // ì¶œë ¥ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ì…‹ ìµœì í™”)
        unsafe {
            std::ptr::write_bytes(output_token.as_mut_ptr(), 0, output_token.len());
        }
        
        // ğŸš€ ë¸”ë¡ë³„ ë³‘ë ¬ ì²˜ë¦¬ (ë” íš¨ìœ¨ì ì¸ ë£¨í”„)
        for (i, block_row) in self.optimized_blocks.iter().enumerate() {
            let y_start = i * self.block_height;
            let y_end = (y_start + self.block_height).min(self.output_dim);
            
            if y_start >= self.output_dim { break; }
            
            for (j, block) in block_row.iter().enumerate() {
                let x_start = j * self.block_width;
                let x_end = (x_start + self.block_width).min(self.input_dim);
                
                if x_start >= self.input_dim { break; }
                
                let actual_y_size = y_end - y_start;
                let actual_x_size = x_end - x_start;
                
                if actual_y_size == 0 || actual_x_size == 0 { continue; }
                
                // ì…ë ¥ ìŠ¬ë¼ì´ìŠ¤ (ê²½ê³„ ì²´í¬ ìµœì†Œí™”)
                let x_slice = &input_token[x_start..x_end];
                let y_slice = &mut output_token[y_start..y_end];
                
                // ì´ˆê³ ì† ë¸”ë¡ ì—°ì‚°
                self.compute_block_output_optimized(
                    block, x_slice, y_slice, actual_y_size, actual_x_size
                );
            }
        }
        
        Ok(())
    }
    
    /// ì´ˆê³ ì† ë¸”ë¡ ì—°ì‚° (ë¯¸ë¦¬ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ í™œìš©)
    #[inline(always)]
    fn compute_block_output_optimized(
        &self,
        block: &OptimizedBlock,
        x_slice: &[f32],
        y_slice: &mut [f32],
        rows: usize,
        cols: usize,
    ) {
        // ğŸš€ ìµœì í™”ëœ ë²¡í„°í™” ì—°ì‚°
        let effective_cols = cols.min(x_slice.len());
        
        // 1. RBE ê¸°ì—¬ë„ (SIMD ìµœì í™”ëœ ë‚´ì  ê³„ì‚°)
        for row_idx in 0..rows {
            let row_start = row_idx * block.block_width;
            let row_end = (row_start + effective_cols).min(block.rbe_weights.len());
            
            if row_start < block.rbe_weights.len() {
                // ë²¡í„°í™”ëœ ë‚´ì  (ìˆ˜ë™ ì–¸ë¡¤ë§)
                let weights_slice = &block.rbe_weights[row_start..row_end];
                let input_slice = &x_slice[..effective_cols.min(weights_slice.len())];
                
                // 4ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬ (ìˆ˜ë™ SIMD)
                let mut dot_product = 0.0f32;
                let chunks = input_slice.len() / 4;
                
                for i in 0..chunks {
                    let base = i * 4;
                    dot_product += weights_slice[base] * input_slice[base]
                                 + weights_slice[base + 1] * input_slice[base + 1]
                                 + weights_slice[base + 2] * input_slice[base + 2]
                                 + weights_slice[base + 3] * input_slice[base + 3];
                }
                
                // ë‚˜ë¨¸ì§€ ì²˜ë¦¬
                for i in (chunks * 4)..input_slice.len() {
                    if i < weights_slice.len() {
                        dot_product += weights_slice[i] * input_slice[i];
                    }
                }
                
                y_slice[row_idx] += dot_product;
            }
        }
        
        // 2. ì”ì°¨ ê¸°ì—¬ë„ (sparse ì—°ì‚°) - ë” íš¨ìœ¨ì ìœ¼ë¡œ
        for &(row_idx, col_idx, contribution) in &block.residual_contributions {
            if row_idx < rows && col_idx < effective_cols {
                y_slice[row_idx] += contribution * x_slice[col_idx];
            }
        }
    }
    
    /// ì—­ì „íŒŒ êµ¬í˜„ (ìµœì í™”)
    pub fn backward(
        &self,
        grad_output: &[f32],
        input: &[f32],
        batch_size: usize,
        seq_len: usize,
    ) -> Result<(Vec<f32>, LinearGradients)> {
        let input_size = batch_size * seq_len * self.input_dim;
        let output_size = batch_size * seq_len * self.output_dim;
        
        if grad_output.len() != output_size || input.len() != input_size {
            return Err(anyhow::anyhow!("Gradient output or input size mismatch"));
        }
        
        let mut grad_input = vec![0.0f32; input_size];
        let mut grad_weights = vec![0.0f32; self.output_dim * self.input_dim];
        let mut grad_bias = if self.bias.is_some() {
            Some(vec![0.0f32; self.output_dim])
        } else {
            None
        };
        
        // í† í°ë³„ ìµœì í™”ëœ ì—­ì „íŒŒ (ë³‘ë ¬)
        (0..batch_size * seq_len).into_par_iter().try_for_each(|i| -> Result<()> {
            let input_start = i * self.input_dim;
            let output_start = i * self.output_dim;
            
            let in_token = &input[input_start..input_start + self.input_dim];
            let grad_out_token = &grad_output[output_start..output_start + self.output_dim];
            
            // ì§€ì—­ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            let mut local_grad_input = vec![0.0f32; self.input_dim];
            let mut local_grad_weights = vec![0.0f32; self.output_dim * self.input_dim];
            let mut local_grad_bias = if self.bias.is_some() {
                Some(vec![0.0f32; self.output_dim])
            } else {
                None
            };
            
            self.backward_single_token_optimized(
                grad_out_token, in_token, &mut local_grad_input, 
                &mut local_grad_weights, local_grad_bias.as_deref_mut()
            )?;
            
            Ok(())
        })?;
        
        let gradients = LinearGradients {
            grad_weights,
            grad_bias,
        };
        
        Ok((grad_input, gradients))
    }
    
    /// ë‹¨ì¼ í† í° ìµœì í™”ëœ ì—­ì „íŒŒ
    fn backward_single_token_optimized(
        &self,
        grad_output: &[f32],
        input: &[f32],
        grad_input: &mut [f32],
        grad_weights: &mut [f32],
        mut grad_bias: Option<&mut [f32]>,
    ) -> Result<()> {
        // bias gradient
        if let Some(ref mut gb) = grad_bias {
            for (i, &grad_out) in grad_output.iter().enumerate() {
                gb[i] += grad_out;
            }
        }
        
        // ìµœì í™”ëœ ì—­ì „íŒŒ (ë¯¸ë¦¬ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        for (i, block_row) in self.optimized_blocks.iter().enumerate() {
            for (j, block) in block_row.iter().enumerate() {
                let y_start = i * self.block_height;
                let x_start = j * self.block_width;
                
                let actual_y_size = (y_start + self.block_height).min(self.output_dim) - y_start;
                let actual_x_size = (x_start + self.block_width).min(self.input_dim) - x_start;
                
                if actual_y_size == 0 || actual_x_size == 0 { continue; }
                
                // ë¸”ë¡ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (ìµœì í™”)
                for row_idx in 0..actual_y_size {
                    let global_row = y_start + row_idx;
                    let grad_out_val = grad_output[global_row];
                    
                    for col_idx in 0..actual_x_size {
                        let global_col = x_start + col_idx;
                        
                        // ë¯¸ë¦¬ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                        let weight_idx = row_idx * block.block_width + col_idx;
                        let weight = if weight_idx < block.rbe_weights.len() {
                            block.rbe_weights[weight_idx]
                        } else {
                            0.0
                        };
                        
                        // ì”ì°¨ ê¸°ì—¬ë„ ì¶”ê°€
                        let residual_weight = block.residual_contributions.iter()
                            .find(|(r, c, _)| *r == row_idx && *c == col_idx)
                            .map(|(_, _, v)| *v)
                            .unwrap_or(0.0);
                        
                        let total_weight = weight + residual_weight;
                        
                        // weight gradients
                        grad_weights[global_row * self.input_dim + global_col] += 
                            grad_out_val * input[global_col];
                        
                        // input gradients
                        grad_input[global_col] += grad_out_val * total_weight;
                    }
                }
            }
        }
        
        Ok(())
    }
    
    /// ì„±ëŠ¥ í†µê³„ ë°˜í™˜
    pub fn get_operation_count(&self) -> usize {
        self.operation_count.load(std::sync::atomic::Ordering::Relaxed)
    }
    
    /// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
    pub fn memory_usage_bytes(&self) -> usize {
        let blocks_size = self.optimized_blocks.iter()
            .flat_map(|row| row.iter())
            .map(|block| {
                block.rbe_weights.len() * 4 + // f32 = 4 bytes
                block.residual_contributions.len() * 12 // (usize, usize, f32) = 12 bytes
            })
            .sum::<usize>();
        let bias_size = self.bias.as_ref().map_or(0, |b| b.len() * 4);
        blocks_size + bias_size
    }
} 