use std::collections::HashMap;
use std::path::PathBuf;
use anyhow::Result;
use serde_json::Value;

/// í’ˆì§ˆ ë“±ê¸‰ (coreì™€ í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€)
#[derive(Debug, Clone, Copy)]
pub enum QualityGrade {
    S,  // RMSE â‰¤ 0.000001
    A,  // RMSE â‰¤ 0.001
    B,  // RMSE â‰¤ 0.01
    C,  // RMSE â‰¤ 0.1
}

/// ëª¨ë¸ ë¶„ì„ ê²°ê³¼
#[derive(Debug, Clone)]
pub struct ModelAnalysis {
    pub model_info: ModelInfo,
    pub layer_analysis: LayerAnalysis,
    pub parameter_analysis: ParameterAnalysis,
    pub compression_suitability: CompressionSuitability,
    pub performance_estimate: PerformanceEstimate,
}

/// ê¸°ë³¸ ëª¨ë¸ ì •ë³´
#[derive(Debug, Clone)]
pub struct ModelInfo {
    pub model_name: String,
    pub model_type: String,
    pub total_parameters: u64,
    pub model_size_mb: f64,
    pub architecture: String,
    pub vocab_size: Option<u32>,
    pub hidden_size: Option<u32>,
    pub num_layers: Option<u32>,
    pub num_attention_heads: Option<u32>,
}

/// ë ˆì´ì–´ë³„ ë¶„ì„
#[derive(Debug, Clone)]
pub struct LayerAnalysis {
    pub layer_types: HashMap<String, u32>,
    pub layer_parameters: HashMap<String, u64>,
    pub largest_layers: Vec<LayerInfo>,
    pub compression_candidates: Vec<LayerInfo>,
}

#[derive(Debug, Clone)]
pub struct LayerInfo {
    pub name: String,
    pub layer_type: String,
    pub parameters: u64,
    pub shape: Vec<u32>,
    pub compression_ratio_estimate: f32,
}

/// íŒŒë¼ë¯¸í„° ë¶„ì„
#[derive(Debug, Clone)]
pub struct ParameterAnalysis {
    pub total_parameters: u64,
    pub trainable_parameters: u64,
    pub embedding_parameters: u64,
    pub linear_parameters: u64,
    pub attention_parameters: u64,
    pub parameter_distribution: ParameterDistribution,
}

#[derive(Debug, Clone)]
pub struct ParameterDistribution {
    pub mean: f32,
    pub std: f32,
    pub min: f32,
    pub max: f32,
    pub sparsity_ratio: f32,
}

/// ì••ì¶• ì í•©ì„± ë¶„ì„
#[derive(Debug, Clone)]
pub struct CompressionSuitability {
    pub overall_score: f32,
    pub rbe_suitability: f32,
    pub recommended_block_size: u32,
    pub estimated_compression_ratio: f32,
    pub bottleneck_layers: Vec<String>,
    pub memory_reduction_estimate: f32,
}

/// ì„±ëŠ¥ ì¶”ì •
#[derive(Debug, Clone)]
pub struct PerformanceEstimate {
    pub inference_speed_ms: f32,
    pub memory_usage_mb: f32,
    pub gpu_memory_mb: f32,
    pub throughput_tokens_per_sec: f32,
}

/// ëª¨ë¸ ë¶„ì„ê¸°
pub struct ModelAnalyzer {
    pub analysis_cache: HashMap<String, ModelAnalysis>,
}

impl ModelAnalyzer {
    /// ìƒˆë¡œìš´ ë¶„ì„ê¸° ìƒì„±
    pub fn new() -> Self {
        Self {
            analysis_cache: HashMap::new(),
        }
    }
    
    /// ëª¨ë¸ ê²½ë¡œë¡œë¶€í„° ë¶„ì„ ìˆ˜í–‰
    pub async fn analyze_model(&mut self, model_path: &PathBuf) -> Result<ModelAnalysis> {
        let model_name = model_path.file_name()
            .unwrap_or_default()
            .to_string_lossy()
            .to_string();
            
        // ìºì‹œ í™•ì¸
        if let Some(cached) = self.analysis_cache.get(&model_name) {
            return Ok(cached.clone());
        }
        
        println!("ğŸ” ëª¨ë¸ ë¶„ì„ ì‹œì‘: {}", model_name);
        
        // 1. ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
        let model_info = self.extract_model_info(model_path).await?;
        println!("âœ“ ê¸°ë³¸ ì •ë³´ ë¶„ì„ ì™„ë£Œ");
        
        // 2. ë ˆì´ì–´ ë¶„ì„
        let layer_analysis = self.analyze_layers(model_path).await?;
        println!("âœ“ ë ˆì´ì–´ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ");
        
        // 3. íŒŒë¼ë¯¸í„° ë¶„ì„
        let parameter_analysis = self.analyze_parameters(model_path, &model_info).await?;
        println!("âœ“ íŒŒë¼ë¯¸í„° ë¶„ì„ ì™„ë£Œ");
        
        // 4. ì••ì¶• ì í•©ì„± ë¶„ì„
        let compression_suitability = self.analyze_compression_suitability(&model_info, &layer_analysis)?;
        println!("âœ“ ì••ì¶• ì í•©ì„± ë¶„ì„ ì™„ë£Œ");
        
        // 5. ì„±ëŠ¥ ì¶”ì •
        let performance_estimate = self.estimate_performance(&model_info)?;
        println!("âœ“ ì„±ëŠ¥ ì¶”ì • ì™„ë£Œ");
        
        let analysis = ModelAnalysis {
            model_info,
            layer_analysis,
            parameter_analysis,
            compression_suitability,
            performance_estimate,
        };
        
        // ìºì‹œì— ì €ì¥
        self.analysis_cache.insert(model_name, analysis.clone());
        
        Ok(analysis)
    }
    
    /// HuggingFace ëª¨ë¸ IDë¡œ ë¶„ì„ (ë‹¤ìš´ë¡œë“œ í›„)
    pub async fn analyze_huggingface_model(&mut self, model_id: &str) -> Result<ModelAnalysis> {
        use crate::nlp::model_tools::ModelDownloader;
        
        println!("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {}", model_id);
        let downloader = ModelDownloader::new(model_id);
        let model_path = downloader.download().await?;
        
        self.analyze_model(&model_path).await
    }
    
    /// KoMiniLM-23M ë¶„ì„ (ê¸°ë³¸ ì¶”ì²œ ëª¨ë¸)
    pub async fn analyze_kominilm_23m(&mut self) -> Result<ModelAnalysis> {
        println!("ğŸ¯ KoMiniLM-23M ë¶„ì„ ì‹œì‘ (ê¸°ë³¸ ì¶”ì²œ ëª¨ë¸)");
        self.analyze_huggingface_model("BM-K/KoMiniLM").await
    }
    
    /// config.jsonì—ì„œ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
    pub async fn extract_model_info(&self, model_path: &PathBuf) -> Result<ModelInfo> {
        let config_path = model_path.join("config.json");
        
        if !config_path.exists() {
            return Ok(ModelInfo {
                model_name: model_path.file_name().unwrap_or_default().to_string_lossy().to_string(),
                model_type: "unknown".to_string(),
                total_parameters: 0,
                model_size_mb: 0.0,
                architecture: "unknown".to_string(),
                vocab_size: None,
                hidden_size: None,
                num_layers: None,
                num_attention_heads: None,
            });
        }
        
        let config_content = tokio::fs::read_to_string(&config_path).await?;
        let config: Value = serde_json::from_str(&config_content)?;
        
        // ëª¨ë¸ íŒŒì¼ í¬ê¸° ê³„ì‚°
        let model_size_mb = self.calculate_model_size(model_path).await?;
        
        // íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
        let total_parameters = self.estimate_parameters_from_config(&config)?;
        
        Ok(ModelInfo {
            model_name: model_path.file_name().unwrap_or_default().to_string_lossy().to_string(),
            model_type: config.get("model_type")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown")
                .to_string(),
            total_parameters,
            model_size_mb,
            architecture: config.get("architectures")
                .and_then(|v| v.as_array())
                .and_then(|arr| arr.get(0))
                .and_then(|v| v.as_str())
                .unwrap_or("unknown")
                .to_string(),
            vocab_size: config.get("vocab_size").and_then(|v| v.as_u64()).map(|v| v as u32),
            hidden_size: config.get("hidden_size").and_then(|v| v.as_u64()).map(|v| v as u32),
            num_layers: config.get("num_hidden_layers").and_then(|v| v.as_u64()).map(|v| v as u32),
            num_attention_heads: config.get("num_attention_heads").and_then(|v| v.as_u64()).map(|v| v as u32),
        })
    }
    
    /// ë ˆì´ì–´ êµ¬ì¡° ë¶„ì„
    async fn analyze_layers(&self, model_path: &PathBuf) -> Result<LayerAnalysis> {
        let mut layer_types = HashMap::new();
        let mut layer_parameters = HashMap::new();
        let mut largest_layers = Vec::new();
        
        // config.jsonì—ì„œ ë ˆì´ì–´ ì •ë³´ ì¶”ì¶œ
        let config_path = model_path.join("config.json");
        if config_path.exists() {
            let config_content = tokio::fs::read_to_string(&config_path).await?;
            let config: Value = serde_json::from_str(&config_content)?;
            
            // ê¸°ë³¸ ë ˆì´ì–´ë“¤ ì¶”ì •
            if let Some(num_layers) = config.get("num_hidden_layers").and_then(|v| v.as_u64()) {
                layer_types.insert("attention".to_string(), num_layers as u32);
                layer_types.insert("feed_forward".to_string(), num_layers as u32);
                layer_types.insert("layer_norm".to_string(), num_layers as u32 * 2);
                
                // íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
                if let Some(hidden_size) = config.get("hidden_size").and_then(|v| v.as_u64()) {
                    let attention_params = num_layers * hidden_size * hidden_size * 4; // Q, K, V, O
                    let ffn_params = num_layers * hidden_size * hidden_size * 8; // ëŒ€ëµì ì¸ FFN í¬ê¸°
                    
                    layer_parameters.insert("attention".to_string(), attention_params);
                    layer_parameters.insert("feed_forward".to_string(), ffn_params);
                    
                    // í° ë ˆì´ì–´ë“¤ ì‹ë³„
                    largest_layers.push(LayerInfo {
                        name: "transformer_layers".to_string(),
                        layer_type: "transformer_block".to_string(),
                        parameters: attention_params + ffn_params,
                        shape: vec![num_layers as u32, hidden_size as u32],
                        compression_ratio_estimate: 3.0, // ì˜ˆìƒ ì••ì¶•ë¥ 
                    });
                }
            }
        }
        
        // ì••ì¶• í›„ë³´ ë ˆì´ì–´ë“¤ ì‹ë³„
        let compression_candidates = largest_layers.iter()
            .filter(|layer| layer.parameters > 1_000_000) // 1M íŒŒë¼ë¯¸í„° ì´ìƒ
            .cloned()
            .collect();
        
        Ok(LayerAnalysis {
            layer_types,
            layer_parameters,
            largest_layers,
            compression_candidates,
        })
    }
    
    /// íŒŒë¼ë¯¸í„° ë¶„ì„
    async fn analyze_parameters(&self, _model_path: &PathBuf, model_info: &ModelInfo) -> Result<ParameterAnalysis> {
        // ì‹¤ì œ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì§€ ì•Šê³  config ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
        let total_parameters = model_info.total_parameters;
        let trainable_parameters = total_parameters; // ëŒ€ë¶€ë¶„ í›ˆë ¨ ê°€ëŠ¥
        
        // ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ë¶„ë°° ì¶”ì •
        let vocab_size = model_info.vocab_size.unwrap_or(32000) as u64;
        let hidden_size = model_info.hidden_size.unwrap_or(768) as u64;
        
        let embedding_parameters = vocab_size * hidden_size * 2; // input + output embeddings
        let attention_parameters = total_parameters / 2; // ëŒ€ëµ ì ˆë°˜ì´ attention
        let linear_parameters = total_parameters - embedding_parameters - attention_parameters;
        
        // ê°€ìƒì˜ ë¶„í¬ í†µê³„ (ì‹¤ì œë¡œëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•´ì•¼ í•¨)
        let parameter_distribution = ParameterDistribution {
            mean: 0.0,
            std: 0.1,
            min: -1.0,
            max: 1.0,
            sparsity_ratio: 0.05, // 5% í¬ì†Œì„± ì¶”ì •
        };
        
        Ok(ParameterAnalysis {
            total_parameters,
            trainable_parameters,
            embedding_parameters,
            linear_parameters: linear_parameters.max(0),
            attention_parameters,
            parameter_distribution,
        })
    }
    
    /// ì••ì¶• ì í•©ì„± ë¶„ì„
    pub fn analyze_compression_suitability(&self, model_info: &ModelInfo, layer_analysis: &LayerAnalysis) -> Result<CompressionSuitability> {
        // ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì í•©ì„± ì ìˆ˜
        let size_score = match model_info.total_parameters {
            0..=50_000_000 => 0.9,      // 50M ì´í•˜: ë§¤ìš° ì í•©
            50_000_001..=200_000_000 => 0.8,   // 50M-200M: ì í•©
            200_000_001..=1_000_000_000 => 0.7, // 200M-1B: ë³´í†µ
            _ => 0.6,                    // 1B ì´ìƒ: ì œí•œì 
        };
        
        // ë ˆì´ì–´ êµ¬ì¡°ì— ë”°ë¥¸ RBE ì í•©ì„±
        let has_attention = layer_analysis.layer_types.contains_key("attention");
        let has_ffn = layer_analysis.layer_types.contains_key("feed_forward");
        let rbe_suitability = if has_attention && has_ffn { 0.9 } else { 0.7 };
        
        // ê¶Œì¥ ë¸”ë¡ í¬ê¸° (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)
        let recommended_block_size = match model_info.total_parameters {
            0..=25_000_000 => 32,       // 25M ì´í•˜: ì‘ì€ ë¸”ë¡
            25_000_001..=100_000_000 => 64,     // 25M-100M: ì¤‘ê°„ ë¸”ë¡
            _ => 128,                    // 100M ì´ìƒ: í° ë¸”ë¡
        };
        
        // ì••ì¶•ë¥  ì¶”ì •
        let estimated_compression_ratio = match model_info.total_parameters {
            0..=25_000_000 => 5.0,      // ì‘ì€ ëª¨ë¸: ë†’ì€ ì••ì¶•ë¥ 
            25_000_001..=100_000_000 => 4.0,    // ì¤‘ê°„ ëª¨ë¸: ì¤‘ê°„ ì••ì¶•ë¥ 
            _ => 3.0,                    // í° ëª¨ë¸: ë‚®ì€ ì••ì¶•ë¥ 
        };
        
        Ok(CompressionSuitability {
            overall_score: (size_score + rbe_suitability) / 2.0,
            rbe_suitability,
            recommended_block_size,
            estimated_compression_ratio,
            bottleneck_layers: vec!["attention".to_string(), "feed_forward".to_string()],
            memory_reduction_estimate: 1.0 - (1.0 / estimated_compression_ratio),
        })
    }
    
    /// ì„±ëŠ¥ ì¶”ì •
    pub fn estimate_performance(&self, model_info: &ModelInfo) -> Result<PerformanceEstimate> {
        // íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜ ì„±ëŠ¥ ì¶”ì •
        let params_millions = model_info.total_parameters as f32 / 1_000_000.0;
        
        // ì¶”ë¡  ì†ë„ (ms) - íŒŒë¼ë¯¸í„° ìˆ˜ì— ë¹„ë¡€
        let inference_speed_ms = params_millions * 0.1; // ëŒ€ëµì ì¸ ì¶”ì •
        
        // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
        let memory_usage_mb = model_info.model_size_mb as f32;
        let gpu_memory_mb = memory_usage_mb * 1.5; // GPUëŠ” ì¶”ê°€ ë©”ëª¨ë¦¬ í•„ìš”
        
        // ì²˜ë¦¬ëŸ‰ (tokens/sec)
        let throughput_tokens_per_sec = 1000.0 / inference_speed_ms;
        
        Ok(PerformanceEstimate {
            inference_speed_ms,
            memory_usage_mb,
            gpu_memory_mb,
            throughput_tokens_per_sec,
        })
    }
    
    /// ëª¨ë¸ íŒŒì¼ í¬ê¸° ê³„ì‚°
    async fn calculate_model_size(&self, model_path: &PathBuf) -> Result<f64> {
        let mut total_size = 0u64;
        
        // ì¼ë°˜ì ì¸ ëª¨ë¸ íŒŒì¼ë“¤ í™•ì¸
        let model_files = vec![
            "pytorch_model.bin",
            "model.safetensors", 
            "tf_model.h5",
            "model.onnx",
        ];
        
        for file_name in model_files {
            let file_path = model_path.join(file_name);
            if file_path.exists() {
                if let Ok(metadata) = tokio::fs::metadata(&file_path).await {
                    total_size += metadata.len();
                }
            }
        }
        
        Ok(total_size as f64 / (1024.0 * 1024.0)) // MBë¡œ ë³€í™˜
    }
    
    /// configì—ì„œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
    pub fn estimate_parameters_from_config(&self, config: &Value) -> Result<u64> {
        let vocab_size = config.get("vocab_size").and_then(|v| v.as_u64()).unwrap_or(32000);
        let hidden_size = config.get("hidden_size").and_then(|v| v.as_u64()).unwrap_or(768);
        let num_layers = config.get("num_hidden_layers").and_then(|v| v.as_u64()).unwrap_or(12);
        let intermediate_size = config.get("intermediate_size").and_then(|v| v.as_u64()).unwrap_or(3072);
        
        // ì„ë² ë”© íŒŒë¼ë¯¸í„°
        let embedding_params = vocab_size * hidden_size * 2; // input + output
        
        // íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ íŒŒë¼ë¯¸í„°
        let attention_params = num_layers * hidden_size * hidden_size * 4; // Q, K, V, O
        let ffn_params = num_layers * (hidden_size * intermediate_size + intermediate_size * hidden_size);
        let norm_params = num_layers * hidden_size * 4; // layer norms
        
        Ok(embedding_params + attention_params + ffn_params + norm_params)
    }
    
    /// ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    pub fn print_analysis(&self, analysis: &ModelAnalysis) {
        println!("\n=== ğŸ“Š ëª¨ë¸ ë¶„ì„ ê²°ê³¼ ===");
        
        // ê¸°ë³¸ ì •ë³´
        println!("\nğŸ·ï¸  ê¸°ë³¸ ì •ë³´:");
        println!("   ëª¨ë¸ëª…: {}", analysis.model_info.model_name);
        println!("   ì•„í‚¤í…ì²˜: {}", analysis.model_info.architecture);
        println!("   íŒŒë¼ë¯¸í„° ìˆ˜: {:.1}M", analysis.model_info.total_parameters as f64 / 1_000_000.0);
        println!("   ëª¨ë¸ í¬ê¸°: {:.1} MB", analysis.model_info.model_size_mb);
        
        // ì••ì¶• ì í•©ì„±
        println!("\nğŸ—œï¸  ì••ì¶• ì í•©ì„±:");
        println!("   ì „ì²´ ì ìˆ˜: {:.1}/1.0", analysis.compression_suitability.overall_score);
        println!("   RBE ì í•©ì„±: {:.1}/1.0", analysis.compression_suitability.rbe_suitability);
        println!("   ê¶Œì¥ ë¸”ë¡ í¬ê¸°: {}", analysis.compression_suitability.recommended_block_size);
        println!("   ì˜ˆìƒ ì••ì¶•ë¥ : {:.1}:1", analysis.compression_suitability.estimated_compression_ratio);
        println!("   ë©”ëª¨ë¦¬ ì ˆì•½ë¥ : {:.1}%", analysis.compression_suitability.memory_reduction_estimate * 100.0);
        
        // ì„±ëŠ¥ ì¶”ì •
        println!("\nâš¡ ì„±ëŠ¥ ì¶”ì •:");
        println!("   ì¶”ë¡  ì†ë„: {:.1} ms", analysis.performance_estimate.inference_speed_ms);
        println!("   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {:.1} MB", analysis.performance_estimate.memory_usage_mb);
        println!("   GPU ë©”ëª¨ë¦¬: {:.1} MB", analysis.performance_estimate.gpu_memory_mb);
        println!("   ì²˜ë¦¬ëŸ‰: {:.0} tokens/sec", analysis.performance_estimate.throughput_tokens_per_sec);
        
        // ë ˆì´ì–´ ë¶„ì„
        println!("\nğŸ—ï¸  ë ˆì´ì–´ êµ¬ì¡°:");
        for (layer_type, count) in &analysis.layer_analysis.layer_types {
            println!("   {}: {} ê°œ", layer_type, count);
        }
        
        println!("\nâœ… ë¶„ì„ ì™„ë£Œ!");
    }
} 