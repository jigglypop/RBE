{
                "architectures": ["GPT2LMHeadModel"],
                "model_type": "gpt2",
                "n_positions": 1024,
                "n_ctx": 1024,
                "n_embd": 768,
                "n_layer": 12,
                "n_head": 12,
                "vocab_size": 51200,
                "tokenizer_class": "PreTrainedTokenizerFast"
            }