use rbe_llm::{
    encoder::{RBEEncoder, encoder::{QualityGrade, CompressionProfile, CompressionConfig}},
    decoder::WeightGenerator,
    TransformType,
    HybridEncodedBlock,
    nlp::linear::rbe_linear::{RBELinear, RBELinearConfig},
};
use std::{time::Instant, path::Path, collections::HashMap};
use tokenizers::tokenizer::Tokenizer;
use safetensors::{tensor::SafeTensors, SafeTensorError};
use std::fs;
use std::io::Read;
use serde_json::Value;

/// í•œê¸€ sLLM ì••ì¶• ë°ëª¨
/// KoMiniLM-23M ëª¨ë¸ì„ RBEë¡œ ì••ì¶•í•˜ê³  ì‹¤ì œ í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

// ëª¨ë¸ êµ¬ì¡°ì²´
struct CompressedKoreanLLM {
    tokenizer: Tokenizer,
    layers: HashMap<String, CompressedLayer>,
    config: ModelConfig,
}

// ì••ì¶•ëœ ë ˆì´ì–´
struct CompressedLayer {
    blocks: Vec<HybridEncodedBlock>,
    shape: Vec<usize>,
}

// ëª¨ë¸ ì„¤ì •
#[derive(Clone)]
struct ModelConfig {
    vocab_size: usize,
    hidden_size: usize,
    n_layers: usize,
    n_heads: usize,
    max_position_embeddings: usize,
    model_type: String,
}

// LayerNorm (ì••ì¶• ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
struct LayerNorm {
    weight: Vec<f32>,
    bias: Vec<f32>,
    eps: f32,
}

impl LayerNorm {
    fn from_tensors(weight: Vec<f32>, bias: Vec<f32>) -> Self {
        Self { weight, bias, eps: 1e-5 }
    }
    
    fn forward(&self, x: &[f32]) -> Vec<f32> {
        let mean = x.iter().sum::<f32>() / x.len() as f32;
        let variance = x.iter().map(|v| (v - mean).powi(2)).sum::<f32>() / x.len() as f32;
        let std = (variance + self.eps).sqrt();
        
        x.iter().enumerate()
            .map(|(i, &v)| ((v - mean) / std) * self.weight[i] + self.bias[i])
            .collect()
    }
}

// ëª¨ë¸ ë¡œë“œ ë° ì••ì¶•
async fn load_and_compress_korean_model(model_path: &Path) -> Result<CompressedKoreanLLM, Box<dyn std::error::Error>> {
    println!("ğŸ” í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {:?}", model_path);
    
    // 1. ì„¤ì • íŒŒì¼ ë¡œë“œ
    let config_path = model_path.join("config.json");
    let config_str = fs::read_to_string(&config_path)?;
    let config_json: Value = serde_json::from_str(&config_str)?;
    
    let config = ModelConfig {
        vocab_size: config_json["vocab_size"].as_u64().unwrap() as usize,
        hidden_size: config_json["hidden_size"].as_u64().unwrap() as usize,
        n_layers: config_json["num_hidden_layers"].as_u64().unwrap() as usize,
        n_heads: config_json["num_attention_heads"].as_u64().unwrap() as usize,
        max_position_embeddings: config_json["max_position_embeddings"].as_u64().unwrap() as usize,
        model_type: config_json["model_type"].as_str().unwrap().to_string(),
    };
    
    println!("ğŸ“Š ëª¨ë¸ êµ¬ì„±:");
    println!("  - ëª¨ë¸ íƒ€ì…: {}", config.model_type);
    println!("  - ì–´íœ˜ í¬ê¸°: {}", config.vocab_size);
    println!("  - íˆë“  í¬ê¸°: {}", config.hidden_size);
    println!("  - ë ˆì´ì–´ ìˆ˜: {}", config.n_layers);
    
    // 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    println!("\nğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...");
    let tokenizer_path = model_path.join("tokenizer.json");
    let tokenizer = Tokenizer::from_file(&tokenizer_path)
        .map_err(|e| format!("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {}", e))?;
    
    // 3. ê°€ì¤‘ì¹˜ ë¡œë“œ
    println!("\nğŸ“¦ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘...");
    let weights_path = model_path.join("model.safetensors");
    let mut file = fs::File::open(&weights_path)?;
    let mut buffer = Vec::new();
    file.read_to_end(&mut buffer)?;
    let tensors = SafeTensors::deserialize(&buffer)?;
    
    // 4. RBE ì••ì¶• ì„¤ì •
    let compression_config = CompressionConfig {
        block_size: 64,
        quality_grade: QualityGrade::A,
        transform_type: TransformType::Dwt,
        profile: CompressionProfile::Balanced,
        custom_coefficients: Some(200), // Aê¸‰ í’ˆì§ˆì„ ìœ„í•œ ê³„ìˆ˜
        min_block_count: None,
        rmse_threshold: Some(0.001),
        compression_ratio_threshold: Some(500.0),
    };
    
    // 5. ë ˆì´ì–´ë³„ ì••ì¶•
    println!("\nğŸ”¨ ë ˆì´ì–´ë³„ RBE ì••ì¶• ì‹œì‘...");
    let start_time = Instant::now();
    
    let mut layers = HashMap::new();
    
    // ì••ì¶•í•  í…ì„œë“¤ì˜ ëª©ë¡ì„ ìˆ˜ì§‘
    let tensor_names: Vec<_> = tensors.names().into_iter().collect();
    let total_tensors = tensor_names.len();
    
    for (idx, tensor_name) in tensor_names.iter().enumerate() {
        if idx % 10 == 0 {
            println!("  ì§„í–‰ë¥ : {}/{} ({:.1}%)", idx, total_tensors, 
                     idx as f32 / total_tensors as f32 * 100.0);
        }
        
        // LayerNormì€ ì••ì¶•í•˜ì§€ ì•ŠìŒ
        if tensor_name.contains("LayerNorm") || tensor_name.contains("ln_") {
            continue;
        }
        
        // í…ì„œ ë°ì´í„° ì¶”ì¶œ
        let tensor = tensors.tensor(tensor_name)?;
        let shape = tensor.shape();
        let data = tensor.data();
        
        // f32ë¡œ ë³€í™˜
        let weights: Vec<f32> = data.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        // ì••ì¶•
        let compressed = compress_tensor(&weights, shape, &compression_config)?;
        layers.insert(tensor_name.to_string(), compressed);
    }
    
    println!("\nâœ… ì „ì²´ ì••ì¶• ì™„ë£Œ! ì‹œê°„: {:.2}ì´ˆ", start_time.elapsed().as_secs_f64());
    
    // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
    print_memory_usage(&layers, &config);
    
    Ok(CompressedKoreanLLM {
        tokenizer,
        layers,
        config,
    })
}

// í…ì„œ ì••ì¶•
fn compress_tensor(
    weights: &[f32],
    shape: &[usize],
    config: &CompressionConfig,
) -> Result<CompressedLayer, Box<dyn std::error::Error>> {
    // 2Dë¡œ ë³€í™˜
    let (rows, cols) = match shape.len() {
        1 => (shape[0], 1),
        2 => (shape[0], shape[1]),
        _ => {
            let rows = shape[0];
            let cols = shape[1..].iter().product();
            (rows, cols)
        }
    };
    
    // RBE ì••ì¶•
    let (blocks, _, compression_ratio, rmse) = RBEEncoder::compress_with_profile(
        weights,
        rows,
        cols,
        config.block_size,
        config.custom_coefficients.unwrap_or(200),
        config.transform_type,
    )?;
    
    Ok(CompressedLayer {
        blocks,
        shape: shape.to_vec(),
    })
}

// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
fn print_memory_usage(layers: &HashMap<String, CompressedLayer>, config: &ModelConfig) {
    println!("\nğŸ“Š ì••ì¶•ë¥  í†µê³„:");
    
    let mut total_original = 0usize;
    let mut total_compressed = 0usize;
    
    for (name, layer) in layers {
        let original_size = layer.shape.iter().product::<usize>() * 4; // f32
        let compressed_size = layer.blocks.len() * std::mem::size_of::<HybridEncodedBlock>();
        
        total_original += original_size;
        total_compressed += compressed_size;
        
        if name.contains("embedding") || name.contains("lm_head") {
            println!("  - {}: {:.2}MB â†’ {:.2}MB (ì••ì¶•ë¥  {:.1}x)", 
                     name,
                     original_size as f32 / 1024.0 / 1024.0,
                     compressed_size as f32 / 1024.0 / 1024.0,
                     original_size as f32 / compressed_size as f32);
        }
    }
    
    println!("\nğŸ“ˆ ì „ì²´ í†µê³„:");
    println!("  - ì›ë³¸ í¬ê¸°: {:.2}MB", total_original as f32 / 1024.0 / 1024.0);
    println!("  - ì••ì¶• í¬ê¸°: {:.2}MB", total_compressed as f32 / 1024.0 / 1024.0);
    println!("  - ì „ì²´ ì••ì¶•ë¥ : {:.1}x", total_original as f32 / total_compressed as f32);
    println!("  - ë©”ëª¨ë¦¬ ì ˆì•½: {:.1}%", 
             (1.0 - total_compressed as f32 / total_original as f32) * 100.0);
}

// í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„±
async fn generate_korean_text(
    model: &CompressedKoreanLLM,
    prompt: &str,
    max_length: usize,
) -> Result<String, Box<dyn std::error::Error>> {
    println!("\nğŸš€ í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘");
    println!("ğŸ“ í”„ë¡¬í”„íŠ¸: {}", prompt);
    
    // í† í°í™”
    let encoding = model.tokenizer.encode(prompt, false)
        .map_err(|e| format!("í† í°í™” ì‹¤íŒ¨: {:?}", e))?;
    let mut input_ids: Vec<u32> = encoding.get_ids().to_vec();
    
    println!("ğŸ”¢ ì…ë ¥ í† í°: {:?} ({}ê°œ)", &input_ids[..5.min(input_ids.len())], input_ids.len());
    
    // WeightGenerator ìƒì„±
    let weight_generator = WeightGenerator::new();
    
    // ìƒì„± ë£¨í”„
    for step in 0..max_length {
        let start = Instant::now();
        
        // ê°„ë‹¨í•œ ì¶”ë¡  ë°ëª¨ (ì‹¤ì œë¡œëŠ” ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ë¥¼ êµ¬í˜„í•´ì•¼ í•¨)
        // ì—¬ê¸°ì„œëŠ” ì„ë² ë”© ë ˆì´ì–´ë§Œ ì‚¬ìš©í•´ì„œ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ ì‹œë®¬ë ˆì´ì…˜
        let token_id = input_ids.last().unwrap();
        
        // í† í° ì„ë² ë”© ê°€ì ¸ì˜¤ê¸° (ì˜ˆì‹œ)
        let embedding_key = match model.config.model_type.as_str() {
            "gpt2" => "wte.weight",
            "bert" | "electra" => "embeddings.word_embeddings.weight",
            _ => "embeddings.token_embeddings.weight",
        };
        
        if let Some(embedding_layer) = model.layers.get(embedding_key) {
            // ì••ì¶•ëœ ë¸”ë¡ì—ì„œ í† í° ì„ë² ë”© ë””ì½”ë”©
            // ì‹¤ì œë¡œëŠ” ì „ì²´ forward passë¥¼ êµ¬í˜„í•´ì•¼ í•˜ì§€ë§Œ, ë°ëª¨ë¥¼ ìœ„í•´ ê°„ë‹¨íˆ ì²˜ë¦¬
            let next_token = generate_next_token(&weight_generator, embedding_layer, *token_id as usize);
            input_ids.push(next_token);
            
            // ë””ì½”ë”©
            let generated = model.tokenizer.decode(&input_ids, false)
                .map_err(|e| format!("ë””ì½”ë”© ì‹¤íŒ¨: {:?}", e))?;
            
            if step % 5 == 0 {
                println!("  Step {}: {} ({:.1}ms)", step, generated, start.elapsed().as_millis());
            }
            
            // ì¢…ë£Œ ì¡°ê±´ (EOS í† í°)
            if next_token == 2 {  // ì¼ë°˜ì ì¸ EOS token ID
                break;
            }
        } else {
            println!("âš ï¸  ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
            break;
        }
    }
    
    let final_text = model.tokenizer.decode(&input_ids, false)
        .map_err(|e| format!("ìµœì¢… ë””ì½”ë”© ì‹¤íŒ¨: {:?}", e))?;
    
    Ok(final_text)
}

// ë‹¤ìŒ í† í° ìƒì„± (ê°„ë‹¨í•œ ì˜ˆì‹œ)
fn generate_next_token(
    weight_generator: &WeightGenerator,
    embedding_layer: &CompressedLayer,
    current_token: usize,
) -> u32 {
    // ì‹¤ì œë¡œëŠ” ì „ì²´ ëª¨ë¸ì„ í†µê³¼í•´ì•¼ í•˜ì§€ë§Œ, ë°ëª¨ë¥¼ ìœ„í•´ ê°„ë‹¨íˆ ì²˜ë¦¬
    // ì—¬ê¸°ì„œëŠ” ëœë¤í•˜ê²Œ ë‹¤ìŒ í† í°ì„ ì„ íƒ
    use rand::Rng;
    let mut rng = rand::thread_rng();
    
    // í† í° ë²”ìœ„ ë‚´ì—ì„œ ëœë¤ ì„ íƒ (ì‹¤ì œë¡œëŠ” í™•ë¥  ë¶„í¬ì— ë”°ë¼ ì„ íƒí•´ì•¼ í•¨)
    rng.gen_range(0..50000) as u32
}

// RBE ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì¶”ë¡  í•¨ìˆ˜
fn forward_rbe_layer(
    weight_generator: &WeightGenerator,
    compressed_layer: &CompressedLayer,
    input: &[f32],
) -> Vec<f32> {
    // ë¸”ë¡ë³„ë¡œ ë””ì½”ë”©í•˜ê³  í–‰ë ¬ ê³±ì…ˆ ìˆ˜í–‰
    let mut output = vec![0.0f32; compressed_layer.shape[0]];
    
    for (block_idx, block) in compressed_layer.blocks.iter().enumerate() {
        // ë¸”ë¡ ë””ì½”ë”©
        let decoded_block = weight_generator.decode_block(block);
        
        // ë¶€ë¶„ í–‰ë ¬ ê³±ì…ˆ (ê°„ë‹¨í•œ êµ¬í˜„)
        // ì‹¤ì œë¡œëŠ” ë¸”ë¡ ìœ„ì¹˜ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ê³„ì‚°ì´ í•„ìš”
        for i in 0..block.rows {
            for j in 0..block.cols {
                if i < output.len() && j < input.len() {
                    output[i] += decoded_block[i * block.cols + j] * input[j];
                }
            }
        }
    }
    
    output
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ RBE í•œêµ­ì–´ sLLM ì••ì¶• ë° ì¶”ë¡  ë°ëª¨");
    println!("=====================================\n");
    
    let start_time = Instant::now();
    
    // 1. ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    let model_path = Path::new("models/kominilm-23m");
    if !model_path.exists() {
        println!("âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € setup_korean_test_model.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.");
        println!("   python setup_korean_test_model.py");
        return Ok(());
    }
    
    // 2. ëª¨ë¸ ë¡œë“œ ë° ì••ì¶•
    let compressed_model = load_and_compress_korean_model(model_path).await?;
    
    // 3. í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    println!("\nğŸ§ª í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸");
    println!("==========================");
    
    let test_prompts = vec![
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€",
        "RBE ì‹œìŠ¤í…œì€",
        "í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬",
    ];
    
    for prompt in test_prompts {
        println!("\nğŸ“ í”„ë¡¬í”„íŠ¸: \"{}\"", prompt);
        
        match generate_korean_text(&compressed_model, prompt, 20).await {
            Ok(generated) => {
                println!("âœ… ìƒì„±ëœ í…ìŠ¤íŠ¸: {}", generated);
            }
            Err(e) => {
                println!("âŒ ìƒì„± ì‹¤íŒ¨: {}", e);
            }
        }
    }
    
    // 4. RBE ë¸”ë¡ ì§ì ‘ í…ŒìŠ¤íŠ¸
    println!("\nğŸ”¬ RBE ì••ì¶• ë¸”ë¡ ì§ì ‘ í…ŒìŠ¤íŠ¸");
    println!("=============================");
    
    // ì„ë² ë”© ë ˆì´ì–´ì—ì„œ ì²« ë²ˆì§¸ ë¸”ë¡ í…ŒìŠ¤íŠ¸
    if let Some(embedding_layer) = compressed_model.layers.get("wte.weight")
        .or(compressed_model.layers.get("embeddings.word_embeddings.weight")) {
        
        if let Some(first_block) = embedding_layer.blocks.first() {
            let weight_generator = WeightGenerator::new();
            let decoded = weight_generator.decode_block(first_block);
            
            println!("  - ì²« ë²ˆì§¸ ë¸”ë¡ í¬ê¸°: {}x{}", first_block.rows, first_block.cols);
            println!("  - RBE íŒŒë¼ë¯¸í„°: {:?}", &first_block.rbe_params[..4]);
            println!("  - ì”ì°¨ ê³„ìˆ˜ ê°œìˆ˜: {}", first_block.residuals.len());
            println!("  - ë””ì½”ë”©ëœ ê°’ ìƒ˜í”Œ: {:?}", &decoded[..5.min(decoded.len())]);
            
            // RMSE ê³„ì‚° (ì›ë³¸ ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ìì²´ ê²€ì¦)
            let re_encoded = RBEEncoder::new(200, TransformType::Dwt)
                .encode_block(&decoded, first_block.rows, first_block.cols);
            let re_decoded = weight_generator.decode_block(&re_encoded);
            
            let rmse: f32 = decoded.iter()
                .zip(re_decoded.iter())
                .map(|(a, b)| (a - b).powi(2))
                .sum::<f32>() / decoded.len() as f32;
            
            println!("  - ì¬ì¸ì½”ë”© RMSE: {:.6}", rmse.sqrt());
        }
    }
    
    println!("\nğŸ ì „ì²´ ì‹¤í–‰ ì‹œê°„: {:.2}ì´ˆ", start_time.elapsed().as_secs_f64());
    println!("\nâœ¨ RBE í•œêµ­ì–´ ëª¨ë¸ ì••ì¶• ë° ì¶”ë¡  ì™„ë£Œ!");
    
    Ok(())
} 