//! Korean sLLM (ì†Œí˜• ì–¸ì–´ ëª¨ë¸) ì••ì¶• ë° ì¶”ë¡  ë°ëª¨
//! ì‹¤ì œ í•œêµ­ì–´ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  RBEë¡œ ì••ì¶•í•œ í›„ í…ìŠ¤íŠ¸ ìƒì„±
#![allow(unused_imports, dead_code, unused_variables, unused_mut)]

use rbe_llm::{
    encoder::{encoder::{QualityGrade, CompressionProfile}, CompressionConfig, RBEEncoder},
    decoder::WeightGenerator,
    TransformType,
    HybridEncodedBlock,
};
use rbe_llm::nlp::bert_inference::{
    CompressedLayer, CompressedBert, BertAttention, BertFeedForward, CompressedBertLayer
};
use std::{time::Instant, path::Path, collections::HashMap};
use tokenizers::tokenizer::Tokenizer;
use safetensors::{tensor::SafeTensors, SafeTensorError, Dtype};
use std::fs;
use std::io::Read;
use serde_json::Value;
use anyhow::{Result, bail, anyhow};

/// í•œê¸€ sLLM ì••ì¶• ë°ëª¨
/// KoMiniLM-23M ëª¨ë¸ì„ RBEë¡œ ì••ì¶•í•˜ê³  ì‹¤ì œ í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

// ëª¨ë¸ ì„¤ì •
#[derive(Clone, Debug)]
struct ModelConfig {
    vocab_size: usize,
    hidden_size: usize,
    n_layers: usize,
    n_heads: usize,
    intermediate_size: usize,
}

// ê°€ì¤‘ì¹˜ ì €ì¥ì†Œ
struct CompressedWeights {
    layers: HashMap<String, CompressedLayer>,
    biases: HashMap<String, Vec<f32>>,
    layernorms: HashMap<String, Vec<f32>>,
}

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸš€ RBE ê¸°ë°˜ í•œêµ­ì–´ sLLM ë°ëª¨ ì‹œì‘");
    
    // ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì˜ˆ: ./models/KoMiniLM-23M)
    let model_path = Path::new("models").join("KoMiniLM-23M");
    if !model_path.exists() {
        println!("ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. `setup_korean_test_model.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.");
        return Ok(());
    }

    // 1. ëª¨ë¸ ë¡œë“œ ë° ì••ì¶•
    let (weights, config, tokenizer) = load_and_compress_model(&model_path).await?;
    
    // 2. ì••ì¶•ëœ BERT ëª¨ë¸ êµ¬ì„±
    let bert_model = build_compressed_bert(&weights, &config)?;

    // 3. í…ìŠ¤íŠ¸ ìƒì„±
    let prompt = "ëŒ€í•œë¯¼êµ­ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì€";
    let generated_text = generate_text(bert_model, &tokenizer, prompt, 30).await?;

    println!("\nâœ… ìµœì¢… ìƒì„± ê²°ê³¼:");
    println!("--------------------");
    println!("{}", generated_text);
    println!("--------------------");

    Ok(())
}

async fn load_and_compress_model(model_path: &Path) -> Result<(CompressedWeights, ModelConfig, Tokenizer)> {
    println!("\n[1/3] ëª¨ë¸ ë¡œë”© ë° ì••ì¶•...");
    let start_time = Instant::now();

    // ì„¤ì • íŒŒì¼ ë¡œë“œ
    let config_path = model_path.join("config.json");
    let mut config_file = fs::File::open(&config_path)?;
    let mut config_str = String::new();
    config_file.read_to_string(&mut config_str)?;
    let json_config: Value = serde_json::from_str(&config_str)?;

    let config = ModelConfig {
        vocab_size: json_config["vocab_size"].as_u64().unwrap() as usize,
        hidden_size: json_config["hidden_size"].as_u64().unwrap() as usize,
        n_layers: json_config["num_hidden_layers"].as_u64().unwrap() as usize,
        n_heads: json_config["num_attention_heads"].as_u64().unwrap() as usize,
        intermediate_size: json_config["intermediate_size"].as_u64().unwrap() as usize,
    };
    println!("ëª¨ë¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {:?}", config);

    // í† í¬ë‚˜ì´ì € ë¡œë“œ
    let tokenizer_path = model_path.join("tokenizer.json");
    let tokenizer = Tokenizer::from_file(&tokenizer_path).map_err(|e| anyhow!("Tokenizer ë¡œë“œ ì‹¤íŒ¨: {}", e))?;
    println!("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ");

    // ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ
    let weights_path = model_path.join("model.safetensors");
    let weights_data = fs::read(&weights_path)?;
    let tensors = SafeTensors::deserialize(&weights_data)?;
    println!("ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ");

    // ì••ì¶• ì„¤ì •
    let compression_config = CompressionConfig {
        block_size: 64,
        quality_grade: QualityGrade::A,
        transform_type: TransformType::Dwt,
        profile: CompressionProfile::High,
        custom_coefficients: None,
        min_block_count: None,
        rmse_threshold: Some(0.1),
        compression_ratio_threshold: None,
    };

    let mut compressed_weights = CompressedWeights {
        layers: HashMap::new(),
        biases: HashMap::new(),
        layernorms: HashMap::new(),
    };

    let mut original_total_size = 0;
    let mut compressed_total_size = 0;

    for (name, tensor_view) in tensors.tensors() {
        let shape = tensor_view.shape();
        let data = tensor_view.data();
        let original_size = data.len();
        original_total_size += original_size;

        // ëª¨ë“  2D í…ì„œë¥¼ ì••ì¶• ëŒ€ìƒìœ¼ë¡œ ê°„ì£¼
        if shape.len() == 2 {
            // í–‰ë ¬ ê°€ì¤‘ì¹˜ ì••ì¶•
            let weights_f32 = bytes_to_f32(data);
            
            match RBEEncoder::compress_with_config(&weights_f32, shape[0], shape[1], &compression_config) {
                Ok((blocks, _time, ratio, rmse)) => {
                    let compressed_size = blocks.iter().map(|b| 32 + b.residuals.len() * 8).sum::<usize>();
                    compressed_total_size += compressed_size;
                    
                    let layer = CompressedLayer { blocks, shape: (shape[0], shape[1]) };
                    compressed_weights.layers.insert(name.clone(), layer);
                    
                    println!("  - ì••ì¶•: {} ({} -> {} bytes, {:.2}x, RMSE: {:.4})", name, original_size, compressed_size, ratio, rmse);
                }
                Err(e) => {
                    println!("  - ì••ì¶• ì‹¤íŒ¨ {}: {}. ì••ì¶• ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.", name, e);
                    let blocks = Vec::new(); 
                    let layer = CompressedLayer { blocks, shape: (shape[0], shape[1]) };
                    compressed_weights.layers.insert(name.clone(), layer);
                    compressed_total_size += original_size;
                }
            }

        } else if shape.len() == 1 {
            // 1D í…ì„œëŠ” bias ë˜ëŠ” layernormìœ¼ë¡œ ê°„ì£¼ (ì••ì¶• ì•ˆí•¨)
            if name.contains("bias") {
                compressed_weights.biases.insert(name.clone(), bytes_to_f32(data));
            } else {
                compressed_weights.layernorms.insert(name.clone(), bytes_to_f32(data));
            }
            compressed_total_size += original_size;
        }
    }
    
    println!("\nì••ì¶• ì™„ë£Œ! ({:.2}s)", start_time.elapsed().as_secs_f32());
    println!("  - ì›ë³¸ í¬ê¸°: {:.2} MB", original_total_size as f32 / 1_048_576.0);
    println!("  - ì••ì¶• í¬ê¸°: {:.2} MB", compressed_total_size as f32 / 1_048_576.0);
    println!("  - ì••ì¶•ë¥ : {:.2}x", original_total_size as f32 / compressed_total_size as f32);

    Ok((compressed_weights, config, tokenizer))
}


fn build_compressed_bert<'a>(weights: &'a CompressedWeights, config: &'a ModelConfig) -> Result<CompressedBert<'a>> {
    println!("\n[2/3] ì••ì¶•ëœ BERT ëª¨ë¸ êµ¬ì„±...");
    
    let mut bert_layers = Vec::with_capacity(config.n_layers);
    for i in 0..config.n_layers {
        let prefix = format!("encoder.layer.{}", i);
        let attention = BertAttention {
            q_w: weights.layers.get(&format!("{}.attention.self.query.weight", prefix)).ok_or_else(|| anyhow!("Missing weight"))?,
            q_b: weights.biases.get(&format!("{}.attention.self.query.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            k_w: weights.layers.get(&format!("{}.attention.self.key.weight", prefix)).ok_or_else(|| anyhow!("Missing weight"))?,
            k_b: weights.biases.get(&format!("{}.attention.self.key.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            v_w: weights.layers.get(&format!("{}.attention.self.value.weight", prefix)).ok_or_else(|| anyhow!("Missing weight"))?,
            v_b: weights.biases.get(&format!("{}.attention.self.value.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            output_w: weights.layers.get(&format!("{}.attention.output.dense.weight", prefix)).ok_or_else(|| anyhow!("Missing weight"))?,
            output_b: weights.biases.get(&format!("{}.attention.output.dense.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            layernorm_w: weights.layernorms.get(&format!("{}.attention.output.LayerNorm.weight", prefix)).ok_or_else(|| anyhow!("Missing layernorm"))?,
            layernorm_b: weights.biases.get(&format!("{}.attention.output.LayerNorm.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            n_heads: config.n_heads,
            hidden_size: config.hidden_size,
        };

        let ffn = BertFeedForward {
            intermediate_w: weights.layers.get(&format!("{}.intermediate.dense.weight", prefix)).ok_or_else(|| anyhow!("Missing weight"))?,
            intermediate_b: weights.biases.get(&format!("{}.intermediate.dense.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            output_w: weights.layers.get(&format!("{}.output.dense.weight", prefix)).ok_or_else(|| anyhow!("Missing weight"))?,
            output_b: weights.biases.get(&format!("{}.output.dense.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
            layernorm_w: weights.layernorms.get(&format!("{}.output.LayerNorm.weight", prefix)).ok_or_else(|| anyhow!("Missing layernorm"))?,
            layernorm_b: weights.biases.get(&format!("{}.output.LayerNorm.bias", prefix)).ok_or_else(|| anyhow!("Missing bias"))?,
        };
        
        bert_layers.push(CompressedBertLayer { attention, ffn });
    }

    let bert_model = CompressedBert {
        token_emb: weights.layers.get("embeddings.word_embeddings.weight").ok_or_else(|| anyhow!("Missing embedding"))?,
        position_emb: weights.layers.get("embeddings.position_embeddings.weight").ok_or_else(|| anyhow!("Missing embedding"))?,
        token_type_emb: weights.layers.get("embeddings.token_type_embeddings.weight").ok_or_else(|| anyhow!("Missing embedding"))?,
        emb_layernorm_w: weights.layernorms.get("embeddings.LayerNorm.weight").ok_or_else(|| anyhow!("Missing layernorm"))?,
        emb_layernorm_b: weights.biases.get("embeddings.LayerNorm.bias").ok_or_else(|| anyhow!("Missing bias"))?,
        layers: bert_layers,
        
        lm_head_dense_w: weights.layers.get("cls.predictions.transform.dense.weight"),
        lm_head_dense_b: weights.biases.get("cls.predictions.transform.dense.bias").map(|v| &**v),
        lm_head_layernorm_w: weights.layernorms.get("cls.predictions.transform.LayerNorm.weight").map(|v| &**v),
        lm_head_layernorm_b: weights.biases.get("cls.predictions.transform.LayerNorm.bias").map(|v| &**v),
        
        lm_head_decoder_w: weights.layers.get("embeddings.word_embeddings.weight").ok_or_else(|| anyhow!("Missing word_embeddings for decoder"))?,
        lm_head_decoder_b: weights.biases.get("cls.predictions.bias").map(|v| &**v),

        hidden_size: config.hidden_size,
        n_heads: config.n_heads,
        generator: WeightGenerator::new(),
    };

    println!("BERT ëª¨ë¸ êµ¬ì„± ì™„ë£Œ");
    Ok(bert_model)
}


async fn generate_text(mut model: CompressedBert<'_>, tokenizer: &Tokenizer, prompt: &str, max_length: usize) -> Result<String> {
    println!("\n[3/3] í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...");
    println!("  - í”„ë¡¬í”„íŠ¸: \"{}\"", prompt);

    let encoding = tokenizer.encode(prompt, false).map_err(|e| anyhow!("í† í°í™” ì‹¤íŒ¨: {}", e))?;
    let mut token_ids = encoding.get_ids().to_vec();

    for i in 0..max_length {
        let start_time = Instant::now();
        
        let logits = model.forward(&token_ids)?;
        
        // ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í° ì„ íƒ (greedy)
        let next_token = logits.iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map(|(index, _)| index as u32)
            .unwrap_or(0);

        token_ids.push(next_token);
        
        let current_text = tokenizer.decode(&token_ids, true).map_err(|e| anyhow!("ë””ì½”ë”© ì‹¤íŒ¨: {}", e))?;
        println!("  - Step {}: \"{}\" ({:.2}ms)", i + 1, current_text, start_time.elapsed().as_millis());

        // [SEP] í† í° ë§Œë‚˜ë©´ ì¢…ë£Œ
        if next_token == tokenizer.token_to_id("[SEP]").unwrap_or(102) {
            break;
        }
    }

    tokenizer.decode(&token_ids, true).map_err(|e| anyhow!("ìµœì¢… ë””ì½”ë”© ì‹¤íŒ¨: {}", e))
}

fn bytes_to_f32(bytes: &[u8]) -> Vec<f32> {
    bytes
        .chunks_exact(4)
        .map(|chunk| f32::from_le_bytes(chunk.try_into().unwrap()))
        .collect()
} 