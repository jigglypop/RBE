#!/usr/bin/env python3
"""
KoMiniLM-23M í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import torch
from transformers import AutoModel, AutoTokenizer, AutoConfig
from pathlib import Path

def setup_korean_test_model():
    """KoMiniLM-23M ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¤ì •"""
    
    MODEL_ID = "BM-K/KoMiniLM"
    MODEL_DIR = Path("models/kominilm-23m")
    
    print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì • ì‹œì‘")
    print(f"ğŸ“¦ ëª¨ë¸: {MODEL_ID}")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {MODEL_DIR}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. ì„¤ì • ë‹¤ìš´ë¡œë“œ
        print("\nğŸ“‹ ëª¨ë¸ ì„¤ì • ë‹¤ìš´ë¡œë“œ ì¤‘...")
        config = AutoConfig.from_pretrained(MODEL_ID)
        config.save_pretrained(MODEL_DIR)
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        print(f"  âœ… ëª¨ë¸ íƒ€ì…: {config.model_type}")
        print(f"  âœ… ìˆ¨ê²¨ì§„ í¬ê¸°: {config.hidden_size}")
        print(f"  âœ… ë ˆì´ì–´ ìˆ˜: {config.num_hidden_layers}")
        print(f"  âœ… ì–´í…ì…˜ í—¤ë“œ: {config.num_attention_heads}")
        print(f"  âœ… ì–´íœ˜ í¬ê¸°: {config.vocab_size}")
        
        # 2. í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        print("\nğŸ”¤ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        tokenizer.save_pretrained(MODEL_DIR)
        
        # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
        test_korean_text = "ì•ˆë…•í•˜ì„¸ìš”! RBE ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        tokens = tokenizer.encode(test_korean_text)
        decoded = tokenizer.decode(tokens)
        
        print(f"  âœ… í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸:")
        print(f"    ì›ë³¸: {test_korean_text}")
        print(f"    í† í° ìˆ˜: {len(tokens)}")
        print(f"    ë³µì›: {decoded}")
        
        # 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print("\nğŸ§  ëª¨ë¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        model = AutoModel.from_pretrained(MODEL_ID)
        model.save_pretrained(MODEL_DIR)
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  âœ… ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,} ({total_params/1e6:.1f}M)")
        
        # 4. RBE ì„¤ì • íŒŒì¼ ìƒì„±
        print("\nâš™ï¸  RBE ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        rbe_config = {
            "model_info": {
                "name": "KoMiniLM-23M",
                "model_id": MODEL_ID,
                "language": "korean",
                "total_parameters": total_params,
                "model_type": config.model_type,
                "architecture": "bert"
            },
            "rbe_settings": {
                "compression_target": {
                    "quality_grade": "A",  # ê³ í’ˆì§ˆë¡œ ì‹œì‘
                    "target_compression_ratio": 500.0,  # 500:1 ì••ì¶• ëª©í‘œ
                    "max_rmse": 1e-4
                },
                "poincare_ball": {
                    "dimension": 128,  # 128ë¹„íŠ¸ ì••ì¶•
                    "coordinate_precision": 112,  # ê°ë„ ì¢Œí‘œ ì •ë°€ë„
                    "radius_precision": 15  # ë°˜ì§€ë¦„ ì •ë°€ë„
                },
                "bit_differential": {
                    "cycle_length": 2048,  # 11ë¹„íŠ¸ ì‚¬ì´í´
                    "num_bit_planes": 128,
                    "error_threshold": 1e-6,
                    "stability_check": True
                }
            },
            "test_settings": {
                "max_sequence_length": 128,
                "batch_size": 4,
                "test_prompts": [
                    "ì•ˆë…•í•˜ì„¸ìš”",
                    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”",
                    "RBE ì••ì¶• ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
                    "í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì…ë‹ˆë‹¤"
                ]
            }
        }
        
        with open(MODEL_DIR / "rbe_config.json", "w", encoding="utf-8") as f:
            json.dump(rbe_config, f, ensure_ascii=False, indent=2)
        
        # 5. ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ë¶„ì„
        print("\nğŸ” ê°€ì¤‘ì¹˜ ë¶„ì„ ì¤‘...")
        weight_analysis = analyze_model_weights(model)
        
        with open(MODEL_DIR / "weight_analysis.json", "w", encoding="utf-8") as f:
            json.dump(weight_analysis, f, indent=2)
        
        print(f"  âœ… ë ˆì´ì–´ ìˆ˜: {weight_analysis['num_layers']}")
        print(f"  âœ… ê°€ì¤‘ì¹˜ ë¶„í¬:")
        for layer_type, stats in weight_analysis['layer_stats'].items():
            print(f"    {layer_type}: í‰ê· ={stats['mean']:.6f}, í‘œì¤€í¸ì°¨={stats['std']:.6f}")
        
        # 6. ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘...")
        model.eval()
        with torch.no_grad():
            test_inputs = tokenizer(
                rbe_config["test_settings"]["test_prompts"], 
                padding=True, 
                truncation=True, 
                return_tensors="pt"
            )
            
            outputs = model(**test_inputs)
            print(f"  âœ… ì¶œë ¥ í˜•íƒœ: {outputs.last_hidden_state.shape}")
            print(f"  âœ… ì¶”ë¡  ì„±ê³µ: {len(rbe_config['test_settings']['test_prompts'])}ê°œ í”„ë¡¬í”„íŠ¸")
        
        print("\nğŸ‰ í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì • ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {MODEL_DIR}")
        print(f"ğŸ“Š ì••ì¶• ëª©í‘œ: {rbe_config['rbe_settings']['compression_target']['target_compression_ratio']}:1")
        print(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: RBETensor êµ¬í˜„ ë° ëª¨ë¸ ì••ì¶•")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def analyze_model_weights(model):
    """ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶„ì„"""
    
    layer_stats = {}
    total_params = 0
    
    for name, param in model.named_parameters():
        layer_type = name.split('.')[0] if '.' in name else name
        
        if layer_type not in layer_stats:
            layer_stats[layer_type] = {
                'count': 0,
                'total_params': 0,
                'values': []
            }
        
        layer_stats[layer_type]['count'] += 1
        layer_stats[layer_type]['total_params'] += param.numel()
        layer_stats[layer_type]['values'].extend(param.flatten().tolist()[:1000])  # ìƒ˜í”Œë§
        total_params += param.numel()
    
    # í†µê³„ ê³„ì‚°
    for layer_type in layer_stats:
        values = layer_stats[layer_type]['values']
        if values:
            import statistics
            layer_stats[layer_type]['mean'] = statistics.mean(values)
            layer_stats[layer_type]['std'] = statistics.stdev(values) if len(values) > 1 else 0.0
            layer_stats[layer_type]['min'] = min(values)
            layer_stats[layer_type]['max'] = max(values)
            del layer_stats[layer_type]['values']  # ë©”ëª¨ë¦¬ ì ˆì•½
    
    return {
        'total_parameters': total_params,
        'num_layers': len(layer_stats),
        'layer_stats': layer_stats
    }

if __name__ == "__main__":
    try:
        success = setup_korean_test_model()
        if success:
            print("\nâœ¨ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ RBE êµ¬í˜„ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("\nğŸ’¥ ì„¤ì • ì‹¤íŒ¨. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {e}")
        print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install transformers torch") 