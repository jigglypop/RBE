#!/usr/bin/env python3
"""
GPT-2 í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
from transformers import GPT2Tokenizer
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors

def download_and_convert_tokenizer():
    """HuggingFaceì—ì„œ GPT-2 í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  tokenizers í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    print("ğŸ”½ GPT-2 í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("models", exist_ok=True)
    
    # HuggingFaceì—ì„œ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
    tokenizer_hf = GPT2Tokenizer.from_pretrained("gpt2")
    
    # í† í¬ë‚˜ì´ì € íŒŒì¼ ì €ì¥
    tokenizer_hf.save_pretrained("models/gpt2_tokenizer_hf")
    
    # tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    print("ğŸ”„ í† í¬ë‚˜ì´ì € í˜•ì‹ ë³€í™˜ ì¤‘...")
    
    # ì–´íœ˜ íŒŒì¼ ë¡œë“œ
    with open("models/gpt2_tokenizer_hf/vocab.json", "r", encoding="utf-8") as f:
        vocab = json.load(f)
    
    # ë³‘í•© íŒŒì¼ ë¡œë“œ - íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    merges_list = []
    with open("models/gpt2_tokenizer_hf/merges.txt", "r", encoding="utf-8") as f:
        lines = f.read().split("\n")[1:]  # ì²« ì¤„ì€ ë²„ì „ ì •ë³´
        for line in lines:
            if line.strip():
                parts = line.split()
                if len(parts) == 2:
                    merges_list.append((parts[0], parts[1]))
    
    # BPE í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer = Tokenizer(models.BPE(vocab=vocab, merges=merges_list))
    
    # Pre-tokenizer ì„¤ì • (GPT-2ì™€ ë™ì¼í•˜ê²Œ)
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    
    # Decoder ì„¤ì •
    tokenizer.decoder = decoders.ByteLevel()
    
    # Post-processor ì„¤ì • (íŠ¹ë³„ í† í° ì²˜ë¦¬)
    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
    
    # íŠ¹ë³„ í† í° ì¶”ê°€
    tokenizer.add_special_tokens(["<|endoftext|>"])
    
    # ì €ì¥
    tokenizer.save("models/tokenizer.json")
    
    print("âœ… í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: models/tokenizer.json")
    
    # í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸:")
    test_text = "Hello, world! This is a test."
    encoding = tokenizer.encode(test_text)
    print(f"  ì›ë³¸: {test_text}")
    print(f"  í† í° ID: {encoding.ids}")
    print(f"  í† í°: {encoding.tokens}")
    
    decoded = tokenizer.decode(encoding.ids)
    print(f"  ë³µì›: {decoded}")
    
    # í•œêµ­ì–´ í…ŒìŠ¤íŠ¸
    test_korean = "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ê³„!"
    encoding_kr = tokenizer.encode(test_korean)
    print(f"\n  í•œêµ­ì–´: {test_korean}")
    print(f"  í† í° ID: {encoding_kr.ids[:10]}...")
    print(f"  í† í° ìˆ˜: {len(encoding_kr.ids)}")

if __name__ == "__main__":
    try:
        download_and_convert_tokenizer()
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("transformers ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install transformers tokenizers") 